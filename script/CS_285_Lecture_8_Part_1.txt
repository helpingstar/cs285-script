All right, in today's lecture we're going to continue our discussion of value-based methods from last time, and we'll discuss some practical deep reinforcement learning algorithms that can utilize Q-functions.
So even though we learned that value-based methods in general are not guaranteed to converge, in practice we can actually get some very powerful and very useful reinforcement learning algorithms out of them, and that's what we'll talk about today.
So first, to recap from last time, we discussed how we could derive a general class of value-based methods which we called fitted Q iteration algorithms, which do not require knowledge of the transition probabilities, and do not require us to explicitly represent a policy.
So in this class of methods there are basically three steps.
Step one, we collect a data set using some policy, and we learn that these are off-policy algorithms, which means that they can collect their data sets using a wide range of different policies.
They can aggregate data from previous iterations, and they can use various exploration rules like epsilon greedy and Boltzmann exploration.
Then in step two, for each transition in our data set, and when I say transition I mean s_i, a_i, s'_i, and r_i tuples, for each transition we calculate what we call the target, which I'm denoting as y_i, which is equal to the current reward plus γ times the max over the actions of the next time step.
And we learned that this max is what allows us to make this algorithm off-policy so the max basically accounts for the fact that you are implicitly computing a greedy argmax policy using your current Q function Q_ϕ and then evaluating the value of that argmax policy by plugging it into Q_ϕ.
So, of course the argmax is simply the max so that gets us our targets y_i and then in step three we update our function approximator for Q which is parameterized by ϕ, by finding the argmin parameters, the parameters that minimize the difference between the output of Q_ϕ and the target's y_i that we just computed.
Step two and three can in general be repeated some number of times before we collect more data.
So this is the general recipe of the Fitted Q iteration algorithm, which has a number of choices that we can make.
We can choose how many transitions to collect in step one.
We can choose how many gradient steps to take in step three when we optimize ϕ.
And we can choose how many times to alternate between steps two and step three before we collect more data.
If we choose each of these hyperparameters to be one, meaning one step of data collection, one gradient step, and only do step two and three once before collecting more data, then we get what is called the online Q-learning algorithm, which I hear called online Q iteration.
This is really Q-learning.
So if you hear someone say Q-learning, they really mean this method.
Step one, take one action a_i and observe the resulting transition, s_i, a_i, s'_i, r_i.
Step two, calculate the target value y_i for that transition.
Step three, perform one gradient step on the difference between the output of the Q function and the value that you just calculated.
And of course, as usual, this algorithm fits into our anatomy of our reinforcement learning method.
The orange box is step one.
The green box is where we fit our Q function.
And the blue box here is somewhat degenerate.
It just involves choosing the action to be the argmax of the Q.
All right, so what are some things that are problematic about this general procedure?
We learned about a few things last time.
For instance, we learned that the update in step three, even though it looks like a gradient update, it looks like it's applying the chain rule, it's not actually the gradient of any well-defined objective.
So Q-learning is not gradient descent.
If you were to substitute in the equation for y_i, you would see that Q_ϕ shows up in two places, but there is no gradient through the second term.
So this is not properly applying the chain rule.
We could properly apply the chain rule, and then we get a class of methods.
It's called residual gradient algorithms.
Unfortunately, such algorithms tend to work very poorly in practice because of very severe numerical ill-conditioning problems.
There's another problem with the online Q-learning algorithm, which is that when you sample one transition at a time, sequential transitions are highly correlated.
So the state I see at time step t is likely to be quite similar to the state that I see at time step t plus one, which means that when I take gradient steps on those samples in step three, I'm taking gradient steps on highly correlated transitions.
This violates commonly held assumptions for stochastic gradient methods, and it has a pretty intuitive reason to not work very well, which we'll discuss next before presenting a solution.
All right, so let's talk about the correlation problem.
So here I've just simplified the algorithm.
I've just plugged in the equation for the target value right into the gradient update, so there's only two steps instead of three, but it's the same exact procedure.
So states that you see one right after the other are going to be strongly correlated, meaning that the state at time t plus one is probably similar to the state at time step t, but even if it's not similar, it probably has a very close relationship to it.
And your target values are always changing.
So even though your optimization procedure looks like supervised regression, in a sense, it's kind of chasing its own tail.
It's trying to catch up to itself and then changing out from under itself and the gradient doesn't account for this change.
So the reason this might be a problem is if you imagine that this is your trajectory and you get a few transitions, you'll kind of locally overfit to these transitions because you're seeing very similar transitions right one right after the other.
And then you get some other transitions and then you overfit a little bit to those.
And then you see some others and you overfit to those and so on and so on.
And then when you start a new trajectory, your function approximator will have been left off at the point where it had overfitted to the end of the previous one and will again be bad.
So if it had seen all the transitions all at once, it might have actually fitted all of them accurately.
But because it's seeing this very local highly correlated window at a time, it has just enough time to overfit to each window and not enough broader context to accurately fit the whole function.
Now we could borrow the same idea that we had in Actor-Critic.
When we talked about Actor-Critic algorithms, we actually discussed a solution to the same exact problem when we discussed online Actor-Critic.
And the particular solution we discussed was to parallelize.
It was to have multiple workers that each collect a different transition, (s, a, s', r), collect a batch consisting of samples from all of these workers, update on that batch, and repeat.
And this procedure can in principle address the problem with sequential states being highly correlated.
The sequential states are still correlated, but now you get a batch of different transitions from different workers and across workers they are hopefully not correlated.
So it doesn't solve the problem fully, but it can mitigate it.
And of course, just like we learned about an Actor-Critic, you could have an asynchronous version of this recipe where the individual workers don't wait for a synchronization point for an update to the parameters, but instead query a parameter server for the latest parameters and then proceed at their own pace.
In fact, with Q-learning, this recipe should in theory work even better, because in Q-learning you don't even need the workers to use the latest policy.
So the fact that you might have a slightly older policy that is being executed on some of the workers is in theory not a major problem.
However, there is another solution to the correlated samples problem that tends to be quite easy to use and works very well in practice.
And that's to use something called a replay buffer.
Replay buffers are a fairly old idea in reinforcement learning.
They were introduced way back in the 1990s.
And here's how they work.
So let's think back to the full-fitted Q-iteration algorithm.
In the full-fitted Q-iteration algorithm, we would collect a data set of transitions using whatever policy we wanted, and then we would make multiple updates on that data set of transitions.
So we would label all of the transitions with target values.
Then we might make a large number of gradient steps regressing onto those target values.
And then we might even go back and compute new target values before we even collect more data.
So we might go back and forth between step 2 and step 3, k times, and if k is larger than 1, we might do quite a bit of learning on the same set of transitions.
So the online Q-learning algorithm is simply the special case of this when k is set to 1, and we take one gradient step in step 3.
And of course, any policy for collection will work so long as it has broad support.
In fact, we could even omit step 1 altogether.
So we could just load data from the buffer.
So we can basically have a bunch of stored data loaded from a buffer in step 2, and then iterate on in step 3.
So this gives us this view of Q-learning or fitted Q-iteration as a kind of data-driven method, where we just have a big bucket of transitions.
We don't really care so much where these transitions came from, so long as they cover the space of all possibilities pretty well.
And we're just going to crank away taking more and more updates on those transitions alternately between computing target values and regressing onto those target values.
So we could still take one gradient step in step 3 if we want.
And then we get something that looks a lot like online Q-learning only without the data collection.
So if we take one gradient step, then we're basically just alternating between computing target values for our transitions and taking gradient step for our transitions.
So this gives us this modified version of the Q-learning algorithm with a replay buffer.
In step one, instead of actually taking steps in the real world, we simply sample a batch, multiple transitions, s_i, a_i, s'_i, r_i, from our buffer, which I'm going to call B.
And then in step two, we sum up our gradient over all of the entries in that batch.
So each time around this loop we might sample a different batch, we sample it iid independently and identically distributed from our buffer, which means that our samples are no longer correlated.
And then the only question I have to answer is where do we get our buffer?
So we have multiple samples in the batch, so we have a low variance gradient, and our samples are no longer correlated, which satisfy the assumption of stochastic gradient methods.
We still unfortunately don't have a real gradient, because we're not computing the derivative through this second term, but at least the sample is not correlated.
So where will the data come from?
Well, what we need to do is we need to periodically feed the replay buffer.
Because initially, our policy will be very bad.
And maybe our initial very bad policy just won't visit all the interesting regions of the space.
So we still want to occasionally feed the replay buffer by using our latest policy, maybe with some epsilon greedy exploration to collect some better data, some data that achieves better coverage.
So then, the diagram you could think of might look like this.
We have our buffer of transitions.
We're cranking away on this buffer doing our off-policy Q learning.
And then periodically, we deploy some policy back into the world.
For example, the greedy policy with epsilon greedy exploration to collect a little bit more data and bring back some more transitions to add back to our buffer.
And that will refresh our buffer with behavior that hopefully gets better coverage.
All right.
So putting it all together we can devise a full kind of Q learning recipe with replay buffers.
Step one, collect a data set of transitions using some policy.
Maybe initially, it's just a random policy.
Later on, it will be the argmax policy with epsilon greedy exploration, and add this data to your buffer B.
So that's this part.
Step two, sample a batch from B, and then do some learning on that batch by summing over the batch, this Q learning pseudo gradient.
So we're going to calculate target values for every entry in the batch.
And then we're going to do the current Q value minus the target value times the derivative.
And then we'll sum it over the whole batch.
Since we sum over the whole batch, we get a lower variance gradient.
It has more than one sample.
And since we sample the batch iid from our buffer, our samples are going to be de-correlated so long as our buffer is big enough.
And we can repeat this K equals one times, K=1 is very common but larger k can be more efficient so if you repeat this k equals one times and then go out and collect more data, then you get a fairly classic kind of deep Q learning algorithm with replay buffers.