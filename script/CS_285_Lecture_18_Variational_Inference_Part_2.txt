[p.10]

Alright, so now let's get into the main technical part of today's lecture, which is to discuss the variational inference framework.

[p.11]

So this framework is basically concerned with this question, how do we calculate p(z|x_i)?
But in the process of deriving this, we'll also see why the expected log likelihood is actually a reasonable objective.
So let's think about making some crude approximation.
So p(z|x_i) is in general a pretty complex distribution, right, because a single point x might come from many different places in the space of 'z's.
But let's make a really simplistic approximation.
Let's say that we're going to approximate p(z|x_i) with some distribution q_i(z), which is a Gaussian, or in general some very simple tractable parametrized distribution class.
And notice that I'm calling it q subscript i of z.
So it's a distribution over z, and it is a distribution that's going to be specific to this point x_i.
All right, so instead of having this complicated thing, we're going to try to approximate it with just a single peak.
And I chose this picture intentionally just to make it clear that this approximation is not necessarily going to be a good one, but we'll try to find the best possible fit within this simple distribution class, the Gaussian distribution class.
It turns out that if you approximate p(z|x_i) with any q_i(z), you can actually construct a lower bound on the log probability of x_i.
And this is going to be a very powerful idea, because if you can construct a lower bound on the log probability of x_i, then maximizing that bound will push up on the log probability of x_i.
Now in general, maximizing lower bounds does not increase the quantity you care about, but if the bound is sufficiently tight than it does.
And we'll see later that under some conditions, the bound is in fact tight.
But for now, let's just not worry about tightness.
Let's just see how we can get a bound by using q_i(z).
So we can write out the log probability of x_i as the log of the integral over all values of z of p(x_i|z)⋅p(z).
And the usual trick, if you want to bring in some quantity that is not currently in the equation, is to multiply by that quantity divided by itself.
So q_i(z)/q_i(z) = 1.
So we can multiply that in whenever we want.
So now we can notice that we have some quantity multiplied by q_i(z).
So we can write that quantity as an expected value under q_i(z).
So we basically take the numerator in that ratio.
That becomes, that turns into an expectation.
And then everything else is left behind.
So we have log of the expected value under q_i(z) of {p(x_i|z)⋅p(z)}/q_i(z).

[p.12]

All right.
So, so far, we haven't made any approximation.
These are just, this is just a little bit of algebraic manipulation.
Next, what we're going to do is we're going to use Jensen's inequality.
Jensen's inequality is a way to relate convex or concave functions applied to linear combinations.
So what I have written out on this slide is a special case of Jensen's inequality for the logarithm, which is a concave function.
But in general, this inequality would hold true for any concave function.
If you have a convex function, then it holds true, but the inequality goes the other way.
So for the case of logarithms, Jensen's inequality says that the logarithm of an expected value over some variable y is greater than or equal to the expected value of the logarithm of that variable.
If this seems a little counterintuitive to you, something you could consider is, is trying to draw a picture.
So the logarithm is a, is a concave function.
So it kind of goes like that.
And if you imagine the logarithm of a sum of functions, because the logarithm, the rate at which the logarithm increases always decreases, then that sum of functions, the logarithm of that sum of functions will be greater than or equal to the sum of the logarithms because of the rate of decrease.
So if this is a little unclear to you, try drawing out a picture on this, you know, of multiple different logarithm functions getting sum together.
Okay, so we can directly apply Jensen's inequality to the result from the previous slide.
And the way that we do that is by noting that we have the log of the expected value of some quantity.
So applying Jensen's inequality simply pushes the expected value outside of the log and replaces the equality with a greater than or equal to sign.
So that means that our previous result is lower bounded by the expected value under q_i(z) of the logarithm of the ratio {p(x_i|z)⋅p(z)}/q_i(z).
But now of course we know that logarithms of products can be written out as sums of logarithms.
So we can equivalently write this out as the expected value under q_i(z) of log{p(x_i|z)} + log{p(z)} minus the expected value under q_i(z) of log{q_i(z)}.
And the reason I wrote it out like this is because I want to collect all the terms that depend on p in the first part, and all the terms that depend on q in the second part.
Now the nice thing about this equation here is that everything is tractable.
And this is true for any q_i(z).
So we could just pick some random q_i(z) and we have a lower bound, although not all 'q_i's will of course lead to the best lower bounds.
But we can pick some q_i(z), sample from it to evaluate the first expectation, and then the second expectation you'll notice actually the equation for the entropy of q_i(z), which for many simple distributions like Gaussians has a closed form solution.
Okay, so we can replace that second term with just the entropy of q_i.
So maximizing this could maximize log{p(x_i)}, although as I mentioned before you need to show that the bound is not too loose.

[p.13]

Now let me make a brief aside to sort of recap some of the information theoretic quantities that we're encountering here.
Much of this we already saw, we already talked about entropy, for example, in the exploration lectures, but I just want to briefly recap it because this stuff is really important for getting a good intuition for what variational inference is actually doing.
So entropy, the entropy of some distribution, is the negative expected value of the log probability of that distribution.
And here is an equation for the entropy of a Bernoulli distribution, so the probability of a binary event, and you can see that the entropy goes up as the probability of that event approaches 0.5, and it goes down to 0 if the event is guaranteed to happen, so probability equals 1, or guaranteed not to happen, probability equals 0.
So one intuition for the entropy is how random is the random variable.
So this makes a lot of sense in the case of the Bernoulli variable here when it's 0.5.
The variable is in some sense the most random, the most unpredictable, and it has the highest entropy.
And the second intuition is how large is the log probability in expectation under itself.
So if you mostly see low log probabilities in expectation under yourself, that means that there are many, many places to which you assign roughly equal probabilities.
If you mostly see very high log probabilities, that means that you're really concentrated around a few points that you assign high probability to.
So the top example has high entropy because log probabilities are generally lower everywhere, and the bottom one has higher entropy, or lower entropy, because the log probability is very high in just a few places, and that's a low entropy distribution.
All right, so then we could ask the question for the variational lower bound that we saw on the previous slide.
What do we expect it to actually do?
So it's the expected value of some quantity plus the entropy of q_i.
So if this graph is showing p(x_i,z), so the thing inside the first expectation, you could imagine that the expected value of this function would be maximized just by putting a lot of probability mass on the tallest peak.
All right, so this is what we would get if we just maximized the first part.
We just want to find a distribution over z inside of which we have the largest values of p(x_i,z).
But we're also trying to maximize the entropy of this distribution so we don't want to make it too skinny.
If we're also trying to maximize the entropy then we want to spread it out as much as possible while still remaining in regions where p(x_i,z) is large.
So because we have that second term we get something that kind of spreads out and the intuition is that because of this, the q_i(z) that maximizes this quantity will kind of cover the p(x_i,z) distribution.

[p.14]

Now, the other concept I want to recap here is KL divergence.
KL divergence between two distributions q and p is given by the expected value under the first distribution of the log of the ratio of the probability of the first distribution divided by the second.
And again, by exploiting the fact that the logarithm of a product is a sum of logarithms, we can write this out as the expected value under q of q(x) minus the expected value under q of log{p(x)}, which we could rewrite in a manner that looks a lot more like the equation on the previous slide if we just trade places and recognize the expected value under q of log{q} is just the negative entropy.
So the KL divergence is the negative of the expected value under q of log{p(x)} and the entropy of q.
One intuition for what the KL divergence measures is how different two distributions are.
You will notice that when q and p are equal, the KL divergence is 0.
It's easy to see why it's 0 because you have q/p = 1, log{1} is 0.
And the second intuition is how small is the expected log probability of one distribution under the other minus entropy.
Now, why entropy?
Well, for the same reason that we saw before, because if you don't have the entropy term, then q will just want to sit at the most likely point under p.
But if we have the entropy, then it wants to cover it.

[p.15]

So the variational approximation says that log{p(x_i)} is greater than or equal to the expected value under q_i(z) of log{p(x_i|z)} + log{p(z)} + {the entropy of q}.
And we call this the evidence lower bound or variational lower bound, which I'm going to denote as L_i(p,q_i).
And as we saw in the previous slide, it's also the negative KL divergence.
So what makes for a good q_i(z)?
Well, the intuition is that a good q_i(z) should approximate p(z|x_i), because then you get the tightest bound.
Approximate in what sense?
Well, you can compare them in terms of KL divergence.
So you can say, well, KL divergence measures the difference between two distributions.
When the KL divergence is zero, then the two distributions are exactly equal.
So let's pick q_i to minimize the KL divergence between q_i(z) and the posterior p(z|x).
Why?
Well, because if we write out this KL divergence using the definition from before, we'll see that it is equal to the expected value under q_i(z) of log{q_i(z)/p(z|x_i)}.
Now, p(z|x_i) can be written as p(x_i,z)/p(x_i).
And since we're doing one over that, we flip the ratio and we get this equation here.
And again, applying the property that the sum of log, the log of a product is the sum of logs, we get this equation on this side.
So we have the first term, the negative expected value under q_i(z) of log{p(x_i|z)} + log{p(z)}.
Then we have the entropy term.
And then we have this prior term.
So substituting in the equation for entropy, we get this.
So that means that the KL divergence between these two quantities is equal to the negative variational lower bound plus the log probability of x_i.
Notice, however, that the log probability of x_i doesn't actually depend on q_i.
So we can rearrange the terms a little bit and we can express log{p(x_i)} as being equal to the KL divergence between q_i and p plus the evidence lower bound.
And this is not an inequality, this is all exact.
Now, we know that KL divergences are always positive.
So this is actually another way to derive the evidence lower bound, right?
Because you know that log{p(x_i)} is equal to some positive quantity plus L, which means that L is a lower bound on log{p(x_i)}.
But furthermore, this equation shows that if we drive the KL divergence to zero, then the evidence lower bound is actually equal to log{p(x_i)}, which means that minimizing that KL divergence is an effective way to tighten the bound.
So this justifies why we want to choose q_i(z) to approximate p(z|x_i).
And it also justifies why we want to use the expected log likelihood.
Because when we use the expected log likelihood, that's like taking the expectation under p(z|x), which is the point at which the bound is tightest.

[p.16]

Okay, so here's the equation we had before.
We used it to derive this bound.
And the KL divergence, we can write out like this.
This is what we saw in the previous slide.
So that means that the KL divergence is given by {the negative variational lower bound} + {this log{p(x_i)} term}.
Now log{p(x_i)} doesn't depend on q_i.
So if we want to optimize q_i over z to minimize the KL divergence, we can equivalently optimize the same evidence-lower bound.
So that's pretty appealing.
Maximizing the same evidence-lower bound with respect to q_i minimizes KL divergence and tightens our bound.
Maximizing with respect to p increases the log likelihood.
So now this immediately suggests a practical learning algorithm.
Take your variation-lower bound, your evidence-lower bound, maximize it with respect to q_i to get the tightest bound, and then maximize it with respect to p to improve your model, to improve your log likelihood, and then alternate these two steps.

[p.17]

Okay, so just to recap this, our goal is to maximize the log{p_θ(x_i)}, but that's intractable, so instead we're going to maximize the evidence-lower bound.
So for each x_i, we'll calculate the gradient with respect to the model parameters by sampling 'z's from our q_i(x_i) and then using those samples to estimate the gradient.
So this is a single sample version, so you sample one z from q_i(x_i), and then assuming your prior p(z) doesn't depend on θ, then the gradient is just the gradient of log{p_θ(x_i|z)}.
And then you improve your θ.
And then you update q_i to maximize the same evidence-lower bound.
So this is the stochastic gradient descent version of variational inference.
Just to state this again so that we're all on the same page, in order to estimate the gradient ∇_θ L_i(p,q_i), sample a z from the approximate posterior q_i, calculate the gradient ∇_θ L_i(p,q_i) as ∇_θ log{p_θ(x_i|z)}, and then take that gradient step.
And then update your q_i to maximize L_i(p,q_i).
Alright, so everything here is straightforward except for the last line.
How do you actually improve your q_i?
Well, let's say that q_i is given by a Gaussian distribution with mean μ_i and variance σ_i.
Well, you could actually calculate the gradient with respect to the mean and variance of the evidence-lower bound.
And then do gradient ascent on μ_i and σ_i.

[p.18]

So what's the problem with this?
Well, how many parameters do we have?
Remember that we have a separate q_i for every data point x_i.
And they each have a different μ and a different σ.
This is not a big deal if you have a few thousand data points, but it becomes a big problem if you have millions of data points.
And in the deep learning setting, typically we would have a very large number of data points.
So the total number of parameters if we have this Gaussian distribution is {the number of parameters in the model, θ} + ({the dimensionality of the mean} + {the dimensionality of the variance}) × {the number of data points N}.
Okay, so that's maybe a little too large.
N might be a pretty large number and this might be intractable, but remember that our intuition is that q_i(z), needs to somehow approximate the posterior p(z|x_i).
So what if instead of learning a separate q_i(z), a separate mean and variance for every data point, what if you train a separate neural network model that approximates q_i(z)?
So instead of having a separate q_i(z) for every data point x_i, we have one network q_i(z|x_i), which aims to approximate the posterior.
So then in our generative model, we would have one neural network that maps from z to x, and another neural network that maps from x to z.
And that neural network gives us a posterior estimate with a mean and variance that are given by neural network functions of x.
So that's the idea behind amortized variational inference, and that's what I'll talk about in the next part of the lecture.