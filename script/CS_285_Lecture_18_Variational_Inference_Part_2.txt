Alright, so now let's get into the main technical part of today's lecture, which is to discuss the variational inference framework.
So this framework is basically concerned with this question, how do we calculate p of z given x i?
But in the process of deriving this, we'll also see why the expected log likelihood is actually a reasonable objective.
So let's think about making some crude approximation.
So p of z given x i is in general a pretty complex distribution, right, because a single point x might come from many different places in the space of z's.
But let's make a really simplistic approximation.
Let's say that we're going to approximate p of z given x i with some distribution q i of z, which is a Gaussian, or in general some very simple tractable parametrized distribution class.
And notice that I'm calling it q subscript i of z.
So it's a distribution over z, and it is a distribution that's going to be specific to this point x i.
All right, so instead of having this complicated thing, we're going to try to approximate it with just a single peak.
And I chose this picture intentionally just to make it clear that this approximation is not necessarily going to be a good one, but we'll try to find the best possible fit within this simple distribution class, the Gaussian distribution class.
It turns out that if you approximate the distribution class, you're going to get a very simple distribution class.
So if you approximate the distribution class, you're going to get a very simple distribution class.
So this is a distribution that's going to give a very simple distribution class.
And if you approximate p of z given x i with any q i z, you can actually construct a lower bound on the log probability of x i.
And this is going to be a very powerful idea, because if you can construct a lower bound on the log probability of x i, then maximizing that bound will push up on the log probability of x i.
Now in general, maximizing lower bounds does not increase the quantity you care about, but if the bound is sufficiently tight, you will be able to apply the distribution class to a greater probability of x i.
tight than it does.
And we'll see later that under some conditions, the bound is in fact tight.
But for now, let's just not worry about tightness.
Let's just see how we can get a bound by using Q I of Z.
So we can write out the log probability of X I as the log of the integral over all values of Z of p(x) I given Z times P of Z.
And the usual trick, if you want to bring in some quantity that is not currently in the equation, is to multiply by that quantity divided by itself.
So Q I Z over Q I Z is equal to one.
So we can multiply that in whenever we want.
So now we can notice that we have some quantity multiplied by Q I of Z.
So we can write that quantity as an expected value under Q I of Z.
So we basically take the numerator in that ratio.
That becomes, that turns into an expectation.
And then everything else is left behind.
So we have log of the integral over all values of p(x) I.
So we can write that quantity as an expected value under Q I of Z of P X I given Z times P of Z divided by Q I of Z.
All right.
So, so far, we haven't made any approximation.
These are just, this is just a little bit of algebraic manipulation.
Next, what we're going to do is we're going to use Jensen's inequality.
Jensen's inequality is a way to relate convex or concave functions applied to linear combinations.
So what I have written out on this slide is a special case of Jensen's inequality for the logarithm, which is a concave function.
But in general, this inequality would hold true for any concave function.
If you have a convex function, then it holds true, but the inequality goes the other way.
So for the case of logarithms, Jensen's inequality says that the logarithm of an expected value over some variable Y is greater than or equal to the expected value of the logarithm of that variable.
If this seems a little counterintuitive to you, something you could consider is, is trying to draw a picture.
So the logarithm is a, is a concave function.
So it kind of goes like that.
And if you imagine the logarithm of a sum of functions, because the logarithm, the rate at which the logarithm increases always decreases, then that sum of functions, the logarithm of that sum of functions will be greater than or equal to the sum of the logarithms because of the rate of decrease.
So if this is a little unclear to you, try drawing out a picture on this, you know, of multiple different logarithm functions, and you can see that the logarithm of the expected value of the expected value of the expected value is greater than or equal to the sum of functions.
So that's the way that we can get the sum together.
Okay, so we can directly apply Jensen's inequality to the result from the previous slide.
And the way that we do that is by noting that we have the log of the expected value of some quantity.
So applying Jensen's inequality simply pushes the expected value outside of the log and replaces the equality with a greater than or equal to sign.
If we integrate the equity as theory along with the given vì, we'll get that theável thing that's left outside the boundary of the too young me shall be 0.
You can also use the Nash theorem 0.0604.
And the value of the expected value implies, so this is that value, than or equal to 2, it out like this is because I want to collect all the terms that depend on p in the first part, and all the terms that depend on q in the second part.
Now the nice thing about this equation here is that everything is tractable.
And this is true for any qi of z.
So we could just pick some random qi of z and we have a lower bound, although not all qis will of course lead to the best lower bounds.
But we can pick some qi of z, sample from it to evaluate the first expectation, and then the second expectation you'll notice actually the equation for the entropy of qi of z, which for many simple distributions like Gaussians has a closed form solution.
Okay, so we can replace that second term with just the entropy of qi.
So maximizing this could maximize log p xi, although as I mentioned before you need to show that the bound is not too loose.
Now let me make a a a a make a brief aside to sort of recap some of the information theoretic quantities that we're encountering here.
Much of this we already saw, we already talked about entropy, for example, in the exploration lectures, but I just want to briefly recap it because this stuff is really important for getting a good intuition for what variational inference is actually doing.
So entropy, the entropy of some distribution, is the negative expected value of the log probability of that distribution.
And here is an equation for the entropy of a Bernoulli distribution, so the probability of a binary event, and you can see that the entropy goes up as the probability of that event approaches 0.5, and it goes down to 0 if the event is guaranteed to happen, so probability equals 1, or guaranteed not to happen, probability equals 0.
So one intuition for the entropy is how random is the random variable.
So this makes a lot of sense in the case of the Bernoulli variable here when it's 0.5.
The variable is in some sense the most random, the most unpredictable, and it has the highest entropy.
And the second intuition is how large is the log probability in expectation under itself.
So if you mostly see low log probabilities in expectation under yourself, that means that there are many, many places to which you assign roughly equal probabilities.
If you mostly see very high log probabilities, that means that you're really concentrated around a few points that you assign high probability to.
So the top example has high entropy because log probabilities are generally lower everywhere, and the bottom one has higher entropy, or lower entropy, because the log probability is very high in just a few places, and that's a low entropy distribution.
All right, so then we could ask the question for the variational lower bound that we saw on the previous slide.
What do we expect it to actually do?
So it's the expected value of some quantity plus the entropy of qi.
So if this graph is showing pxi comma z, so the thing inside the first expectation, you could imagine that the expected value of this function would be maximized just by putting a lot of probability mass on the tallest peak.
All right, so this is what we would get if we just maximized the value of the expected value of this function.
So we could imagine that the expected value of this function would be maximized just by putting a lot of probability mass on the tallest peak.
right.
p of vi and c of xi can mainlyzieh the'vep π and y of t volume is equal to intended volume, which are the properties of no, not the Fang talent of theíp shape of the mask rot.
Here's the intake function part where the registers andvous are frozen add that because of this, the QI of Z that maximizes this quantity will kind of cover the PXI comma Z distribution.
Now, the other concept I want to recap here is KL divergence.
KL divergence between two distributions Q and P is given by the expected value under the first distribution of the log of the ratio of the probability of the first distribution divided by the second.
And again, by exploiting the fact that the logarithm of a product is a sum of logarithms, we can write this out as the expected value under Q of Q of X minus the expected value under Q of log P of X, which we could rewrite in a manner that looks a lot more like the equation on the previous slide if we just trade places and recognize the expected value of the Q of P of X.
So the expected value under Q of log Q is just the negative entropy.
So the KL divergence is the negative of the expected value under Q of log p(x) and the entropy of Q.
One intuition for what the KL divergence measures is how different two distributions are.
You will notice that when Q and P are equal, the KL divergence is zero.
It's easy to see why it's zero because you have Q over P equals one, log of one is zero.
And the second intuition is how small is the expected log probability of one distribution under the other minus entropy.
Now, why entropy?
Well, for the same reason that we saw before, because if you don't have the entropy term, then Q will just want to sit at the most likely point under P.
But if we have the entropy, then it wants to cover it.
So the variational approximation says that log P of XI is greater than the expected value under Q of Q of X.
So the Q of Q of X is greater than or equal to the expected value under Q of log P of XI given Z plus log P of Z plus the entropy of Q.
And we call this the evidence lower bound or variational lower bound, which I'm going to denote as LI of P comma QI.
And as we saw in the previous slide, it's also the negative KL divergence.
So what makes for a good QI of Z?
Well, the intuition is that a good QI of Z should approximate the expected value of the expected value of the expected value of the expected value of P of Z given XI, because then you get the tightest bound.
Approximate in what sense?
Well, you can compare them in terms of KL divergence.
So you can say, well, KL divergence measures the difference between two distributions.
When the KL divergence is zero, then the two distributions are exactly equal.
So let's pick QI to minimize the KL divergence between QI of Z and the posterior P of Z given X.
Why?
Well, because if we write out this KL divergence using the definition from before, we'll see that it is equal to the expected value under QI of Z of log QI of Z divided by P of Z given XI.
Now, P of Z given XI can be written as P of XI comma Z divided by P of XI.
And since we're doing one over that, we flip the ratio and we get this equation here.
And again, applying the property that the sum of log, the log of a product is the sum of logs, we get this equation on this side.
So we have the first term, the negative expected value under QI of Z of log PXI given Z plus log P of Z.
Then we have the entropy term.
And then we have this prior term.
So substituting in the equation for entropy, we get this.
So that means that the KL divergence between these two quantities is equal to the expected value of the QI of Z of log PXI given Z plus log P of Z.
And then we have the second term, the negative expected value of the QI of Z of log PXI given Z plus log PXI given Z.
Notice, however, that the log probability of XI doesn't actually depend on QI.
So we can rearrange the terms a little bit and we can express log P of XI as being equal to the KL divergence between QI and P plus the evidence lower bound.
And this is not an inequality, this is all exact.
Now, we know that KL divergences are always positive.
So this is actually another way to derive the evidence lower bound, right?
Because you know that log P of XI is equal to some positive quantity plus L, which means that L is a lower bound on log P of XI.
But furthermore, this equation shows that if we drive the KL divergence to zero, then the evidence lower bound is actually equal to log P of XI, which means that minimizing that KL divergence is an effective way.
So we can use the expected log likelihood to justify the P of Z given XI to tighten the bound.
So this justifies why we want to choose QI of Z to approximate P of Z given XI.
And it also justifies why we want to use the expected log likelihood.
Because when we use the expected log likelihood, that's like taking the expectation under P of Z given X, which is the point at which the bound is tightest.
Okay, so here's the equation we had before.
So we can see that the P of Z given XI is equal to the KL divergence.
And this is what I saw before.
We used it to derive this bound.
And the KL divergence, we can write out like this.
This is what we saw in the previous slide.
So that means that the KL divergence is given by the negative variational lower bound plus this log P of XI term.
Now log P of XI doesn't depend on QI.
So if we want to optimize QI over Z to minimize P of XI, minimize the KL divergence, we can equivalently optimize the same evidence-lower bound.
So that's pretty appealing.
Maximizing the same evidence-lower bound with respect to QI minimizes KL divergence and tightens our bound.
Maximizing with respect to P increases the log likelihood.
So now this immediately suggests a practical learning algorithm.
Take your variation-lower bound, your evidence-lower bound, maximize it with respect to QI to get the tightest bound, and then maximize it with respect to P to improve your model, to improve your log likelihood, and then alternate these two steps.
Okay, so just to recap this, our goal is to maximize the log P theta of Xi, but that's intractable, so instead we're going to maximize the evidence-lower bound.
So for each Xi, we'll calculate the gradient with respect to the model parameter, and then we'll use the gradient to estimate the gradient.
So this is a single sample version, so you sample one Xi from QI of Xi, and then assuming your prior P of Xi doesn't depend on theta, then the gradient is just the gradient of log P theta Xi given that Xi.
And then you improve your theta.
And then you update QI to maximize the same evidence-lower bound.
So this is the stochastic gradient descent version of variational inference.
Just to state this again so that we're all on the same page, in order to estimate the gradient grad theta of Li P comma QI, sample a Z from the approximate posterior QI, calculate the gradient grad theta Li P comma QI as grad theta log P theta Xi given Z, and then take that gradient step.
And then update your QI to the gradient gradient descent version of variational inference.
So this is the stochastic gradient descent version of variational inference.
And then you update your QI to maximize Li P comma QI.
Alright, so everything here is straightforward except for the last line.
How do you actually improve your QI?
Well, let's say that QI is given by a Gaussian distribution with mean mui and variance sigma i.
Well, you could actually calculate the gradient with respect to the mean and variance of the evidence-lower bound.
And then do gradient descent on mui and sigma i.
So what's the problem with this?
Well, how many parameters do we have?
Remember that we have a separate QI for every data point Xi.
And they each have a different μ and a different sigma.
This is not a big deal if you have a few thousand data points, but it becomes a big problem if you have millions of data points.
And in the deep learning setting, typically we would have a very large number of data points.
So the total number of parameters if we have this Gaussian distribution is the number of parameters in the Model, theta.
plus the dimensionality of the mean plus the dimensionality of the variance, times the number of data points N.
Okay, so that's maybe a little too large.
N might be a pretty large number and this might be intractable, but remember that our intuition is that Q , needs to somehow approximate the posterior p of z given xi.
So what if instead of learning a separate qi of z, a separate mean and variance for every data point, what if you train a separate neural network model that approximates qi of z?
So instead of having a separate qi of z for every data point xi, we have one network q of z given xi, which aims to approximate the posterior.
So then in our generative model, we would have one neural network that maps from z to x, and another neural network that maps from x to z.
And that neural network gives us a posterior estimate with a mean and variance that are given by neural network functions of x.
So that's the idea behind amortized variational inference, and that's what I'll talk about in the next part of the lecture.