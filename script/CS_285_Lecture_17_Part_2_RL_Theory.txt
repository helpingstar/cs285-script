[p.14]

In the second part of today's lecture, we're going to do some theoretical analysis of a model-free reinforcement learning algorithm that is kind of similar to one that we might actually want to use, namely fitted Q iteration.
Now of course we know that real fitted Q iteration in general is not guaranteed to converge, so we're going to use a kind of an idealized model of fitted Q iteration that is, you know, a little more simplified than the real method but is amenable to theoretical analysis.

[p.15]

So here is our abstract model of exact Q iteration.
In exact Q iteration we're going to, at every iterate, set Q_{k+1} to be equal to some operator T⋅Q_k.
The operator T is going to be the Bellman optimality operator, so T⋅Q = r + γ⋅P⋅max_a{Q}.
So the the max operator here is kind of weird because Q is an S-A length matrix and we're gonna say that max_a of that S-A length vector results in a new vector that is of length S.
So it's max_a but it's kind of this blockwise max where over all the actions corresponding to the same states it computes one entry which is the max over those actions.
So this is not a full max over the whole vector Q, it's actually this block max.
So there's a little bit of notational convenience.
Anyway don't worry too much if my notation here is confusing, TQ is basically exactly what you think it is, it's just the thing that takes a Q function and performs a max Bellman backup.
Now that's exact Q-iteration.
Here's how we're going to model approximate fitted Q-iteration.
We're going to say that ^{Q}_{k+1} is going to perform some kind of minimization over ^{Q} to minimize ∥^{Q} - ^{T}⋅^{Q}_k∥.
So there are going to be two sources of error here.
One is that ^{T} is not the same as T.
I'll talk about that in a second.
And the other one is that the minimization will not be exact either.
So ^{Q}_{k+1} will not actually be equal to ^{T}⋅^{Q}_k.
So we're going to have an approximate Bellman operator, and we're going to have an approximate minimization.
So the approximate Bellman operator, ^{T}⋅Q, is going to be equal to ^{r} + γ⋅^{P}.
Let's unpack this a little bit.
So ^{r} is, in this case, for the purpose of this analysis, just the reward averaged over all the times when we've seen the state {s,a}.
So the value in ^{r}(s,a) is just the sample average, 1 over the number of times we've seen (s,a), times the sum over all of our samples of r_i for every r_i that whose (s_i,a_i) corresponds to (s,a) so it's basically exactly what you think it is it's just the average reward we've seen for that state action tuple.
And ^{P}, basically the same as before, is the number of times we've seen the transition (s,a,s') divided by the number of times we've seen (s,a).
Now this might look like the same kind of idea that we had in the model-based analysis before, but note that these are not models.
This is the effect of averaging together different transitions in the data.
So what we would do in a real fitted q-iteration algorithm is we would have different losses for every single sample.
So for every sample, we would have something like Q(s_i,a_i) - r_i + γ⋅max_{a'}{Q(s'_i, a'_i)} squared, or some other difference, maybe not squared, maybe absolute value.
And we would average them all together.
So what we're doing in this idealized model is we're basically saying that the effect of averaging together these different losses kind of looks like doing a backup under this empirical model.
So ^{P} and ^{r} are basically empirical models of the reward and transitions.
Technically, this is what you would get if you were to average together all the target values for a given state action tuple (s,a), right?
So if you've seen a given (s,a) five times and you average together the target values for those five instances of that same (s,a), you would get exactly the same thing as if you were to employ this version of ^{r} and ^{P}.
Okay.
Now I'm saying all of this, but this is just kind of justification for this idealized model.
So from here on out, we're just going to deal with '^{r}'s and '^{P}'s.
So all that explanation was just to justify why viewing approximate fittted Q iteration as doing this ^{T} backup is reasonable.
It's reasonable because if you were to just average together the target values for that state action tuple over all the samples that contain that state action tuple, this is exactly what you would get.
Okay.
Now at this point, what we're actually going to see in our analysis is that the fact that ^{r} and ^{P} are not exactly the same as r and P is going to induce some error.
And we call that sampling error because the reason for the error is that ^{r} and ^{P} are inexact because we have a finite number of samples.
If we had an infinite number of samples, then ^{r} would be equal to r and ^{P} would be equal to p.
But for a finite number of samples, we incur sampling error.
But that's not the only source of error.
This minimization will also be inexact.
So we won't actually be able to get ^{Q}_{k+1} to perfectly match ^{T}⋅^{Q}_k because it's fitted Q iteration and there's some kind of function approximation or some kind of, you know, inexact learning going on.
So we need some model for this error.
And an important thing here is which norm we're going to use.
So we saw before in our discussion of, fitted Q iteration back in the beginning of the course, that if we do this with squared error, the problem is that we can't even prove convergence of the algorithm.
Now, that's a significant problem.
But even though we can't prove convergence of the real algorithm, maybe we can assume some kind of idealized algorithm and at least study how error in this idealized algorithm depends on the problem parameters.
So we need to idealize this a little bit.
And in order to idealize it, we're going to assume that we're actually minimizing the infinity norm.
And furthermore, we're going to assume that we can get the infinity norm to be less than or equal to some constant.
So we'll assume that every iteration, we compute this approximate Bellman backup ^{T}⋅^{Q}_k, and then we fit the new ^{Q}_{k+1} to this ^{T}⋅^{Q}_k with an infinity norm that is less than or equal to some constant.
And I'll come back to this later.
So this is, this assumption is kind of made out of convenience because it's difficult to do this with the L2 norm.
Okay, so which Questions are we gonna want to study?
Well, one question is, as the number of fitted Q iteration iterations approaches infinity, what is ^{Q}_k actually approach?
In particular?
How much does ^{Q}_k differ from Q^{*}? asymptotically as we take infinitely many approximate fitter Q iteration, iterations?
Well, where will our errors come from?
They'll come from two sources.
One is that T ≠ ^{T}.
So that's sampling error, and the other one is that ^{Q}_{k+1} ≠ ^{T}⋅^{Q}_k and that's we call it approximation error.
Basically, when we approximate the backed up previous Q function meaning the target values with some new Q function we incur some approximation error and we can try to quantify that.

[p.16]

Okay.
So, let's first analyze just the sampling error.
So we'll just analyze the problems we get from the fact that T ≠ ^{T} and this is going to look pretty similar to what we saw in the previous section so we're basically going to figure out how the real thing that we're doing the one that has ^{T}⋅^{Q} is different from what we would have gotten if we used T⋅^{Q}.
So in particular we want to understand this difference.
How different will ^{T}⋅Q be from T⋅Q for some Q function Q.
We don't care what Q is at this point we just want to understand the difference between applying ^{T} to it and applying T to it.
So if we were to write that out well let's just substitute in the definition of ^{T} and T in there, and we'll collect the terms so we'll collect the r terms and the next value terms we get ^{r} minus r plus γ times the expected value of the max under ^{P} minus the expected value of the max under p.
And by the triangle inequality as usual we can bound this the norm of this sum with the sum of their norms so we get ∣^{r} - r∣ plus γ times the norm of the difference of expectations.
Now the first value here is exactly the estimation error of a continuous random variable.
What did we learn about that allows us to bound the approximation error of a continuous random variable?
Well this is exactly Hoeffding's inequality.
So if we just directly plug in the formula for Hoeffding's inequality we get that the difference between ^{r} and r is just going to be less than or equal to 2 times the max of impossible reward, that's basically the range of rewards of the b plus minus b minus, times the square root of 1 over δ over 2n.
So our familiar bound, the error will scale as 1 over root n.
For the second part this is just the sum over all next states of the difference of ^{P} and P times the maximum over the action of Q(s',a').
And we can bound that by a replacing the max_{a'} with a max_{s',a'}.
So that'll make this this Q term independent of s', right?
Because if you average together the you know the values of some vector that's the same that that's bounded by summing together the maximum values of that vector because any entry in the vector is less than or equal to its maximum.
And now looking at this equation hopefully you'll recognize this as the total variation divergence between ^{P} and P times some quantity that depends on Q, some constant quantity.
And in particular that constant quantity is just the infinity norm of Q.
So this is exactly equal to the total variation divergence between ^{P} and P times the infinity norm of q.
And we already had a bound for that so that's basically going to use that concentration inequality for estimating categorical distributions and therefore this is bounded by some constant times the infinity norm of Q times the square root of the log{1/δ}/N.
So again it scales as root N and there's some constant that comes from the dimensionality and so on and the infinity norm of Q.
All right.
So that's sampling error.
And this is you know more or less following the same logic as we had in previous sections.

[p.17]

So, what we have is that the difference between applying these empirical Bellman backup, the approximate backup and the true backup is bounded by two terms, one that depends on the error in the reward and the other one that depends on the error in the dynamics.
So that means that the infinity norm of the difference between ^{T}⋅Q and TQ is basically also going to have this form just with slightly different constants and with some terms that depend on the number of states and actions, and we get that by using the union bound.
Remember the reason that we need to use the union bound is that these inequalities all hold with probability {1-δ}, so if you have n different events then you need to bound the probability of all of those events happening, and that's what the union bound does.
But you don't really have to worry about this, all this really changes as the constants.

[p.18]

Okay so that's sampling error.
Now what about approximation error?
Well let's make some assumption.
Let's assume that when you fit ^{Q}_{k+1} to the target values, which we're going to say are T⋅^{Q}_k, your fit has an infinity norm error of at most ϵ_k.
And for now we're going to analyze the case where you have an exact backup, but we'll come back to the approximate backup later.
So let's just pretend for a minute that our backup is exact, so there's no difference between T and ^{T}, and we're just studying the effect of error in the fit.
So if we had an exact fit, if we had an exact tabular Q iteration method, then ^{Q}_{k+1} would be exactly equal to T⋅^{Q}_k.
And now we're going to assume that it's not exact, that it incurs some error, and that that error is bounded in the infinity norm.
Now this is a strong assumption.
In reality, if you're doing supervised learning, your error is not bounded in the infinity norm.
It's going to be bounded in something like a weighted L2 norm in expectation under some distribution.
We're going to assume it's bounded in the infinity norm, which means that for the worst possible state action tuple, your error is at most ϵ_k.
So this is a strong assumption, but it will make this very convenient.
Okay, so now what we're going to try to understand is the difference between ^{Q}_k at some iteration k and the real Q^{*}, again in the infinity norm.
So here's how we're going to do it.
We're going to use the same trick as before.
We're going to subtract and add some quantity, and the particular quantity that we're going to put in is T⋅^{Q}_{k-1}.
The reason is that, well, we're fitting ^{Q}_k to T⋅^{Q}_{k-1}, so if we put that in, that's the quantity we can bound.
So we're going to subtract that, and we're going to add it, and then we'll group these two terms together.
So we're going to have one term, which is ^{Q}_k - T⋅^{Q}_{k-1}.
So that's convenient, because that's the quantity that we're going to be bounding by assumption.
And then we have this other term, which consists of the backup of ^{Q}_{k-1} minus the backup of Q^{*}.
Now, Q^{*}, is the fixed point of T, so you can always replace Q^{*} with T⋅Q^{*}, which is what I did on this line.
And then we'll apply the triangular equality again to bound the infinity norm of the sum by the sum of their infinity norms.
And the first term here, ^{Q}_k - T⋅^{Q}_{k-1}, is just bounded by ϵ_{k-1} by our assumption at the top.
The indexing is off by 1, that's why it's {k-1}.
So that leaves us with a second term.
Now, for the second term, we're going to recognize an interesting fact that we saw way back in the day when we first learned about Q learning, which is that the Bellman backup is a contraction.
The fact that the Bellman backup is a contraction in the infinity norm means that the infinity norm of what you get by applying T to two different Q functions is less than or equal to γ times the infinity norm of the difference between those Q functions.
So using the fact that T is a contraction in γ, we can bound this by γ times ^{Q}_{k-1} minus Q^{*}.
And now what we've done is we've related ^{Q}_k - Q^{*} recursively to ^{Q}_{k-1} - Q^{*}, but with the addition of this little error term ϵ_{k-1}.

[p.19]

So now we're going to unroll this recursion.
So applying the same thing again to ^{Q}_{k-1} minus Q^{*}, we bound the whole thing by ϵ_{k-1} + γ⋅ϵ_{k-2} + γ^2 times the difference.
Then we're going to do it again, and we get γ^2⋅ϵ_{k-3} + γ^3, and so on and so on.
If we go all the way back to the beginning, then we end up bounding the whole thing by this γ discounted sum from i=0 to k-1, γ^i times the corresponding ϵ plus γ^k times the difference between ^{Q}_0, and Q^{*}.
Now, this tells us something very interesting, and also very useful, which is that the more iterations we take, the more we essentially forget our initialization.
Because as k goes to infinity, this γ^k term will vanish, because γ is less than 1, which means that the effect of our starting point, ^{Q}_0, is going to vanish.
So what that means is that if we take the limit as k goes to infinity, the second term γ^k vanishes, because γ^k approaches 0, and for the first term, we're going to simplify it a little bit.
We're going to replace all those ϵ_{k-i-1} terms with just the maximum ϵ we get over all the iterations.
That's probably reasonable to do, because if our fitting error is bounded for every iteration, we'll just say that we can also bound it over all iterations.
And now we get our familiar geometric series, the sum from i=0 to infinity of γ^i times some constant.
So that's 1/{1-γ}⋅∥ϵ∥_∞, where ϵ is a big vector with k dimensions, where every dimension has the error at that iteration.
So that's pretty neat.
Now we see how error scales for just the approximation error.
So if we incur some ϵ fit every step, then the total error we'll get will be ϵ times 1/{1-γ}.
So the longer our horizon essentially the more error we get, get, which intuitively kind of you can think of as saying that every time you back up, which is every iteration of fitted Q iteration, you incur some additional error.
So since the number of backups you need to make is equal to your horizon, that's the order of the approximation error that you'll see.

[p.20]

So now let's put these two things together.
We've got our sampling error, and that quantifies the difference between ^{T} and T, and we've got our approximation error, which is how much ^{Q}_{k+1} will differ from T⋅^{Q}_k, and they'll differ due to sampling error and due to the approximation error.
So essentially what we're going to do is we're going to subsume the sampling error inside the 'ϵ's, and that will let us connect these two parts up.
So put it stated another way, that bound from the previous slide can also be rewritten as the limit as k goes to infinity of ^{Q}_k minus Q^{*} is less than or equal to 1/{1-γ} times the max over all of your iterations of the difference between ^{Q}_k and T⋅^{Q}_{k-1}.
So this contains both kinds of errors because there's a T in there, so if you're actually backing up using ^{T} instead of T, you'll have an error there, and it contains the approximation error due to an imperfect fit.
So let's just examine what this quantity is.
^{Q}_k minus T⋅^{Q}_{k-1}.
So we're going to put in ^{T}⋅^{Q}_{k-1}, so we'll subtract and add up the same trick as before.
We'll again group the terms.
So we're bounding this whole thing by ∥^{Q}_k - ^{T}⋅^{Q}_{k-1}∥_∞ + ∥^{T}⋅^{Q}_{k-1} - T⋅^{Q}_{k-1}∥_∞.
So the second term is basically taking care of the sampling error, and the first term is taking care of the approximation error.
So we know that the first term is that ϵ_k, and the second term is that big sampling error bound up above.
So what we can do is we can just take these two terms and plug them in here, and we can use that to calculate the difference between ^{Q}_k and Q^{*} in the limit as k goes to infinity, and it will be 1/{1-γ} times a bunch of terms, basically a sum of three terms, two of them coming from sampling error and one coming from approximation error.

[p.21]

So here's what we have on the previous slide.
We can see here that error compounds with the horizon over iterations, and due to sampling.
Notice that in the sampling error, the second term actually is also of order 1/{1-γ}, because ∥Q∥_∞ is on the order of R_{max}⋅1/{1-γ}.
We discussed this before.
We talked about how the value functions and Q functions, basically their magnitudes are the reward times the horizon, so it's R_{max}/{1-γ}.
So if you imagine what will happen if we substitute in ϵ_k plus sampling error into that second equation, you have a 1/{1-γ} term in front, and then you have a sum of three terms, one of which itself also has a 1/{1-γ} term in it.
So the overall order of the error will be quadratic in 1/{1-γ}, just like we saw in part one.
Now so far we needed strong assumptions, specifically infinity norm assumptions, on the error that we're incurring.
So that is a fairly strong assumption that is not always going to hold.
More advanced results can actually be derived with p-norms under some distributions.
So infinity norms are not really realistic for practical learning algorithms.
It is possible to do some analysis with p-norms, and you can learn more about that in the RL theory book referenced at the bottom of the slide.
Basically this analysis studies norms of this form, where the p,mu norm is just the expected value under μ of the difference raised to the power p, and then the whole thing is raised to 1/p.
So if p is equal to 2, this is actually the quadratic Bellman error that we're used to.
So there's some analysis that we can do with that, but we need some assumptions there too to avoid this non-convergence issue.