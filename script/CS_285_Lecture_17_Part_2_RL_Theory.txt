In the second part of today's lecture, we're going to do some theoretical analysis of a model-free reinforcement learning algorithm that is kind of similar to one that we might actually want to use, namely fitted Q iteration.
Now of course we know that real fitted Q iteration in general is not guaranteed to converge, so we're going to use a kind of an idealized model of fitted Q iteration that is, you know, a little more simplified than the real method but is amenable to theoretical analysis.
So here is our abstract model of exact Q- iteration.
In exact Q iteration we're going to, at every iterate, set Q to be equal to some operator T times Q .
The operator T is going to be the Bellman optimality operator, so T times Q is equal to R plus γ times P times the max over A of four-dimensional.
So the the max operator here is kind of weird because Q is an S-A length matrix and we're gonna say that max A of that S-A length vector results in a new vector that is of length S.
So it's max A but it's kind of this blockwise max where over all the actions corresponding to the same states it computes one entry which is the max over those actions.
So this is not a full max over the whole vector Q, it's actually this block max.
So there's a little bit of notational convenience.
Anyway don't worry too much if my notation here is confusing, TQ is basically exactly what you think it is, it's just the thing that takes a Q function and performs a max Bellman backup.
Now that's exact Q-iteration.
Here's how we're going to model approximate fitted Q-iteration.
We're going to say that Q hat is going to perform some kind of minimization over Q hat to minimize Q hat minus T hat over T hat over T hat.
We're going to say that Q hat is going to perform some kind of minimization over Q hat over T hat over T hat over T hat.
q hat k.
So there are going to be two sources of error here.
One is that t hat is not the same as t.
I'll talk about that in a second.
And the other one is that the minimization will not be exact either.
So q hat k plus 1 will not actually be equal to t hat q hat k.
So we're going to have an approximate Bellman operator, and we're going to have an approximate minimization.
So the approximate Bellman operator, t hat q, is going to be equal to r hat plus γ p hat.
Let's unpack this a little bit.
So r hat is, in this case, for the purpose of this analysis, just the reward averaged over all the times when we've seen the state S a.
So the value in r hat for S a is just the sample average, 1 over the number of times we've seen S a, times the sum over all of our samples of r hat.
So that's the average reward we've seen for that state action tuple.
And p hat, basically the same as before, is the number of times we've seen the transition S a s' divided by the number of times we've seen S a.
Now this might look like the same kind of idea that we had in the model-based analysis before, but note that these are not models.
This is the effect of averaging together different transitions in the data.
So what we would do in a real fitted q-iteration algorithm is we would have different losses for every single sample.
So for every sample, we would have something like q S i a i minus r i plus γ max a prime q S i prime a prime squared, or some other difference, maybe not squared, maybe absolute value.
And we would average them all together.
So what we're doing in this idealized model is we're basically saying that, you know, we're going to have a very simple, very simple, very simple, very simple, very simple, very simple, very simple model.
The effect of averaging together these different losses kind of looks like doing a backup under this empirical model.
So ^{p} and r hat are basically empirical models of the reward and transitions.
Technically, this is what you would get if you were to average together all the target values for a given state action tuple S a, right?
So if you've seen a given S a five times and you average together the target values for those five instances of that same S a, you would get exactly the same thing as if you were to employ this version of r hat and p hat.
Okay.
Now I'm saying all of this, but this is just kind of justification for this idealized model.
So from here on out, we're just going to deal with r hats and p hats.
So all that explanation was just to justify why viewing approximate fit of q iteration as doing this t hat backup is reasonable.
It's reasonable because if you were to just average together the target values for that state action tuple over all the samples that contain that state action tuple, this is exactly what you would get.
Okay.
Now at this point, what we're actually going to see in our analysis is that the fact that r hat and ^{p} are not exactly the same as r and p is going to induce some error.
And we call that sampling error because the reason for the error is that r hat and ^{p} are inexact because we have a finite number of samples.
If we had an infinite number of samples, then r hat would be equal to r and ^{p} would be equal to p.
But for a finite number of samples, we incur sampling error.
But that's not the only source of error.
This minimization will also be inexact.
So we won't actually be able to get q hat k plus one to perfectly match t hat q hat k because it's fit of q iteration and there's some kind of function approximation or some kind of, you know, inexact learning going on.
So we need some model for this error.
And an important thing here is which norm we're going to use.
So we saw before in our discussion of, fit of q iteration back in the beginning of the course, that if we do this with squared error, the problem is that we can't even prove convergence of the algorithm.
Now, that's a significant problem.
But even though we can't prove convergence of the real algorithm, maybe we can assume some kind of idealized algorithm and at least study how error in this idealized algorithm depends on the problem parameters.
So we need to idealize this a little bit.
And in order to idealize it, we're going to assume that we're actually minimizing the infinity norm.
And first of all, we're going to assume that we're actually minimizing the infinity norm.
Furthermore, we're going to assume that we can get the infinity norm to be less than or equal to some constant.
So we'll assume that every iteration, we compute this approximate Bellman backup t hat q hat k, and then we fit the new q hat k plus one to this t hat q hat k with an infinity norm that is less than or equal to some constant.
And I'll come back to this later.
So this is, this assumption is kind of made out of convenience because it's difficult to do this with the L2 norm.
Okay, so which Questions are we gonna want to study?
Well, one question is, as the number of θ q iteration iterations approaches infinity, what is q hat k actually approach?
In particular?
How much does q hat k differ from q star?
asymptotically as we take infinitely many approximate fitter Q iteration, iterations?
Well, where will our errors come from?
They'll come from two sources.
One is that T is not equal to t hat.
So that's simplest, but they'll come from two sources.
One is the t is not equal to T hat.
So that's �ongjaland Emergency sed dalam hedger keep findingeu 61 s there.
Then there's them.
T is also here, but there are not great mathematics.
era error and the other one is that q hat k plus 1 is not equal to t hat q hat k and that's we call it approximation error basically when we approximate the backed up previous Q function meaning the target values with some new Q function we incur some approximation error and we can try to quantify that right so let's first analyze just the sampling error so we'll just analyze the problems we get from the fact that t is not equal to t hat and this is going to look pretty similar to what we saw in the previous section so we're basically going to figure out how the real thing that we're doing the one that has t hat q hat is different from what we would have gotten if we used t times q hat so in particular we want to understand this difference how different will t hat q be from t q for some Q function q we don't care what q is at this point we just want to understand the difference between applying t hat to it and applying t to it so if we were to write that out well let's just substitute in the denominator so let's subtract out the denominator so let's just substitute in the denominator so let's just subtract out the denominator so let's just substitute in the definition of t hat and t in there, and we'll collect the terms so we'll collect the r terms and the next value terms we get r hat minus r plus γ times the expected value of the max under ^{p} minus the expected value of the max under p.
And by the triangle inequality as usual we can bound this the norm of this sum with the sum of their norms so we get r hat minus r the norm of that plus γ times the norm of the difference of expectations.
Now the first value here is exactly the estimation error of a continuous random variable.
What did we learn about that allows us to bound the approximation error of a continuous random variable?
Well this is exactly Hoeffding's inequality.
So if we just directly plug in the formula for Hoeffding's inequality we get that the difference between r hat and r is just going to be less than or equal to 2 times the max of impossible reward, that's basically the range of rewards of the b plus minus b minus, times the square root of 1 over δ over 2n.
So our familiar bound, the error will scale as 1 over root n.
For the second part this is just the sum over all next states of the difference of ^{p} and p times the maximum over the action of qs prime a prime.
And we can bound that by a replacing the max over a prime with a max over s' and a prime.
So that'll make this this q term independent of s', right?
Because if you average together the you know the values of some vector that's the same that that's bounded by summing together the maximum values of that vector because any entry in the vector is less than or equal to its maximum.
And now looking at this equation hopefully you'll recognize this as the total variation divergence between ^{p} and p.
So we can just use the constant quantity of q times some quantity that depends on q, some constant quantity.
And in particular that constant quantity is just the infinity norm of q.
So this is exactly equal to the total variation divergence between ^{p} and p times the infinity norm of q.
And we already had a bound for that so that's basically going to use that concentration inequality for estimating categorical distributions and therefore this is bounded by some constant times the infinity norm of q times the square root of the log of q.
And we can literally dec огромal divide the past PETER by d seldom, okay?
So π plus q times the of rho times the valor of change of neutral Hello the answer is 1 or δ 0.
Alright, so it's 100 billion unit vector times the base sale over d integral is 0.
And for where does this巨 fun this struggling argument or v hors there's some constant that comes for the optimality and so on and the infinity norm of q.
So write that statement.
And this is you know more or less following the same logic as we had in previous sections.
So, what we have is that the difference between applying these empirical Bellman backup, by two terms, one that depends on the error in the reward and the other one that depends on the error in the dynamics.
So that means that the infinity norm of the difference between t hat q and tq is basically also going to have this form just with slightly different constants and with some terms that depend on the number of states and actions, and we get that by using the union bound.
Remember the reason that we need to use the union bound is that these inequalities all hold with probability 1 minus delta, so if you have n different events then you need to bound the probability of all of those events happening, and that's what the union bound does.
But you don't really have to worry about this, all this really changes as the constants.
Okay so that's sampling error.
Now what about approximation error?
Well let's make some assumption.
Let's assume that we have x and y equals 1 and x equals 1 and y equals 1, and we're going to use a function that does this.
So we're going to say that when you fit q hat k plus 1 to the target values, which we're going to say are t q hat k, your fit has an infinity norm error of at most ϵ k.
And for now we're going to analyze the case where you have an exact backup, but we'll come back to the approximate backup later.
So let's just pretend for a minute that our backup is exact, so there's no difference between t and t hat, and we're just studying the effect of error in the fit.
So if we have a set of n and n, we're going to do that.
So we're going to do that.
So we're going to do that.
So we're going to do that.
So we're going to do that.
So we're going to do that.
So we're going to do that.
So we had an exact fit, if we had an exact tabular q iteration method, then q hat k plus 1 would be exactly equal to t q hat k.
And now we're going to assume that it's not exact, that it incurs some error, and that that error is bounded in the infinity norm.
Now this is a strong assumption.
In reality, if you're doing supervised learning, your error is not bounded in the infinity norm.
It's going to be bounded in something like a weighted L2 norm in expectation under some distribution.
We're going to assume it's bounded in the infinity norm, which means that for the worst possible state action tuple, your error is at most ϵ k.
So this is a strong assumption, but it will make this very convenient.
Okay, so now what we're going to try to understand is the difference between q hat k at some iteration k and the real q star, again in the infinity.
So here's how we're going to do it.
We're going to use the same trick as before.
We're going to subtract and add some quantity, and the particular quantity that we're going to put in is t times q hat k minus 1.
The reason is that, well, we're fitting q hat k to t q hat k minus 1, so if we put that in, that's the quantity we can bound.
So we're going to subtract that, and we're going to add it, and then we'll group these two terms together.
So we're going to have one term, which is q hat k minus t q hat k minus 1.
So that's convenient, because that's the quantity that we're going to be bounding by assumption.
And then we have this other term, which consists of the backup of q hat k minus 1 minus the backup of q star.
Now, q star, is the fixed point of t, so you can always replace q star with t q star, which is what I did on this line.
And then we'll apply the triangular equality again to bound the infinity norm of the sum by the sum of their infinity norms.
And the first term here, q hat k minus t q hat k minus 1, is just bounded by ϵ k minus 1 by our assumption at the top.
The indexing is off by 1, that's why it's k minus 1.
So that leaves us with a second term.
Now, for the second term, we're going to recognize an interesting fact that we saw way back in the day when we first learned about Q learning, which is that the Bellman backup is a contraction.
The fact that the Bellman backup is a contraction in the infinity norm means that the infinity norm of what you get by applying t to two different Q functions is less than or equal to γ times the infinity norm of the difference between those Q functions.
So using the fact that t is a contraction in gamma, we can bound this by γ times q hat k minus 1 minus q star.
And now what we've done is we've related q hat k minus q star recursively to q hat k minus 1 minus q star, but with the addition of this little error term ϵ k minus 1.
So now we're going to unroll this recursion.
So applying the same thing again to q hat k minus 1 minus q star, we're going to unroll this recursion.
So applying the same thing again to q hat k minus 1 minus q star, we bound the whole thing by ϵ k minus 1 plus γ ϵ k minus 2 plus γ squared times the difference.
Then we're going to do it again, and we get γ squared times ϵ k minus 3 plus γ cubed, and so on and so on.
If we go all the way back to the beginning, then we end up bounding the whole thing by this γ discounted sum from i equals 0 to k minus 1, γ to the i times the corresponding ϵ plus γ to the k times the difference between q hat 0, and q star.
Now, this tells us something very interesting, and also very useful, which is that the more iterations we take, the more we essentially forget our initialization.
Because as k goes to infinity, this γ to the k term will vanish, because γ is less than 1, which means that the effect of our starting point, q hat 0, is going to vanish.
So what that means is that if we take the limit as k goes to infinity, the second term γ to the k vanishes, because γ to the k approaches 0, and for the first term, we're going to simplify it a little bit.
We're going to replace all those ϵ k minus i minus 1 terms with just the maximum ϵ we get over all the iterations.
That's probably reasonable to do, because if our fitting error is bounded for every iteration, we'll just say that we can also bound it over all iterations.
And now we get our familiar geometric series, the sum from i equals 0 to infinity of γ to the i times some constant.
So that's 1 over 1 minus γ times the infinity norm of ϵ, where ϵ is a big vector with k dimensions, where every dimension has the error at that iteration.
So that's pretty neat.
Now we see how error scales for just the approximation error.
So if we incur some ϵ fit every step, then the total error we'll get will be ϵ times 1 over 1 - γ.
So the longer our horizon essentially the more error we get, get, which intuitively kind of you can think of as saying that every time you back up, which is every iteration of fit or queue iteration, you incur some additional error.
So since the number of backups you need to make is equal to your horizon, that's the order of the approximation error that you'll see.
So now let's put these two things together.
We've got our sampling error, and that quantifies the difference between t hat and t, and we've got our approximation error, which is how much q hat k plus 1 will differ from t q hat k, and they'll differ due to sampling error and due to the approximation error.
So essentially what we're going to do is we're going to subsume the sampling error inside the ϵs, and that will let us connect these two parts up.
So put it stated another way, that bound from the previous slide can also be rewritten as the result of the approximation error.
So that's the way that we're going to do it.
So let's put this back together.
So we've got our sampling error, and that quantifies the limit as k goes to infinity of q hat k minus q star is less than or equal to 1 over 1 minus γ times the max over all of your iterations of the difference between q hat k and t q hat k minus 1.
So this contains both kinds of errors because there's a t in there, so if you're actually backing up using t hat instead of t, you'll have an error there, and it contains the approximation error due to an imperfect fit.
So let's just examine what this quantity is.
q hat k minus t q hat k minus 1.
So we're going to put in t hat q hat k minus 1, so we'll subtract and add up the same trick as before.
We'll again group the terms.
So we're bounding this whole thing by q hat k minus t hat q hat k minus 1 plus t hat q k minus 1 minus t q hat k minus 1.
So the second term is basically taking care of the sampling error, and the first term is taking care of the approximation error.
So we know that the first term is the approximation error, and the second term is the approximation error.
The first term is that ϵ k, and the second term is that big sampling error bound up above.
So what we can do is we can just take these two terms and plug them in here, and we can use that to calculate the difference between q hat k and q star in the limit as k goes to infinity, and it will be 1 over 1 minus γ times a bunch of terms, basically a sum of three terms, two of them coming from sampling error and one coming from approximation error.
So here's what we have.
We can see that the sampling error is the same as the one on the previous slide.
We can see here that error compounds with the horizon over iterations, and due to sampling.
Notice that in the sampling error, the second term actually is also of order 1 over 1 minus gamma, because q infinity is on the order of r max over 1 over 1 - γ.
We discussed this before.
We talked about how the value functions and Q functions, basically their magnitudes are the reward times the reward, and so on.
So we can see that in the sampling error the first term is actually 1 over 1 minus gamma, because q infinity is on the order of r max over 1 over 1 - γ.
reward times the horizon, so it's r max over 1 - γ.
So if you imagine what will happen if we substitute in ϵ k plus sampling error into that second equation, you have a 1 over 1 minus γ term in front, and then you have a sum of three terms, one of which itself also has a 1 over 1 minus γ term in it.
So the overall order of the error will be quadratic in 1 over 1 minus gamma, just like we saw in part one.
Now so far we needed strong assumptions, specifically infinity norm assumptions, on the error that we're incurring.
So that is a fairly strong assumption that is not always going to hold.
More advanced results can actually be derived with p-norms under some distributions.
So infinity norms are not really realistic for practical learning algorithms.
It is possible to do some analysis with p-norms, and you can learn more about that in the RL theory book referenced at the bottom of the slide.
Basically this analysis studies norms of this form, where the p-mu norm is just the expected value under μ of the difference raised to the power p, and then the whole thing is raised to 1 over p.
So if p is equal to 2, this is actually the quadratic Bellman error that we're used to.
So there's some analysis that we can do with that, but we need some assumptions there too to avoid this non-convergence issue.