[p.25]

The second class of classic batch RL methods that I want to cover are methods that use linear fitted value functions.
Now, these days we typically don't use linear fitted value functions.
We would usually use deep neural nets to fit our value functions.
But it's still pretty valuable to understand how these methods work, because a lot of the analysis tools that we might use to try to understand deep RL methods could be based on linear function approximation.
And also because some of these techniques, basically in the linear case, they will give us closed form solutions that look like solutions to least squares problems.
And these closed form solutions can give us a hint for how to develop more effective deep RL methods in the future.
So, you know, since this is a graduate class and the point is to really give you guys an in-depth view of reinforcement learning, I want to go over the linear fitted value function methods.
These are kind of the first class of value-based methods for batch RL.
And although the ideas that we'll talk about for practical modern deep offline RL methods are quite distinct from these, I think they still give us good perspective and a good toolkit that we might use in the future, especially if you're interested in algorithms development.
So I'll just be upfront about it and say that if you want to use offline RL methods today, probably you wouldn't use the things that I'm going to talk about next.
But if you want to develop new algorithms or do theoretical analysis, understanding this stuff can be really helpful.

[p.26]

Okay, so offline value estimation.
How have people thought about it before?
Well, classically, people have thought about the problem of offline value function estimation as extending existing ideas for approximate dynamic programming and Q-learning to the offline setting and deriving tractable solutions with simple function approximators like linear function approximators.
How are people thinking about it now?
Well, these days, mostly research in this area deals with deriving approximate solutions with highly expressive function approximators like neural nets.
And the primary challenge in this case turns out to be distributional shift.
So there's a little bit of a disconnect because the more classic work doesn't really deal with distributional shift.
The reason it doesn't deal with it is because they're assuming that your function approximator is pretty simple.
You're given a good set of features and the bad effects of distributional shift are not too bad.
Whereas with deep nets, the bad effects of distributional shift are very large because the deep nets are so much more powerful.
So the thing that makes these methods work better is also the thing that exacerbates the distributional shift challenge.
So what that means is that the techniques I'll talk about next do not address distributional shift in any meaningful way.
They rather just directly address the problem of how do you estimate the value function from batch data.
And that means that if you apply those techniques with deep nets, they won't work for the reasons that I talked about in the first part of the lecture.
Okay.
So I'll discuss these older methods next for completeness.
And then in the Wednesday lecture, in lecture 16, we'll talk about modern techniques.

[p.27]

Okay.
So let's do a little warm-up.
Let's do a little warm-up and talk about linear models.
This might seem like a little bit of a digression, but I'll actually connect it back to value-based methods shortly.
So let's say that you have a feature matrix.
So this feature matrix is...
Its dimensionality is |S| × K where this cardinality of S is the number of states.
So we're talking about a discrete state MDP.
And K is the number of features.
So you can think of it as a vector-valued function, capital Φ(s), where for every state s, it gives you a k-dimensional feature.
But you can also think of it as a matrix with K columns.
And each column has an entry for that feature for that corresponding state.
So if you want to read off the feature vector for state 1, you would just take the first row and you would transpose it.
Okay.
So that's a feature matrix.
So this feature matrix fully defines features at all the states in your MDP.
And don't worry so much about the number of states being finite.
This can be extended to infinite states using samples.
So that's not really a problem.
Okay.
Can we do offline model-based RL in the feature space?
So what do we need to do model-based RL?
Well, we need to estimate the reward in terms of our features, and we need to estimate the transitions in terms of our features.
And then we need to recover the value function.
And then we use that to improve the policy.
So we're going to do everything with linear function approximation, which means that we're going to assume that it's generally okay to represent your reward as a linear function of the features.
So there's some weight vector w_r.
And if you just multiply ϕ by w_r, that'll be an approximation of the true reward.
Now remember, ϕ is a matrix that has a number of rows equal to the number of states.
r, the reward, is a vector whose length is the number of states.
w_r is a weight vector with a number of entries equal to the number of features.
So you would multiply every column in ϕ by its corresponding weight in w_r and sum them together, and that should give you an estimate of the reward.
So w_r looks like this.
And if you imagine transposing that w_r and multiplying that into the feature matrix and then summing that up, you'll get a vector of length equal to the cardinality of S.
And you want to solve some least-square system to get this thing to be as close to r as possible.
So here's the least-square solution.
This is just from basic introductory statistics.
If you write down the normal equations, this is what you get.
So this is the least-square solution for w_r.
(ϕ^T⋅ϕ)^{-1} ⋅ ϕ^T times the vector of rewards.
This is not a clever equation.
This is literally just the standard normal equations for least-squares.
And for now, we're just assuming that we have access to a vector of the rewards for all states or state-action tuples in general.
We could extend this to state-action tuples.
But we'll talk about the sample-based setting soon.
The other thing we need is we need a transition model.
So a transition model describes how the features right now become the features tomorrow.
So just like we can turn ϕ into r by multiplying it by some weights w_r, we can turn ϕ into future ϕ by multiplying it by some transition probabilities P_ϕ.
Now, this is a little bit more subtle.
So P_ϕ...
Well, first, let's talk about what we're targeting.
What are we trying to approximate?
We're trying to approximate the effect on ϕ of the real transition matrix.
So there's some real transition matrix, which is |S|×|S|.
And that real transition matrix describes which states go to which other states.
Now, the transition matrix on states depends on the policy, which is why I have it written as P^π.
So different policies induce different state transitions.
So it's a policy-specific thing.
Everything here is policy-specific.
So essentially, we're doing policy evaluation, and then we'll do policy improvement in alternating phases.
So the true transitions are obtained by taking this state-to-state transition matrix and multiplying it by ϕ.
Because if you multiply the matrix ϕ by P^π, then you will get the features of the next states, the next states for your current policy.
And what you want to find is a matrix P_ϕ that, when you multiply ϕ by it, will approximate this as closely as possible.
So P_ϕ is, you can think of it as a feature space transition matrix.
It's going to be K×K.
And this is really important.
It's really important that P_ϕ is K×K.
Because essentially what you're doing is you're embedding your original MDP into this feature space.
And, you know, typically K would be much smaller than the cardinality of S.
So you can think of P^π ϕ as a K×K matrix.
Just like w_r was a K by 1 vector.
So w_r when multiplied into ϕ gave you a vector of rewards.
P_ϕ when multiplied into ϕ will give you a matrix of features at the next time step.
And all of this is for a fixed policy behind.
Now if you're wondering how you solve for P_ϕ, it's the same normal equations as before.
So the same least squares formula can be applied to solve for P_ϕ.
It's just our targets now instead of being r are P^π ϕ.
So P^π ϕ is what we want this to be equal to.
Same exact normal equations.
The fact that our matrix value doesn't actually change anything.
So exactly the same logic applies.
So we have a least squares solution to w_r and a least squares solution to P_ϕ.

[p.28]

So we've got our reward model and we've got our transition model.
And now we're going to try to estimate our value function.
Now let's say that our value function is also linear in ϕ.
That means that the value function V^π_ϕ can be written as the same matrix ϕ multiplied by w_V.
w_V is just some vector of weights.
Just like w_r is a vector of weights, w_V is some vector of weights.
By the way, here's a little aside.
This doesn't have anything to do with linear function approximation, but it's a useful formula to know.
So if we want to solve for V^π in terms of P^π and r, there's actually a really neat solution.
So we can write our Bellman equation in vector form.
You can probably think back to our theoretical analysis of Q-learning from before.
The same exact idea.
You can write everything in terms of vectors and matrices.
So the vector-valued version of the Bellman equation is that V^π = r + γ ⋅ P^π ⋅ V^π.
And this is not policy improvement.
This is not doing the math.
This is just evaluating the value function for a particular policy π.
And now some of you might have noticed that this is actually a linear equation.
V^π is a vector.
By the way, what's the dimensionality of V^π?
Well, the dimensionality of V^π is the same as the dimensionality of r, which means it's S by 1.
The dimensionality of P^π is S by S.
So everything works out.
So we have S linear equations.
We have S unknowns, the S entries in V^π.
So we can actually solve them.
So we can subtract γ ⋅ P^π ⋅ V^π from both sides.
And we get (I - γ ⋅ P^π) ⋅ V^π = r.
So then you just invert that matrix.
And you get V^π = (I - γ ⋅ P^π)^{-1} ⋅ r.
It's not entirely trivial to show, but it is actually true that (I - γ ⋅ P^π) is always invertible, which means that the solution always exists and it's always unique.
So that's pretty cool.
You can recover the value function as a solution to a system of linear equations.
Now you can apply the same idea in feature space.
So in feature space, you can write down a feature space Bellman equation.
And by the same logic, you get w_V is equal to (I - γ ⋅ P^π)^{-1} ⋅ w_r.
So you replaced r with w_r.
We replaced P^π by P_ϕ.
And all the same stuff works out.
But then we could say, well, hang on a minute.
So we spent all this time describing models, describing how you can fit the reward, how you can fit the transitions.
And now we just did like a very model-free thing.
Do we even need the model?
Well, let's see what happens when we actually substitute in the equations that we derived on the previous slide for w_r and P_ϕ.
So we have these equations for w_r that we obtained by basically applying least squares.
And we have an equation for P_ϕ that we obtained by doing least squares.
So let's substitute them into this equation for w_V.
And we get kind of a monster.
So I didn't do anything clever.
I just literally copy and pasted that equation.
But after a bit of algebra, you can actually simplify this a little bit.
So there's nothing clever about this algebra.
I'm not going to bore you by going through it, but I just applied a little bit of algebraic simplification I get this formula for w_V and this is called Least Square Temporal Difference.
So if someone says I'm doing LSTD, Least Squares Temporal Difference learning, this is the formula they're referring to.
It's a formula that relates a transition matrix for a particular policy P^π and reward vector r, as well as the feature matrices ϕ to the corresponding weights on the value function for π.
So this is actually a fairly famous formula in classic reinforcement learning.
And it is the Least Squares Temporal Difference formula, or the LSTD formula.

[p.29]

OK.
Now let's talk about doing all of this with samples.
By the way, just to recap a little bit what we just did.
Essentially, we took this journey through the world of model-based RL and arrived at actually a model-free formula for solving for the value function with linear features.
But this model-free formula has a few issues because it requires us to know P^π and it requires us to know r.
So what we're going to do next is we're going to replace this all with samples.
Because once we replace it with samples, we'll get a fully model-free way to solve for the weights on the approximation to V^π without requiring knowledge of P^π or r.
So our samples, just like before, consist of a bunch of transitions.
So that's our offline data set of transitions, (s, a, r, s').
And what we're going to do is we're going to replace, notice how we just replaced the number of rows in our matrix ϕ by the number of samples.
So before we had one row for every state.
Now we have one row for every sample.
And we're going to replace P^π ⋅ ϕ with the features at the next time step.
Right.
Because we don't know what P^π is.
We do know what s' is.
So we can just evaluate the feature function at s'.
And we can get what we call capital ϕ'.
And we can just substitute that in there.
And similarly, we can replace our reward vector, which had one entry for every state, with a reward vector that had one entry for every sample.
So we just rewrite everything in terms of samples.
And it turns out that everything still works exactly the same way.
And this is sometimes...
This trick of replacing the set of states with the set of samples is sometimes referred to as the empirical MDP.
Because it's the MDP that is induced by our empirical samples.
Everything works exactly the same way, only now we're going to have some sampling error.

[p.30]

So we can just use this to get a sample-wise estimate with the same exact equation.
So now let's turn this into a complete algorithm.
So before we said that we're going to estimate the reward, estimate the transitions, recover the value function, and then improve the policy.
But now we're going to do step one, two, and three just with this LSTD equation.
So instead of estimating the reward and estimating the transitions explicitly, we're just going to directly estimate the value function using our samples and then improve the policy.
So the typical policy improvement step would be to recover your policy as the greedy policy under your value function and then repeat the process of estimating the value function.
The way you estimate...
You can estimate your value function is by using those LSTD equations.
But the problem is that this actually requires samples from π.
Right.
Notice that we said this is P^π.
So that's actually a little bit of a problem.
So we can use all that stuff on the previous slide to estimate the value function of the policy that collected the data.
But we can't actually use it to estimate the value function for some other policy because we need P^π.
And P^π actually depends on π.
But we're trying to evaluate some other policy that we're learning, not the one that collected the data.
So what we're going to do is exactly the same thing that we did when we talked about value iteration versus Q iteration.
Instead of estimating the value function, we're going to estimate the Q function.
So this trick is not going to work for offline RL for value functions, but it will work for Q functions.

[p.31]

And that's what gives us least squares policy iteration or LSPI, which is an actual offline RL method that we can do with just previously collected data.
And the main idea is to basically replace LSTD, Least Squares Temporal Difference, with what we can call LSTDQ, which is just LSTD for Q functions.
So now, instead of having a feature matrix for a feature for every state, we have state action features.
Right.
So the number of rows in the full version, the non-sample based version, is |S|⋅|A|.
And we probably have more features because we need different features for different actions.
There are a few ways of doing it.
If you have a small discrete action set.
You could simply have a different copies of your features, one for every possible action.
So everything else actually stays exactly the same.
So you can still derive the same w_Q.
Again, I won't go through the massive algebra to do this, but it's exactly the same as it was before.
So you have the same LSTD equations, but now you use state action features for everything and state action rewards.
And you can do the same sample based substitution.
So you have your data set of state, actions, rewards, and next states.
You can compute the reward as a big vector for an entry for every sample.
You can still compute ϕ'.
But notice now there's a little subtlety.
ϕ' is the next feature that you get from taking the action in the data set.
But then because we're featurizing states and actions, we give it the next state in the data set, s'.
But the action that the policy would have taken at s'.
Now we don't have to actually evaluate the following state.
We don't have to know what the dynamics will do for π(s').
We do need to featurize the action from the policy.
And this is very similar to the principle that we had in Q iteration.
So we're going to featurize s' and π(s').
And that will give us ϕ'.
So a very important property here is that ϕ doesn't change as the policy changes.
But ϕ' does change.
For ϕ', we always featurize the same s', the one we have in the data set.
But a different action because that action depends on our policy π.
So ϕ' is actually the only thing in here that depends on π.
So you would encode the action π, not the action in the data.
So then LSBI proceeds like this.
Compute w_Q for your current policy.
Let's call it π_k.
And then compute a new policy, π_{k+1}, which is greedy with respect to w_Q.
Because with w_Q, you can get a full Q function.
And therefore, you can compute a full policy.
And then you set your ϕ' to be the new ϕ' feature matrix that you would get with this new policy π_{k+1}.
So this is now a complete algorithm for doing offline RL.
You would actually evaluate this w_Q by just computing the right-hand side of this equation.
Recover π_{k+1}.
And then set the ϕ' to be the corresponding feature matrix.

[p.32]

All right.
So what's the problem with all this?
Well, the issue with actually doing all this in practice is the distributional shift problem that we had before.
So this linear approach, you know, you can, of course, re-derive this for nonlinear systems and do some kind of iterative thing.
But it doesn't address the distributional shift problem.
It doesn't address the distributional shift problem because it's just doing empirical risk minimization at some level via least squares on your training set.
And that means that all those issues that we talked about before where you're essentially doing this adversarial example discovery by maximizing your policy would still afflict you.
Where does it afflict you?
Well, it afflicts you in step two where you take the argmax.
So in general, all approximate dynamic programming methods, fitted value iteration, Q iteration, etc.
They'll all suffer from this action distributional shift and we have to fix it.
So it's good to understand these linear methods because they provide a useful tool for analysis.
They provide good perspective on how we arrived at the techniques that we have today.
But they don't by themselves really give us a great tool for doing offline RL in the deep RL setting in practice.
And how to do that properly is what we'll cover in the next lecture.