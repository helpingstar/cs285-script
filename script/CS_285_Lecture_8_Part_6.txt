All right, in the last portion of today's lecture, I'll go through some tips and tricks for implementing Q-learning algorithms, which might be useful for homework 3.
And then I'll give a few examples of papers that have used variants of the methods that I described in this lecture.
So first, a few practical tips.
Q-learning methods are generally quite a bit more finicky to use than policy gradient methods, so they tend to require a little bit more care to use correctly.
It takes some care to stabilize Q-learning algorithms.
And what I would recommend is to start off by testing your algorithms on some easy, reliable problems where you know that your algorithms should work, just to make sure your implementation is correct.
Because essentially, you have to go through several different phases of troubleshooting.
You first have to make sure that you have no bugs, then you have to make sure that you tune your hyperparameters, and then get it to work on your real problems.
So you want to do the debugging before the hyperparameter tuning, which means that you want to do it on really easy problems.
where basically any correct implementation should really work.
Q-learning performs very differently on different problems.
So these are some plots of DQN-type experiments on a variety of different Atari games.
And something you might notice is that there's a huge difference in the stability of these methods.
So for Pong, your reward basically steadily goes up and then flatlines.
For Breakout, it kind of goes up and then wiggles a whole bunch.
And then for some of the harder games, like video pinball and Venture, it's just completely all over the place.
And the different colored lines here simply represent different runs of the same exact algorithm with different random seeds.
You can see the different random seeds for Pong are basically identical.
For Breakout, they're kind of qualitatively the same but have different noise.
Whereas for something like Venture, some of the runs work and some fail completely.
Large replay buffers do tend to help to improve stability quite a lot.
So using a replay buffer of a size of about 1 million can be a pretty good choice.
And at that point, the algorithm really starts looking a lot more like fit-a-q iteration, which is perhaps part of the explanation for its improved stability.
And lastly, q-learning takes a lot of time, so be patient.
It might be no better than random for a long time while that random exploration finds the good transitions, and then it might take off once those good transitions are found.
And many of you will probably experience this in homework 3 when you train on the Pong video game.
Start with high exploration, start with large values of epsilon, and then gradually reduce to a higher value of epsilon.
Start with high exploration, start with large values of epsilon, and then gradually reduce to a higher value of epsilon.
So you can start with a lot of exploration as you go, because initially your q function is garbage anyway, so it's mostly the random exploration that will be doing most of the heavy lifting.
And then later on, once your q function gets better, then you can decrease epsilon.
So it often helps to put it on a schedule.
A few more advanced tips for q-learning.
The errors of the Bellman error, the gradients of the Bellman error, can be very big.
So it's kind of a least squares regression, so these squared error quantities can be large quantities, their gradients can be very large.
And something that's a little troublesome is that if you have a really bad action, you don't really care about the value of that action, but your squared error objective really cares about figuring out exactly how bad it is.
So if you have some good actions that are like plus 10, plus 9, plus 8, and you have some bad actions that are minus 1 million, that minus 1 million will create a huge gradient, even though you don't really care that it's minus 1 million.
Like if you were to guess minus 900,000, it would result in the same policy.
But your q function objective is to get the same result, so you can get the same result.
And the squared error objective really cares about that.
And that will result in big gradients.
So what you can do is you can either clip your gradients, or you can use what's called a Huber loss.
A Huber loss, you can think of as kind of interpolating between a squared error loss and an absolute value loss.
So far away from the minimum, the Huber loss looks like absolute value, and close to the minimum, because absolute value is a non-differentiable cusp, the Huber loss actually flattens it out with a quadratic.
So the green curve here on the bottom, the green curve on the bottom, the red curve on the right, shows a Huber loss, whereas the blue curve shows a quadratic loss.
So the Huber loss actually mechanically behaves very similarly to clipping gradients, but it can be a little easier to implement.
Double Q learning helps a lot in practice.
It's very simple to implement, and it basically has no downsides.
So probably a good idea to use double Q learning.
N-step returns can help a lot, especially in the early stages of training, but they do have some downsides, because n-step returns bias your objective, especially for larger values of n.
So be careful with n-step returns, but do keep in mind that they can improve things in the early stages of training.
Schedule exploration and schedule learning rates.
Adaptive optimization rules like Atom can also help a lot.
So some of the older work use things like RMS prop, which doesn't work quite as well as the most recent adaptive optimizers like Atom.
So good idea to use Atom.
And also when debugging your algorithm, make sure to run multiple runs, and make sure that you run multiple random seeds, because you'll see a lot of variation between random seeds.
You'll see that the algorithm is very inconsistent between runs, so you should run a few different random seeds to make sure that things are really working the way you expect, and that you didn't get a fluke, and the fluke can either be unusually bad or unusually good.
So keep that in mind.
Okay, so in the last portion of the lecture, what I'm going to do is I'm going to go through a few examples of previous papers that have used algorithms that relate to the ones that I covered in this lecture.
The first paper that I want to briefly talk about, is this paper called Autonomous Reinforcement Learning from Raw Visual Data by Lang and Riedmiller.
So this is a paper from 2012.
It's quite an old paper, and it's one of the actually earliest papers that used deep learning with Theta-Q duration methods.
The particular procedure this paper used though is a little different from the methods that we covered in this lecture.
It's actually more similar to some of the model-based algorithms that I'll discuss later.
So in this paper, what the authors did is they actually learned, a kind of a latent space representation of images by using an autoencoder, and then they actually run Theta-Q duration on the latent space of this autoencoder, on the feature space.
But the particular Theta-Q duration they use actually doesn't use neural networks.
It uses something called random trees.
So they use a non-deep, but still Theta-Q duration procedure on the representation learned by a deep neural network.
So it's Q learning on top of a latent space learned with an autoencoder using Theta-Q duration.
And something called extra random trees for functional approximation.
You can think of extra random trees as basically very similar to random forces.
And the demonstration that they had in this paper, which is pretty cool, is to use an overhead camera to look at this little slot car racetrack, and then learn to control the slot car to drive around the racetrack.
Here is a paper that uses convolutional neural networks with Q learning.
This is deep Q learning.
So this is a paper called human level control.
And this paper uses Q learning with convnets, with replay buffers and target networks, and this kind of simple one-step backup that I mentioned, and one gradient step to play Atari games.
And this can be improved a lot.
So it can be improved a lot with double Q learning.
The original method in this paper can actually also be improved a lot just by using Atom.
So that alone actually gets you much, much better performance.
But for homework 3, you'll be implementing something fairly similar to this paper.
Here is a paper on Q learning with continuous actions for robotic control application, or kind of a simulated control application.
So this is the DDPG paper called continuous control with deep reinforcement learning.
It uses continuous actions with a maximizer network, and uses a replay buffer and target networks with polycaveraging with a one-step backup and one gradient step per simulation step.
And they evaluated on some kind of simple toy low dimensional model.
So this is a paper that's really useful for simulating robotics tasks.
Here's a paper that actually uses a deep Q learning algorithm with continuous actions for real-world robotic control.
And this actually kind of exploits some of the parallelism ideas that I discussed before.
So here you have multiple robots learning in parallel to open doors.
It's a paper called robotic manipulation with deep reinforcement learning and asynchronous soft policy updates.
And this uses that NAF representation.
So this is a Q function that is quadratic in the actions, making maximum performance.
And it's really useful for maximization easier.
You use a replay buffer and target network, a one-step backup.
And this one actually uses four grading steps per simulation step to improve the efficiency.
Because collecting data from the robots, I guess it's not even simulations, it's actually real world.
Collecting data from the robots is expensive.
So you'd like to do as much computation with as little data as possible.
And it's further parallelized across multiple robots for better efficiency.
This method, which I showed actually in lecture one, a deep queue learning algorithm that takes this paralyzed interpretation of FIDA queue iteration to the extreme.
So here there are multiple robots that are learning grasping all in parallel, and there are actually multiple workers that are all computing target values, multiple workers that are all performing regression, and a separate worker that is managing the replay buffer.
So this is literally instantiating that system that I showed before with process one, process two, and process three.
And in this case, each of those processes are themselves forked off into multiple different workers on a large server farm.
All right, if you want to learn more about queue learning, some suggested readings.
Classical papers.
This is the Watkins queue learning paper that introduced the queue learning algorithm in 1989.
This paper called neural FIDA queue iteration introduces batch mode queue learning with neural networks.
Some deep RL papers for queue learning.
The Wang and Riedmiller paper that I mentioned.
The DQN paper.
This is the paper that introduced double queue learning.
This is the paper that introduced that approximate maximization with mu theta.
This is the paper that introduced NAF.
This is a paper that introduces something called dueling network architectures, which is very, very similar to the NAF architecture, but adapted for discrete action spaces.
I didn't cover this in the lecture, but it's also a pretty useful trick for making queue learning work better.
All right, so these are the suggested readings, and you can find them in the slides.
If you want to learn more, I highly encourage you to check it out.
And just to recap what we've covered in today's lecture, we talked about queue learning in practice, how we can use replay buffers and target networks to stabilize it.
We talked about a generalized view of FIDA queue iteration in terms of three processes.
We talked about how double queue learning can make queue learning algorithms work a lot better, how we can do multi-step queue learning, how we can do queue learning with continuous actions, including with a single queue learning algorithm.
And we talked about how we can do multiple queue learning with random sampling, analytic optimization, and a second actor network.
And that's it.