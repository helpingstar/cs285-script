All right, next we're going to talk about some design decisions for actually implementing Actor-Critic algorithms.
So we'll start with the discussion of neural network architectures.
In order to actually instantiate these algorithms as deep RL algorithms, we have to pick how we're going to represent the value function and the policy.
So before, in the last lecture, we just had the policy to deal with.
Now we have to represent both of these objects.
And there are a couple of choices we could make.
So one very reasonable starting choice, and this is the one that I would recommend if you're just getting started, is to have two completely separate networks.
So you have one network that maps a state to the value, and then you have another completely separate network that maps that same state to the distribution over actions.
And these networks have nothing in common.
This is a convenient choice because it's relatively simple to implement, and it tends to be fairly stable to train.
The downside is it may be regarded as somewhat inefficient because there's no sharing of features between the actor and critic.
This could be a more important issue if, for example, you are learning directly from images, and both these networks are convolutional neural nets.
Maybe you would really want them to share their internal representations so that, for example, if the value function figures out good representations first, the policy could benefit from them.
In that case, you might opt for a shared network design where you have one trunk.
Maybe this represents the convolutional layers.
And then you have separate heads, one for the value and one for the policy action distribution.
This shared network design is a little bit more complicated.
It's a little bit more complicated.
But it's a little bit more complicated.
So you have two different layers.
The shared network design is a little bit harder to train.
It can be a little bit more unstable because those shared layers are getting hit with very different gradients.
The gradients from the value regression and the gradients from the policy gradient, they'll be on different scales.
They'll have different statistics.
And therefore, it might require more hyperparameter tuning in order to stabilize this approach.
But it can, in principle, be more efficient because you have these shared representations.
Now there is another important point that we have to discuss before we get an actual practical deep review.
And that is the question of batch sizes.
So as described here, this algorithm is fully online, meaning that it learns one sample at a time.
So it takes an action, gets a transition, updates the value function on that transition, and then updates the policy on that transition.
And both updates use just one sample.
Now, we know from the basics of deep learning that updating deep neural nets with stochastic gradient descent using just one sample is not going to be a good idea.
It's going to be a little bit more complicated.
But it's going to be a little bit more complicated.
So we're going to be looking at some of these steps.
And I'm going to start with the first step, which is the deep learning.
And then we're going to look at the second step, which is the learning of the deep neural nets.
We're going to increased it.
Now, here.
What we've got here is the students here.
And these students are going to be learning mobility systems that we've been taught how to use in the in the intraspect arena reviews classroom and encima.
And what we saw here is that companies have gone out of the verdure system.
What you're seeing in the video is the way you Давайте Okay.
am I going to explain the families here.
Why is that it happens because you can do it.
What is it here for?
What images is this designed for for people who have the device running This is the most basic kind of parallelized actor critic.
It's a synchronized parallel actor critic.
Instead of having just one data collection thread, instead of just running one simulator, you might run multiple simulators, and each of them will choose an action in step one and generate a transition.
But they're going to use different random seeds, so they'll do things that are a little bit different.
And then you will update in step two and step four using data from all of the threads together.
So the update is synchronous, meaning that you take one step in step one for each of the threads, then collect all the data into your batch and use it to update the value function, and then use it to update the policy synchronously.
And then you repeat this process.
So this will give you a batch size equal to the number of worker threads.
It can be a little bit expensive, right, because if you want a batch size of like 32, then you need 32 worker threads, but it does work decently well.
Now it can be made even faster if we make it into asynchronous parallel actor critic.
Meaning that we basically drop the synchronization point.
So now we have these different threads that are all running at their own speed.
And when it comes time to update, what we're going to do is we're going to pull in the latest parameters and we're going to make an update for that thread, but we will not actually synchronize all the threads together.
So just as soon as we accumulate some number of transitions, let's say we got 32 transitions from all the workers, we'll make an update.
Now the problem with this approach, of course, is that the actual transitions might not have been collected by exactly the same parameters.
So if one of the threads is lagging behind, maybe its transition was generated by an older actor, and then you will basically not actually update until you get transitions from faster threads, and those will be using a newer actor.
So in general, all of the transitions that you're pulling together into your batch, which is the first one here, may have been generated with slightly different actors.
Now they're not going to be too different because these threads aren't going to be running at such egregiously different rates, but there will be a little bit lagging behind.
So an obvious question to ask here is, well, is this kind of update, the asynchronous update, mathematically equivalent to the standard synchronous update?
And the answer is that it isn't, that you have this small amount of lag, which is similar to what you would get with asynchronous SGD.
But in practice, it usually turns out that making the method asynchronous, leads to gains in performance that outweigh the bias incurred from using slightly older actors.
The crucial thing here is slightly older, right?
Because the actors are not going to be too old.
If they're too old, then of course this won't work.
But as long as none of the threads hang up, then you'll be okay.
But this might get us thinking about another question.
Well, in the asynchronous actor critic algorithm, the whole point was that we could use transitions that were generated by slightly older actors.
If we can somehow get the transition that we're going to use from the same actor, and somehow get away with using transitions that were generated by much older actors, then maybe we don't even need multiple threads.
Maybe we could use older transitions from the same actor.
Basically, maybe we could use a history and load in transitions from that history and not even bother with multiple threads.
And that's the principle behind off-policy actor critic.
So the design of off-policy actor critic is that now you're going to have one thread, and you'll update with that one thread.
But when you update, you're going to use a replay buffer of old transitions that you've seen, and you will actually load your batch from that replay buffer.
So you're actually not going to necessarily use the latest transition.
You'll collect a transition, store it in the replay buffer, and then sample an entire batch from that replay buffer, maybe 32 transitions rather than just one, and update on that batch.
Now at this point, we have to modify the algorithm, because doing this naively won't work.
This batch that we loaded in from the replay buffer definitely came from much older policies.
So it's not like the asynchronous actor critic buffer that we saw before, where the transitions came from just slightly older actors and we could just ignore that.
Now it's coming from much older actors, and we can't ignore that.
We have to actually change our algorithm.
So when I say replay buffer, basically I just mean a buffer that contains transitions that we saw in prior timestamps.
The most straightforward way to implement a replay buffer is to implement it as a ring buffer, a first-in, first-out buffer, where you batch up, let's say, one million transitions.
I will say here that we will discuss replay buffer and replay buffer much, much more in a subsequent lecture.
So don't get too caught up on this for now.
It's just a buffer that stores all the data, all the experience you've seen so far.
And then, of course, we're going to form a batch for each of these updates by using previously seen transitions.
Okay, so let's see what this might look like in an off-policy actor critic algorithm.
We're going to take an action, as usual, from our latest policy.
Get the corresponding transition.
But then instead of using that transition for learning, we'll actually store it in our replay buffer.
Then we will sample a batch from that replay buffer.
So this notation denotes a set of n transitions, each of them indexed with i.
It might not even contain our latest transition.
So when we load this batch from the buffer, it might not contain that latest transition that we sampled, and that's okay.
And then we're going to update our value function using targets for each of these transitions in our batch.
So we have capital N transitions, which means we have capital N targets.
So we're going to compute the gradient of our loss averaged over the batch.
So n here is the batch size.
It's not the total buffer size, it's just the size of the batch.
So it might be 32 or 64.
Then we'll evaluate our advantage, again, for each of the samples in our batch.
And then we'll update our gradient, our policy gradient, by using that batch.
So now the policy gradient is also averaged over n samples.
And then we'll update our gradient, and then we'll apply the policy gradient like before.
So this algorithm is not going to work the way I described.
It's actually quite broken, and we have to do a bunch of things to fix it.
One thing that I would recommend as an exercise here is to pause the video, look at this algorithm, and try to guess where it's broken.
I'll tell you right now it's broken in at least two places.
Meaning that in at least two places in the pseudocode there's something that doesn't make sense.
Try to pause the video and find it, and then you can resume and I'll tell you what it is.
Okay.
So the first problem is that when you load these transitions from the replay buffer, is that when you load these transitions from the replay buffer, remember that the actions in those transitions were taken by older actors.
So when you use those older actors to get the action and compute the target values, that's not going to give you the right target value.
It'll give you the value of some other actor, not your latest actor.
And that is not what you want.
So formally the answer is that the target value is not the same as the target value.
The issue is that AI did not come from the latest π theta.
It came from some older π theta.
And therefore si prime also was not the result of taking an action with the latest actor.
And that's a problem.
The second issue is that for that same reason, because AI didn't come from the latest policy π theta, you can't compute the policy gradient this way.
Remember from the previous lecture it is very very important when computing the policy gradient that we actually get actions that were sampled from our policy, because this needs to be an expected value under π theta.
If that is not the case, we need to employ some kind of correction, such as importance sampling.
And we could actually do this with importance sampling, but it turns out there's actually a better way to do it for off-policy actor critic, which I will tell you about next.
But first, let's talk about fixing the value function.
So I'll first fix the problem in step 3, and then I'll fix the problem in step 5.
So to fix the problem in step 3, instead of working with value functions, let's instead think back to lecture 4, where we also introduced this notion of a Q function.
If the value function tells you the expected reward you will get if you start in state s and then follow the policy π, the Q function tells you the reward you'll get if you start in state s , then take action a and then follow the policy π.
Now notice here that there's no assumption that the action a actually came from your policy.
So the Q function is a valid function for any action.
It's just in all subsequent steps you follow π.
So what we're going to do to accommodate the fact that our transition s , a , s did not come from our latest policy π is that we will actually not learn v, but we will instead learn q.
So we will not keep track of ^{V}^π_ϕ, we will keep track of ^{Q} π ϕ.
It's going to be a different neural network.
We'll take in a state and an action and output a q value.
But otherwise the principle behind the update is the same.
So we're going to compute target values and then we will regress onto those target values.
It's just that now we'll give the action as an input to the Q function.
Another way to think about it is we can no longer assume that our action came from our latest policy π theta, so we'll instead learn a state action value function that is valid for any action so that we can train it even using actions that didn't come from π theta, but then query it using actions from π theta.
Now those of you that are paying attention might notice that there's a little bit of an issue here.
Because before I was learning v hat and I was using v hat in my targets.
And that's okay because I'm learning v hat so I have it available to me to use in my targets.
But now I'm learning q hat, but I still need v hat for my target values.
So where do I get that?
Well, remember that the value function can also be expressed as the expected value of the Q function where the expectation is taken under your policy.
So what we can do is we can replace the v in our target value with q, evaluate it at the action ai prime, except that ai prime now is not the action from our replay buffer.
ai prime is actually the action that your current policy π_θ would have taken if it had found itself in si prime.
So you'll actually sample si ai si prime from your replay buffer, but then you will sample ai prime by actually running your latest policy.
And you can do that because your policy is just a neural network.
You don't have to actually interact with a simulator to ask the policy what action it would have taken.
So it's a little trick that we're pulling here.
We're actually exploiting the fact that we have functional access to our policy so we can ask the policy what action it would have taken.
We can ask our policy what it would have done if it had found itself in this old state si prime even though that had never actually happened.
So then we get this action ai prime and we plug it into the q value.
And that gets us a target value that actually represents the value of the latest policy at this old state si prime.
That's really cool.
Okay, so we've resolved our issue with the value function.
Instead of learning v, we're going to learn q and we're going to exploit the fact that we can evaluate the value function with the expected value of the Q function under the policy.
Now, how are we going to deal with step 5?
How are we going to deal with a policy gradient?
Well, all we're going to do is we're going to use the same trick but this time we're going to use it for ai instead of ai prime.
So in order to evaluate the policy gradient we need to figure out an action sampled from the latest policy π_θ at the state si.
But of course we can do that.
We can just ask our policy what it would have done at the state si if it had the option to act there.
And we'll call this action ai π to differentiate it from ai.
So ai was actually from the buffer.
ai prime is what the policy would have done if it had been in the buffer state si.
And now we'll just plug in this ai π into our policy gradient equation and that's now correct because ai prime did in fact come from π_θ so this is in fact an unbiased estimator of expectations under π theta.
So remember, ai π here is not the action that we're going to be using.
So we're going to use this as an example of how we can evaluate the policy gradient at the state from the replay buffer.
It's the action sampled from your policy at the state from the replay buffer.
Now in practice when we do this kind of off-policy actor critic we don't actually use the advantage values.
We just plug in our ^{Q} directly into this equation.
We don't have to do it.
We could actually calculate advantages.
There's nobody stopping us from doing that.
But it turns out that it's very convenient to just plug in q values.
They have higher variance because they're not being baseline.
But higher variance is actually okay here.
Why is that?
Well it's because we don't need to interact with a simulator to sample these actions ai prime.
So it's actually very easy to lower our variance just by generating more samples of the actions without actually generating more sampled states.
So it doesn't require any simulation it just requires running the network a few more times.
So in practice we're actually okay with a higher variance here because in exchange we get a larger batch size and it's all good.
And it spares us the complexity of computing the advantage of step four.
So we're actually going to completely drop step four for off-policy actor critic algorithms and we'll use ^{Q} instead of a hat.
Which is still unbiased it just doesn't have the baseline.
So that gives us the more or less complete algorithm for off-policy actor critic.
What else is left?
Well there is still a little bit of an issue.
Because SI the state that we're actually using itself it didn't come from the state margin of the latest policy.
It came from the state margin of an old policy.
Unfortunately there's basically nothing we can do here.
So this is going to be a source of bias in this procedure and we'll just have to accept it.
The intuition for why it's not so bad is because ultimately we want the optimal policy on p θ of s but we get the optimal policy on a broader distribution.
So our replay buffer will contain samples from the latest policy as well as many samples from other older policies.
So the distribution is sort of broader than the one we want.
So we don't want to be on the states from our latest policy we just also have to be good on a bunch of other states which we might never visit.
So we're doing kind of extra work but we're not missing out on important stuff.
And that's the intuition for why this basically tends to work.
Okay so a few details here.
If you actually read some papers and I'll reference a paper here shortly that implement this procedure one of the things you'll notice is that often times there's much fancier things we can do for step four.
There's something called the reparameterization trick which I'll discuss in the second half of the course much later so don't worry about it for now but that can be a better way to estimate this integral.
There are also many fancier ways to fit the Q function and we'll discuss this in the next two lectures when we talk about Q learning.
So I described a very naive way to fit the Q function but there are actually better ways to do it.
If you want an example of a practical algorithm that builds on this idea check out the algorithm called soft actor critic.
This is actually one of the most widely used actor critic methods today.
Although the online value based actor critic methods are more classical the off policy Q value based actor critic methods are more commonly used.
And we'll also learn about algorithms that do this kind of thing with deterministic policies later.
So this is for a stochastic actor later on when we talk about Q learning we'll actually revisit off policy actor critic methods also with deterministic actors.