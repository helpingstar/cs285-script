Okay, so at this point we've almost developed a practical deep Q learning algorithm that we could actually use, but there's another component that we need to discuss to really get a stable and reliable procedure.
So there is another problem that we haven't tackled yet.
So so far we dealt with the problem of our samples being correlated by introducing a replay buffer where we store all of the data that we've collected so far, and each time we have to take a step on the parameters of our Q function, we actually take that step using a batch of transitions that are sampled IID from our buffer.
But we still have this other problem to contend with, which is that Q learning is not gradient descent, and in particular the problem that Q learning has is that it has a moving target.
So you could think of it as a moving target.
You could think of it as squared error regression, except that the regression target itself changes all the time, and it changes out from under us, which makes it very hard for the learning process to ever converge.
So we'll deal with the correlation by using a replay buffer, but this part is still a problem.
So what does Q learning really have to do with regression?
In the full-fitted Q iteration algorithm that I described before, step 3.
Step 3 performs what looks a lot like supervised learning, essentially regression onto target values yi.
And in fact, in general, step 3 in the full-fitted Q iteration algorithm will converge if you run it to convergence, but then your targets will change out from under you, so maybe you don't want them to converge.
Essentially, trading to convergence on the wrong targets isn't necessarily a good thing to do.
Which is why in practice, we often use a much smaller number of gradient steps, as few as one gradient step.
And then we have this moving target problem, where every gradient step our targets change, and our gradient is not accounting for the change in the targets.
So intuitively, what we would like to resolve this issue is something that's a little bit in between, where we could have some of the stability of the full-fitted Q iteration algorithm, where in step 3 we train to convergence, but at the same time, don't actually train to convergence.
So here's how we can do Q learning.
We're going to do Q learning with a replay buffer and a target network.
And this is going to look like a kind of mix between the online Q learning algorithm and the full-batch fitted Q learning algorithm.
So we're going to collect our data set using some policy, and we add that policy to our buffer.
This is now step 2.
Step 1 will be revealed later.
We're going to then, in an inner loop, sample a batch, s_i, a_i, si', ri, from this buffer.
And then we will make a new batch.
And then we will make a new batch.
And then we will make an update on the parameters of our Q function.
This update looks a lot like the update from before with replay buffers, but I've made one very small change, where now the parameters in the max are different parameter vectors.
So it used to be that I would take the max over a' of Q phi, and now it's Q_ϕ prime, where ϕ prime is some other parameter vector.
And then, of course, after I make a k of these back and forth updates, which, again, is a little bit more complicated, I'm going to make a new batch.
And then, of course, after I make a k of these back and forth updates, which, again, is a little bit more complicated, could be just k equals 1, could be just k equals 1, I'd go out and collect more data.
And then I have a big outer loop where, after n steps of data collection, I'm going to actually update ϕ prime and set it to be ϕ.
So this looks a lot like the fitter-q duration procedure I had before, because essentially I'm making multiple updates with the same target values.
Because if ϕ prime stays the same, then the entire target value stays the same.
Except that I might still be collecting more data, except that I might still be collecting more data, data in that inner loop.
So step two, the data collection is now inside the updates to ϕ prime.
And the reason for doing this is because in practice, you often want to collect data as much as possible, whereas for stability, you typically don't want to update your target network parameters quite as often.
So some sensible back of the envelope choices, k could be between one and four.
So we might take between one and four steps between each time when we collect more data, but n might be around 10,000.
So we might take as many as 10,000 steps before we change our target values, and that's to make sure that we're not trying to hit a moving target, because it's very hard to hit a moving target with supervised regression.
So initially, we initialize both ϕ and ϕ prime to be essentially a random initialization, and then after the first n steps, which could be 10,000, we're going to update ϕ prime to set it to be equal to ϕ.
But then ϕ prime will remain static for the next 10,000 steps.
And that means that step four starts looking a lot more like supervised regression, and for that reason, step four is easier to do, it's much more stable, and you're much more likely to get an algorithm that learns a meaningful q function.
So your targets don't change in the inner loop.
And that means that essentially step two, three, and four looks a lot like supervised regression, with the only real difference being that you might collect more data, and that data could be collected using your latest, for instance, epsilon 3d policy.
So based on this general recipe, one of the things we can derive is the kind of classic deep q learning algorithm, which is sometimes called dqn.
Don't be too confused by the name, dqn is essentially just q learning with deep neural networks, which is a special case of this kind of general recipe that I outlined.
So this particular special case looks like this.
Step one, take an action AI and observe the resulting transition, and then add it to the buffer.
So this looks a lot like online q learning.
Step two, sample a mini batch from your buffer uniformly at random.
So this mini batch might not even contain the transition that you just took in the real world.
Step three, compute a target value for every element in your mini batch.
And you compute these target values now using your target network, q5 prime.
Step four, update the current network parameters ϕ by essentially taking the gradient for the regression onto those target values.
Now notice that so far, ϕ has not been used anywhere else, except maybe in step one, if you're using an epsilon 3d policy, because then in step one, you take your action based on the epsilon 3d sampling rule for the policy induced implicitly by the argmax over q5.
So that's the other place where q5 might be used.
And then step five, which is only done every n steps, is to update ϕ prime by replacing ϕ prime with ϕ.
And as I said, n might be around, maybe, 10,000.
And then you repeat this process.
Now something to note is that this procedure is basically a special case of the more general procedure at the top of this slide.
Take a moment to think about this.
Take a moment to think about what particular settings of the parameters are present in this parameter, and what particular of the algorithm at the top would yield the classic deep Q learning algorithm at the bottom.
So you would basically get this algorithm if you choose k equals 1.
That's essentially the only the only thing you have to do and if you choose k equals 1 then you will recover exactly the procedure at the bottom.
Take a moment to think about this.
It's not entirely obvious because the numbering of the steps has been rearranged a little bit but they are basically the same map.
Okay and it's a good idea to have a pretty thorough understanding of this procedure because all of you will actually be implementing it for homework 3.
So if you use k equals 1 then you get exactly the procedure at the top.
Now there are some other ways to handle target networks that have been used in the literature and that could be worth experimenting with.
There's something a little strange about this way of updating target networks.
There are some things that you can do to change the target networks.
There's something a little strange about this way of updating target networks.
There's something a little strange about this way of updating target networks.
And here's some intuition to illustrate some of the strangeness.
This strangeness is not necessarily really bad, it's just a little bit strange.
So let's say that I sampled my transition and then I updated my ϕ and then I sampled another transition and I updated my ϕ again.
So blue boxes are samples, that's basically step one.
Green boxes that's step two, three and four.
And then I keep going like this.
And then I keep going like this.
And then over here on this step, maybe my target network is obtained from the first step.
So perhaps at the first step when I started out, ϕ prime is equal to ϕ.
So at the third step, I get my target values back from the first step.
And at the second step, I get them from the first step.
And the fourth step, I get them from the first step.
And then if the fourth step is where I update ϕ prime to be equal to phi, so basically if my n is equal to four, in practice n would be much larger, but let's say it's equal to four.
Then at the fifth step, I get my ϕ prime from the preceding step.
So it seems like in different steps, the target values are lagged by very different amounts.
If you're at the step right after n, if you're at the n plus one step, your target network is only one step old.
And if you're right before a flip, then it's n minus one steps old.
So it seems like at different points in time, your target values look more like a moving target than others.
If you're right after that point where you set ϕ prime to phi, if you're right after that point where you set ϕ prime to phi, if you're right after that point, if you're right after one of these flips, then your target really looks like a moving target.
And if it's been a long time, then it really doesn't look like a moving target.
So this feels a little off.
It's not actually that big of a problem, but if it feels a little bit off, then one common choice that you could do is you could use a different kind of update, which is kind of similar to polyac averaging.
So those of you that are familiar with convex optimization might recognize this as essentially a variant of polyac averaging.
So a popular alternative is to set ϕ prime at every single step to be tau times the old ϕ prime plus one minus tau times the new ϕ.
So you can think of this as ϕ prime is gradually interpolating between its old value and the new value defined by ϕ.
And you would choose tau to be a pretty big number.
So for instance, you might choose it to be 0.999.
So that means that one part of out of a thousand is essentially coming from ϕ and the rest of the value is going to be 0.9999.
So for example, two- todo is ourselves customize Bailey.
We can change the sheep to one-displaystyle and the sheep to one-deep.
Now the way that that's going to work is if I get all of the animals, I'd actually scenecut them so that I do one-third of and then rotate every animal- bracket.
So if you take 2- None.
So what I'm looking at is a, kind of using an automatic stigator mode where you're justallah' defaulting, saying that I'm actually molecular compiling.
So, justallah.
Alt- cide зн but some, and that comes from the connection to polyac averaging.
I'm not going to go into it in this lecture, but if you want to learn more about why it's okay to linearly interpolate parameters of nonlinear functions in this way, look up polyac averaging.
And of course the caveat is that this only makes sense if ϕ prime is similar to ϕ.
So if ϕ prime was a totally different neural network trained in a totally separate way, this might be a little bit strange, but because you're gradually making ϕ prime more and more similar to phi, this procedure is actually alright.
And of course the nice thing about this is that now ϕ prime is updated the same way every single step, so every step is lagged by the same amount.