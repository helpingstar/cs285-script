[p.14]

All right.
In the next portion of today's lecture, we're going to discuss how this generic form of Fitted Q iteration that we covered can be instantiated as different kinds of practical deep reinforcement learning algorithms.

[p.15]

So first, let's talk a little bit more about what it means for Fitted Q iteration to be an off-policy algorithm.
So just to remind everybody, off-policy means that you do not need samples from the latest policy in order to keep running your RL algorithm.
Typically, what that means is that you can take many gradient steps on the same set of samples or reuse samples from previous iterations.
So you don't have to throw out your old samples.
You can keep using them, which in practice gives you more data to train on.
So intuitively, the main reason that Fitted Q iteration allows us to get away with using off-policy data is that the one place where the policy is actually used is actually utilizing the Q function rather than stepping through the simulator.
So as our policy changes, what really changes is this max.
Remember, the way that we got this max was by taking the argmax, which is our policy, the policy in an argmax policy, and then plugging it back into the Q value to get the actual value for the policy.
So inside of that max, you can kind of unpack it.
And pretend that it's actually Q_ϕ of s'_i comma argmax of Q_ϕ, and that argmax is basically our policy.
So this is the only place where the policy shows up.
And conveniently enough, it shows up as an argument to the Q function, which means that as our policy changes, as our action a'_i changes, we do not need to generate new rollouts.
You can almost think of this as a kind of model.
The Q function allows you to sort of simulate what kind of values you would get if you were to take different actions.
And then, of course, you take the best action if you want to most improve your behavior.
So this max approximates the value of π', our greedy policy, at s'_i.
And that's why we don't need new samples.
We're basically using our Q function to simulate the value of new actions.
So given a state and an action, the transition is actually independent of π.
Right?
If s_i and a_i are fixed, no matter how much we change π, s'_i is not going to change, because π only influences a_i, and here a_i is fixed.
So one way that you can think of Fitted Q iteration kind of structurally is that you have this big bucket of different transitions, and what you'll do is you'll back up the values along each of those transitions, and each of those backups will improve your Q value.
But you don't actually really care so much about which specific transitions they are, so long as they kind of cover the space of all possible transitions quiet well.
So you can imagine that you have this data set of transitions, and you're just plugging away on this data set, running Fitted Q iteration, improving your Q function each time you go around the loop.

[p.16]

Now, what exactly is it that Fitted Q iteration is optimizing?
Well, this step, the step where you take the max, improves your policy.
Right?
So in the tabular case, this would literally be your policy improvement.
And your step 3 is minimizing the error of fit.
So if you had a tabular update, you would just directly write those y_i into your table, but since you have a neural network, you have to actually perform some optimization to minimize an error against those 'y_i's, and you might not drive the error perfectly to 0.
So you could think of Fitted Q iteration as optimizing an error, the error being the Bellman error, the difference between Q_ϕ(s,a), and those target values y, and that is kind of the closest to an actual optimization objective.
But of course, that error itself doesn't really reflect the goodness of your policy.
It's just the accuracy with which you're able to copy your target values.
If the error is 0, then you know that Q_ϕ(s,a) = r(s,a) + γ⋅max_{a'} Q_ϕ(s',a').
And this is an optimal Q function, corresponding to the optimal policy π', where the policy is recovered by the argmax rule.
So this you can show maximizes reward.
But if the error is not 0, then you can't really say much about the performance of this policy.
So what we know about Fitted Q iteration is, in the tabular case, your error will be 0, which means that you'll recover Q^{*}.
If your error is not 0, then most guarantees are lost when we leave the tabular case.

[p.17]

All right.
Now let's discuss a few special cases of Fitted Q iteration, which correspond to very popular algorithms in the literature.
So, so far, the generic form of Fitted Q learning that we talked about has these three steps.
Collect the data set, evaluate your target values, train your neural network parameters to fit those target values, and then alternate these two steps K times.
And then after K times, go out and collect more data.
You can instantiate a special case of this algorithm with particular choices for those hyperparameters, which actually corresponds to an online algorithm.
So in the online algorithm, in step one, you take exactly one action, a_i, and observe one transition, (s_i, a_i, s'_i, r_i).
Then in step two, you compute one target value for that transition that you just took.
Very much analogous to how you calculate the advantage value in Actor-Critic, in online Actor-Critic, for the one transition that you just took.
And then in step three, you take one gradient descent step on the error between your Q values and the target value that you just computed.
So the equation that I have here, it looks a little complicated, but I basically just applied the chain rule of probability to that objective inside the argmin in step three.
So applying chain rule, you get dQ/dϕ(s_i, a_i) times the error (Q_ϕ(s_i, a_i) - y_i).
And the error in those parentheses, that (Q(s_i, a_i) - y_i), is sometimes referred to as the temporal difference error.
So, this is the basic online Q learning algorithm also sometimes called Watkins Q learning.
This is kind of the classic Q learning algorithm that we learn about in textbooks.
And it isn't a on-policy algorithm so you do not have to take the action a_i using your latest greedy policy.

[p.18]

So what policy should you use?
So your final policy will be the greedy policy.
If Q-learning converges and has error zero, then we know that the greedy policy is the optimal policy.
But while learning is progressing, using the greedy policy may not be such a good idea.
Here's a question for you to think about.
Why might we not want to use the greedy policy, the argmax policy, in step one while running online Q-learning or online Q iteration?
Take a moment to think about this question.
So part of why we might not want to do this is that this argmax policy is deterministic.
And if our initial Q function is quite bad, it's not going to be random, but it's going to be arbitrary.
Then, it will essentially commit our argmax policy to take the same action every time it enters a particular state.
And if that action is not a very good action, we might be stuck taking that bad action essentially in perpetuity, and we might never discover that better actions exist.
So in practice, when we run fitted Q iteration or Q-learning algorithms, it's highly desirable to modify the policy that we use in step one to not just be the argmax policy, but to inject some additional randomness to produce better exploration.
And there are a number of choices that we make in practice to facilitate this.
So one common choice is called ϵ-greedy.
This is one of the simplest exploration rules that we can use with discrete actions and that's something that you will all implement in homework 3.
ϵ-greedy simply says that with probability 1 - ϵ, you will take the greedy action and then with probability ϵ, you will take one of other actions uniformly at random.
So the probability of every action is (1 - ϵ) of this the argmax and then ϵ divided by the nubmer of actions(|A|) minus one otherwise.
This is called ϵ-greedy.
Why might this be a good idea?
Well, if we choose ϵ to be some small number, it means that most of the time we take the action that we think is best.
And that's usually a good idea, because if we've got it right, then we'll go to some good region and collect some good data.
But we always have a small but non-zero probability of taking some other action, which will ensure that if our Q function is bad, eventually we'll just randomly do something better.
It's a very simple exploration rule, and it's very commonly used in practice.
A very common practical choice is to actually vary the value of ϵ over the course of training.
And that makes a lot of sense, because you expect your Q function to be pretty bad initially, and at that point you might want to use a larger ϵ, and then as learning progresses, your Q function gets better, and then you can reduce ϵ.
Another exploration rule that you could use is to select your actions in proportion to some positive transformation of your Q values.
And a particularly popular positive transformation is exponentiation.
So if you take actions in proportion to the exponential of your Q values, what will happen is that the best actions will be the most frequent.
Actions that are almost as good as the best action will also be taken quite frequently, because they'll have similarly high probabilities.
But if some action has an extremely low Q value, then it will almost never be taken.
In some cases, this kind of exploration rule can be preferred over ϵ-greedy, because with ϵ-greedy, the action that happens to be the max gets much higher probability, and if there are two actions that are about equally good, the second best one has a much lower probability.
Whereas with this exponentiation rule, if you really have two equally good actions, you'll take them about an equal number of times.
The second reason it might be better is if you have a really bad action, and you've already learned that it's just a really bad action, you probably don't want to waste your time exploring it.
Whereas ϵ-greedy won't make use of that.
So this is sometimes also called the Boltzmann exploration rule, also the softmax exploration rule.
We'll discuss more sophisticated ways to do exploration in much more detail in another lecture in the second half of the course, but these simple rules are hopefully going to be enough to implement basic versions of Q iteration and Q learning algorithms.

[p.19]

All right, so to review what we've covered so far, we've discussed value-based methods, which don't learn a policy explicitly, but just learn a value function or Q function.
We've discussed how if you have a value function, you can recover a policy by using the argmax, and how we can devise this fitted Q iteration method, which does not require knowing the transition dynamics, so it's a true model-free method.
And we can instantiate it in various ways, as a batch mode off-policy method, or an online Q learning method, depending on the choice of those hyperparameters, the number of steps we take to gather data, the number of gradient updates, and so on.