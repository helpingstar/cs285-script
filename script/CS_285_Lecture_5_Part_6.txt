All right.
In the last portion of today's lecture, I'm going to discuss advanced policy gradients.
This material will go by a bit faster than the rest of the lecture, so if it's a little hard to follow, don't worry.
Please ask some questions in the comments, and we can discuss it more in class.
We will also have an entire other lecture later on in the course on even more advanced policy gradients materials.
So the particular issue that I want to discuss is a numerical issue that afflicts policy gradients, particularly in continuous action spaces.
To illustrate this issue, let me first describe an example problem.
Let's say that you have a one-dimensional state space.
So your state is essentially a number line, and your goal is to reach the state s equals zero.
You also have a one-dimensional action space.
So let's say that you're located at this state, and your actions can take you left or right.
Your reward is negative s^2 minus a^2.
So you get a penalty based on squared distance from s zero, and you also get a penalty for taking large actions.
Your policy is going to be univariate and normally distributed with just two parameters.
One parameter k multiplies the state, so your mean is linear in the state, and the other parameter determines your variance, σ.
So you have k and σ as your policy parameters θ.
So you can think of the policy as basically a little Gaussian centered at your current location, and your action is k times your current state.
So you're going to take this kind of noisy walk, hopefully towards the goal at s equals zero.
Now the convenient thing with having a two-dimensional parameter space is that we can actually visualize the entire vector field corresponding to the gradient at all locations in the parameter space.
This figure is borrowed from an excellent paper by Peters and Schaal, which I'm going to cite at the end of this portion of the lecture.
The little blue arrow here shows the gradient, normalized to be unit length.
The horizontal axis is the first parameter k, and the vertical axis is the second parameter σ.
The optimal setting for the parameters is k equals negative one and σ equals zero.
So it's in the middle at the bottom of this plot.
But one of the things you might notice from looking at this plot is that the arrows don't actually point towards the optimum.
And the reason for this is that as σ gets smaller and smaller, the gradient with respect to σ gets larger and larger.
If you look at the form for the for the Gaussian probability, you'll notice this simply because the probability tracks as 1/σ^2.
So when you take the derivative, you get a one over σ to the fourth term, which means that as σ gets smaller the derivative gets larger.
The derivative with respect to k is still there, but the derivative with respect to σ is comparably much larger, so that when we renormalize the gradient, the σ portion completely dominates as σ gets smaller.
So that means that if we follow this gradient, it's going to take us a very very long time to reach the optimal parameter setting, because we'll spend all of our time just reducing σ.
Now those of you that are familiar with numerical methods will probably recognize this as an issue of poor conditioning.
The intuition is that this is a essentially the same problem as optimizing, let's say, a quadratic function where the eigenvalues of the corresponding matrix have a very large ratio.
So if you have a quadratic function with some eigenvalues that are very large and some that are very small, then first-order gradient descent methods are really going to struggle in this kind of function.
This is essentially the same type of issue.
Now, again, if you have some background in numerical methods, at this point you might also be thinking, well, if the problem is poor conditioning, can we solve that problem by using a preconditioner?
And the answer is yes, and in fact what we're going to describe next could be viewed as a preconditioner.
But we're going to actually discuss it from a slightly different perspective, from the perspective of the dependence of your gradient on parameters.
So what I'm going to discuss next is how we can arrive at a covariant or natural policy gradient.
So here is the picture from the previous slide.
When we take a gradient step, via policy gradient, we take a gradient ascent step, choosing the step size for this type of gradient can be very delicate because some parameters affect the policy distribution a lot and some don't affect it very much.
So it's very hard to pick a single step size that works well both for k and for σ, because the derivative with respect to σ is going to get really really really large, whereas the one for k won't.
So what's really going on here is that different parameters affect the policy to different degrees.
Some parameters change the probabilities a lot, others don't change it very much.
But you want all of the parameters to reach their optimal value, so intuitively what you would like to do is to essentially have larger learning rates for those parameters that don't change the policy very much, and smaller learning rates for those that change it a lot.
If we want to view this a little bit more mathematically, one of the things we can do is look at the constraint optimization view of first-order gradient ascent.
So first-order gradient ascent, can be viewed as iteratively solving the following constraint optimization problem.
Take the argmax with respect to θ' that's the new parameter value, where the objective is the first-order Taylor expansion of your original objective that's given by (θ'-θ) times ∇J and you have a constraint that says (θ'-θ)^2 should be small.
So it's like saying within an ϵ ball around your current parameter value find the parameter value that maximizes the linearization of your objective.
That's essentially what first-order gradient descent is doing, and you can think of α as basically the Lagrange multiplier for that constraint.
So those of you that have studied mirror descent or projected gradient descent would probably recognize this equation.
Usually we pick α rather than ϵ, but α is basically just the Lagrange multiplier that corresponds to ϵ.
So what this means is that when we do first-order gradient descent, we're finding the best value for θ' within an ϵ ball, but that ϵ ball is in θ space.
Now our linearized objective is valid in only a small region around our current policy.
That's why we can't use very large step sizes.
But that region is very awkward to select if you have to select it in parameter space because some parameters will change the policy a lot and some will change it very little.
So intuitively what we would like to do is we would like to somehow reparameterize this process so that our steps are of equal size in policy space rather than parameter space, which would essentially rescale the gradient so that parameters that change the policy a lot get smaller rates, parameters that change the policy very little get larger rates.
So this is basically the problem.
This controls how far we go, and it's basically in the wrong space.
So can we rescale the gradient so that this doesn't happen?
What if we instead iteratively solve this problem, maximize the linearized objective, but subject to a constraint that the distributions don't change too much?
So here D is some measure of divergence between π_θ' and π_θ, and we'd like that divergence measure to be less than or equal to ϵ.
So we'd like to pick some parameterization-independent divergence measure, a divergence measure that doesn't care about how you're parameterizing your policy, just which distribution it corresponds to.
A very good choice for this is the KL divergence.
The KL divergence is a standard divergence of bregman divergence and I won't go into too much detail about how KL divergences are defined or derived, just that it's a measure of divergence on distributions, and it is parameter-independent, meaning that no matter how you parameterize your distributions, the KL divergence will remain the same.
Now, the KL divergence is a little complicated to plug into this kind of constrained optimization.
We want that constrained optimization to be very simple, because we're going to do that at every step of our gradient ascent procedure.
But if we take the second-order Taylor expansion, the KL divergence, around the point θ' equals θ, then the KL divergence can be expressed as approximately as a quadratic form for some matrix F, right?
That's just what a second-order Taylor expansion is.
And it turns out that F is equal to what's called a Fisher information matrix.
The Fisher information matrix is the expected value under π_θ, that's your old policy, of ∇log π times ∇log π transpose.
So it's the expected value of the, the outer product of the gradient with itself.
Now notice that the Fisher information matrix is an expectation under π_θ, which should immediately suggest that we can approximate it by taking samples from π_θ, and actually trying to estimate this expectation.
And that's in fact exactly what we're going to do.
So now we've arrived at this formulation for our covariant policy gradient.
At every single step of our optimization, we maximize the linearized objective, subject to this approximate divergence constraint, which is just the difference between θ' and θ, under the matrix F.
So it's just like what we had before, (θ' - θ), only before it was under the identity matrix, and now it's under the matrix F.
And if you actually write down Lagrangian for this, and solve for the optimal solution, you'll find that the solution is just to set the new θ to be θ + α where α is langrange multipler of F^{-1}∇_θ J(θ).
So before we had θ + α ∇θ J(θ).
Now we have θ + α F^{-1} ∇θ J(θ).
So F is basically our pre-conditioner now.
And it turns out that if you apply this F^{-1} in front of your gradient, then your vector field changes in a very nice way.
So the picture on the right shows what you get by using this so-called natural gradient.
And now you can see that the red lines actually very nicely point towards the optimum.
And that means that you can converge a lot faster, and also you don't have to work nearly as hard at tuning your step size.
Now there are a number of algorithms that use this trick.
The classical one, natural gradient or natural policy gradient, selects α.
A more modern variant called trust region policy optimization selects ϵ and then derives α.
So the way that you derive α is by solving for the optimal α at the same time while solving for F^{-1} ∇θ J(θ).
We won't go into how to do this, but the high level idea is that by using conjugate gradient you can actually get both α and the natural gradient simultaneously.
So for more details on that, you can check out the paper called trust region policy optimization.
The takeaway from all of this is that the policy gradient can be numerically very difficult to use because different parameters affect your distribution to very different degrees.
And you can address this by using the natural gradient, which simply requires multiplying your gradient by F^{-1}, where F^{-1} is an estimate of the Fisher information matrix.
And you can do this efficiently by using conjugate gradient.
Alright, a few notes on advanced policy gradient topics.
What more is there?
Well, next time we'll talk about Actor-Critic algorithms, where we'll introduce value functions and Q functions, and talk about how those can further decrease the variance of the policy gradient.
And then later in the class we'll talk more about natural gradient, automatic step size adjustment, and trust regions.
For now, let me briefly go over some papers that actually use policy gradients in interesting ways.
This is a paper actually by myself and Vladan Kolton from 2013, that used an off-policy version of policy gradient to incorporate examples.
So here, example demonstrations were incorporated using importance sampling, but unlike imitation learning, the policy wasn't just trying to copy the examples, it was actually trying to do better than those examples by using policy gradients.
And this used neural network policies for some locomotion tasks.
Here are some videos from the trust region policy optimization paper.
So this paper used a natural gradient with automatic step size adjustment, with both continuous and discrete actions.
And there was some code available for this if you want to check that out, from a paper from 2016 by Rocky Duan.
If you want to read more about policy gradients, here are some suggested readings.
The classical papers.
REINIFORCE was introduced in this paper by Williams in 1992.
This paper by Baxter and Bartlett introduced the, what I call the causality trick in the lecture.
They call it GPOMDP.
This is actually not the first paper to introduce it.
I'll actually mention the first paper when I talk about actual critic in the next lecture.
And this paper by Peters and Schaal describes the natural gradient trick with some very nice illustrations.
Deep RL papers that use policy gradients.
The guided policy search paper that I mentioned before, which uses important sampled policy gradients.
This is the trust region policy optimization paper.
And then the PPO paper.
So these would be ones to check out if you're interested in policy gradients for Deep RL.