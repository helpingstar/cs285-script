In the next section of today's lecture, I'm going to discuss some practical considerations that we need to take into account when actually implementing Q-learning algorithms, and then some improvements that can make them work a bit better.
So one question that we can start with, are our Q values actually accurate?
We can think of Q values as this kind of abstract object that we use inside reinforcement learning to help us improve our policy and get that arc max.
But a Q function is also a prediction.
It's a prediction about the total reward that you will get in the future if you start in a particular state of action and then follow your policy.
So it makes sense to ask whether these predictions are actually accurate predictions.
Do they match up with the reality?
Do they match up with what you actually get when you run the policy?
So if we look at the kind of a basic learning curve, where the x-axis is the number of iterations of Q, and the y-axis is the average reward per episode, and we look at it on a bunch of, let's say, Atari games, we'll see that for all of these Atari games, our average reward per episode is going up, so things are getting better.
If we look at the average Q values that are being predicted, and that's the two plots on the right, we'll see that the Q function is predicting larger and larger Q values as training progresses.
And then in the next section, we're going to look at the Q function.
So this intuitively makes sense.
As training progresses, our policy gets better, it's getting higher rewards, so our Q function should also predict that it's getting higher Q values.
So as predicted, Q increases, and so does the return.
We can also look at whether the actual Q values or value function values occur in places that essentially anticipate future rewards.
So this is the game of breakout.
For those of you that are not familiar with breakout, the goal was to use the little orange paddle at the bottom to hit a ball, and the ball is reflected by the paddle, bounces up, and hits these rainbow-colored blocks.
And every block you break gets you a point.
A particularly cool thing you can do in breakout is if you break through all of the blocks on one side, which is happening there in panel number three, then you can get the ball to bounce all the way up, and it will actually bounce off the ceiling, and ricochet off the top blocks, and they'll get you lots of points, because it's just bouncing back and forth there, breaking all the blocks from the top.
So it's quite a cool strategy if you can break through to the top like that and have it bounce around.
And the graph at the bottom shows the value function value, which is essentially the Q value for the best action, at different points in time, with the particular frames one, two, three, and four labeled on the graph.
And what you can see here is that some of these values actually make a lot of sense.
So number one, you're about to break a block, and you have the highest value.
After you break that block, you bounce back down, and your value dips, because you know that you're not going to get any points for a little while, while your ball flies down and needs to get bounced from the paddle.
In step three, you're about to break through the top, so your value becomes quite large, but in step three, you actually don't quite make it.
So that last red block that you break, you break it, but then you bounce back down.
So your value goes down for a while, and then it rises right back up.
And in step four, your value is actually at its largest, even though you actually haven't broken any blocks for a while.
So in step four, you just bounced off the paddle, you haven't broken any blocks, but you're about to ricochet off the ceiling, and you're about to get those mad points, so that's why your value function is actually at the largest value.
And it actually only goes down from there, because once you actually get the points, the value function is going to drop, because it knows you've received your points.
You're going to get fewer points left over.
So that all makes sense.
That all seems reasonable.
And we can also look at the relative value, the Q values, for different actions.
So these are frames from the game Pong.
So in Pong, you need to use your paddle, which is green on the right side, to hit the ball so that it ricochets and goes to the other side, and your opponent with the orange paddle needs to hit it back, and your goal, kind of like in tennis, is to hit the ball back so that your opponent can return it.
If your opponent can't return it, then you get a point.
If your opponent can return it, then they might get a point on you, because you might fail to hit it back.
So what we're seeing in frame 1 is that all actions have about the same Q value.
Take a moment to think about why this might be.
Why does this make sense?
Well, the reason it makes sense is because when the ball is quite far away, many different actions will still allow you to catch the ball later.
Even though it might seem like moving the paddle up is the right thing to do, in reality the Q function here is very good, and it understands that even if it fails to move the paddle up, it'll be able to move it up at the next time step.
Which actually means that the Q values of different actions at this time step are about equal.
It's a little counterintuitive at first, but it really makes sense.
At time step 2, now the ball is getting pretty close to the zone where you have to return it, and here now the up action has a much larger Q value.
So the Q function understands that it still has a split second chance to return the ball, but only if it moves up right now.
So now the Q value for moving up is very large, the Q value for moving down or staying still is very, very negative.
And of course in step 4, once you've actually returned the ball, again it's saying that the values for different actions don't really matter.
So again, this basically agrees with our intuition.
In terms of their relative values, the Q values make sense with respect to actions, and they make sense with respect to states.
But there's a little bit of a problem.
While the relative Q values of different states and actions seem to make sense on a successful training run, their actual absolute values will in practice actually not be very predictive of real values.
And you can verify this yourself in homework 3 when you implement Q learning.
You can measure the numerical value of the Q value that you're getting, and then measure the actual return that you get, and compare those two numbers, you'll find they don't agree very well.
There are a few details that you have to get right.
One thing that you have to get right is that you have to make sure that when you calculate the true value, you use a discounted estimator.
So you calculate the true value by taking the trajectories that you actually executed, and taking the reward times step 1, plus γ times the reward at 2, plus γ squared times the reward at 3, etc., etc., etc., and then compare that to the Q value at step 1.
Because the Q value at step 1 is trying to predict the expected sum of discounted rewards.
So if you compare that to the discounted sum of rewards that you actually got, if your Q value is a good predictor, you should see that those are similar.
And what you'll actually see is that they're not very similar.
So what these graphs are showing is basically exactly this.
What you should look at is the red lines.
The blue lines, don't worry about those, we'll talk about those later.
But the red lines represent the following.
The kind of spiky red line, the one that's usually higher, represents the estimate of your Q function.
So this is basically how, this is what your Q function thinks the total discounted reward that you'll get will be.
The solid flat line represents the actual sum of discounted rewards that you're actually getting when you run that policy.
And what you're seeing here is that the Q function estimates are always much, much larger than the actual sums of discounted rewards that you're getting.
And that seems kind of strange.
Like why is it that the Q function seems to systematically think it's going to get larger rewards than it actually gets?
This is not a fluke.
It's not just that the Q function is wrong and it can be above or below the true reward, it's actually systematically larger.
And this is a very consistent pattern, and if you try this in homework 3, you'll also see this pattern.
So why is that?
This problem is sometimes referred to as overestimation in Q-learning.
And it has actually a fairly straightforward and intuitive reason.
Let's look at how we're computing our target values.
When you compute your target value, you take your current target Q function, Q5', and you take the max of that Q function with respect to the action, AJ'.
And it's really this max that's the problem.
So here's how we can think about why a max could cause overestimation.
Let's forget about Q values for a minute, and let's just imagine that you have two random variables, x1 and x2.
You could think that maybe x1 and x2 are normally distributed random variables.
So maybe they have some true value plus some norms.
You can prove that the expected value of the max of x1 and x2 is always greater than or equal to the max of the expected value of x1 and the expected value of x2.
The intuition for why this is true is that when you take the max of x1 and x2, you're essentially picking the value that has the larger noise.
And even though the noise for x1 and x2 might be 0-mean, maybe they're both univariate Gaussians, the max of two 0-mean noises is not in general 0-mean.
So you can imagine that one noise is positive or negative with 50%, the other is positive or negative with 50%.
But when you take the max of the two, if either of them is positive, you'll get a positive answer.
So, of course, the probability that one of the two noises is positive is going to be pretty high.
Right?
So in order for them to both be negative, that has only a 25% probability.
So one of them being positive, that's 75% probability.
So with 75% probability, when you take the expected value of their max, you'll get a positive number.
When you take the max of their expected values, you'll get 0, because their expected values are 0.
But when you take the expected value of their max, you'll get a positive value.
Now, what does this have to do with Q-learning?
Well, if you imagine that your Q-function is not perfect, if you imagine that your Q-function kind of looks like the true Q-function, plus some noise, then when you take this max and the target value, you're doing exactly this.
So imagine that your Q5' for different actions represents the true Q-value of that action, plus some noise.
So it might be up and down.
And those errors are not biased, so those errors are just as likely to be positive as negative.
But when you take the max in the target value, then you're actually selecting the positive errors.
And for the same reason that the expected value of the max of X1 and X2 is greater than or equal to the max of their expected values, the max over the actions will systematically select the errors in the positive direction, which means that it will systematically overestimate the true Q-values, even if your Q-function initially does not systematically have errors that are positive or negative.
So for this reason, the max over A' of Q5' S' A' systematically overestimates the next value.
It basically preferentially selects errors in the positive direction.
So how can we fix this?
Well, one way that we can think about fixing this is to note, if we think back to the third Q-iteration, the way that we got this max was by basically modifying the policy iteration procedure.
So we had our greedy policy, which is the argmax over A', and then we then send that argmax back into our Q-function to get its value.
So this is just another way of saying that the max over A' of Q5' is just Q5' evaluated at the argmax.
And this is actually the observation that we're going to use to try to mitigate this problem.
See, the trouble is that we select our action according to Q5'.
So if Q5' erroneously thinks that some action is a little bit better because of some noise, then that's the action we'll select, and then the value that we'll use for our target value is the value for that same action, which has that same noise.
But if we can somehow de-correlate the noise in the action selection mechanism from the noise in the value evaluation mechanism, then maybe this problem can go away.
So the problem is that the value also comes from the same Q5', which has the same noise as the rule that we used to select our action.
Alright, so one way to mitigate this problem is to use something called double-Q learning.
If the function that gives us the value is de-correlated from the function that selects the action, then in principle this problem should go away.
So the idea is to just not use the same network to choose the action as the network that we use to evaluate the value.
So double-Q learning uses two networks.
One network, which we're going to call Phi A, and another network, which we're going to call Phi B.
And Phi A uses the values from Phi B to evaluate the target values, but selects the action according to Phi A.
So if you assume that Phi B and Phi A are de-correlated, then the action that Phi A selects for the argmax will be corrupted by some noise, but that noise will be different from the noise that Phi B has, which means that when Phi B evaluates that action, if the action was selected because it had a positive noise, then Phi B will actually give it a lower value.
So the system will be kind of self-correcting.
And then analogously Phi B is updated by using Phi A as its target network, and then Phi B as the action selection rule.
So this is the essence of double-Q learning, and its purpose is to de-correlate the way that you select the action from the way that you evaluate the value of that action.
So if the two Q networks are noisy in different ways, then in principle the problem should go away.
Now in practice, the way that we can implement double-Q learning is without actually adding another Q function, but actually using the two Q functions we already have.
So we already have a Phi and a Phi prime, and they are different networks, so we'll just use those in place of Phi A and Phi B.
So in standard Q learning, if we write it out in this argmax way, which is exactly equivalent, our target value is Q Phi prime, evaluated at the argmax from Q Phi prime.
In double-Q learning, we select the action using Q Phi, but evaluate it using Q Phi prime.
So now as long as Phi prime and Phi are not too similar, then these will be de-correlated.
So this is the only difference.
We're using Phi to select the action instead of Phi prime.
And we still use the target network to evaluate our value to avoid this kind of moving targets problem.
Now you could say that we do still have a little bit of the moving targets problem, because as our Phi changes, so does our action, but presumably the change in the argmax is a very sudden, discrete change, and it doesn't happen all the time.
So if you have three different actions, the argmax isn't going to change as often.
Now something I might mention here is that, and many of you might already be thinking about this, Phi prime and Phi are of course not totally separate from each other, because periodically you do set Phi prime to be equal to Phi.
So this solution is far from perfect.
It doesn't totally de-correlate Phi prime and Phi.
But in practice, it actually tends to work pretty well, and it actually mitigates a large fraction of the problems with overestimation.
But of course not all of them.
Alright.
There's another trick that I should mention that we can use to improve Q-learning algorithms, and it's similar to something that we saw in the actor critic lecture.
And that's the use of multi-step returns.
So our Q-learning target is, and here I intentionally am writing it out with time steps, is Rjt plus the max at t plus one.
And where does the signal in this learning process come from?
Well, if your initial Q function is very bad, it's essentially random, then almost all of your learning has to come from the R.
So if your Q Phi prime is good, then the target values do most of the heavy lifting.
If your Q Phi primes are bad, then the only thing that really matters is the reward, and that second term is essentially just contributing noise.
And early on in training, your Q function is pretty bad, so almost all of your learning signal really comes from the reward.
Later on in training, your reward, you know, your Q function becomes better, and the Q values are much larger in magnitude than the rewards, so later on in training, the Q values dominate.
But your takeoff, your initial learning period, can be very slow if your Q function is bad, because this target value is mostly dominated by the Q value.
So this is quite similar to what we saw in actor critic, when we talked about how the actor critic style update that uses the reward plus the next value has lower variance, but it's not unbiased, because if the value function is wrong, then your advantage values are completely messed up.
And Q learning is the same way.
If the Q function is wrong, then your target values are really messed up, and you're not going to be making much learning progress.
The alternative that we had in the actor critic lecture is to use a Monte Carlo sum of rewards, because the rewards are always the truth, they're just higher variance, because they represent a single sample estimate.
We can use the same basic idea in Q learning.
So Q learning, by default, does this kind of one-step backup, which has maximum bias and minimum variance, but you could construct a multi-step target, just like an actor critic.
Take a moment to imagine what this multi-step target would look like.
If you have a piece of paper in front of you, consider writing it down, and then you can check what you wrote down against what I'm going to tell you on the next slide.
Sorry, it's actually on this slide.
So the way that you can construct a multi-step target is basically exactly analogous to what we saw in the actor critic lecture.
So the way that you can construct your multi-step target is by not just using one reward, but making a little sum from (t'=t) to t plus n minus one, and for each of those, you take rj t prime multiplied by γ to t minus t prime.
And you can verify that if n equals one, then you recover exactly the standard rule that we had for Q learning.
But for n larger than one, you sum together multiple reward values, and then you use your target network for the t plus n step multiplied by γ to the n.
So this is sometimes called an n-step return estimator, because instead of summing the reward for one step, you sum it for n steps.
So this is the n-step return estimator.
And just like with actor critic, the trade-off of the n-step return estimator is that it gives you a higher variance because of that single sample estimate for r, but lower bias because even if your Q function is incorrect, now it's being multiplied by γ to the n, and for larger values of n, γ to the n might be a very small number.
Okay.
So let's talk about Q learning with these n-step returns.
It's less biased because the target, because the Q values are multiplied by a small number, and it's typically faster early on because when the target values are bad, those sums of rewards really give you a lot of useful learning signal.
Unfortunately, once you use n-step returns, this is actually only a correct estimate of the Q value when you have an on-policy sample.
So the reason for this is that if you have a sample collected with a different policy, then that second step, T plus 1, might actually be different for your new policy, right?
Because if on the second step you take a different action, that won't match what you're getting from your n-step return.
So n-step returns technically are not correct with off-policy data anymore.
With off-policy data, technically you're only allowed to use n equals 1.
With n equals 1, everything is pretty straightforward because you're not actually assuming anywhere that your transition came from your policy.
Your Q function is conditioned on action, so that'll be valid for any policy, and your second time step, where this would matter, in the second time step you actually take the max with respect to action, you don't use the action that was actually sampled.
So for n equals 1, it's valid to do off-policy, but for n greater than 1, it's no longer valid.
Basically, your new policy might never have landed in the state SjT plus n.
So why?
Because you actually end up using the action from the sample for those intermediate steps, which is not the action that your new policy would have taken.
As an interesting thought exercise, and this is something you can think about at home after the lecture, you could imagine how to utilize the same trick that we used to make Q-learning off-policy to try to make this n-step version off-policy.
As a hint, to make the n-step version off-policy, you can't learn a Q function anymore, you have to learn some other object, condition on some other information.
And if you think a little bit about how you could do this, that might shed some light on kind of giving you a better intuitive understanding for how it is that Q-learning can be off-policy.
So as a homework exercise after the lecture, maybe take a moment to think about how to make n-step returns off-policy, and what kind of object you would need to learn to make that possible.
So the estimate that we get from regular n-step returns is an estimate of Q for pi, but for that you need transitions from pi for all the intermediate steps.
And this is not an issue when n equals 1.
So how can we fix it?
Well, we can ignore the problem, which often works very well.
The other thing we can do is we can dynamically cut the trace.
So we can dynamically choose n to only get on-policy data.
Essentially we can look at what our deterministic greedy policy would do, we could look at what we actually did in the sample, and we can choose n to be the largest value such that all of the actions exactly match what our policy would have done.
And that will also remove the bias.
So this works well when data is mostly on-policy, and the action space is pretty small.
Another thing we can do is importance sampling.
So we can construct a stochastic policy and importance weight these n-step return estimators.
I won't talk about this in detail, but if you want to learn more about this, check out this paper called Safe and Efficient Off-Policy Reinforcement Learning by Munoz et al.
And then there's this mystery solution that I haven't told you about where you don't do any of this stuff, but you condition the Q function on some other additional information that allows you to make it off-policy.
And that's a solution that you can think about in your own time after the lecture.