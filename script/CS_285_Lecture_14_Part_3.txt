All right.
So in the next portion of the lecture, we'll dive a little deeper into this concept of state coverage and we'll generalize the notion of maximizing the entropy of your state more generally to matching some target distribution over states.
And in the course of this, I think we'll see some interesting connections between the kind of exploration concepts that we're discussing in this lecture and the kind of concepts that we covered on Monday.
All right.
So let's start with a little aside.
Let's talk about how we can do unsupervised exploration with intrinsic motivation objectives.
So intrinsic motivation is another term that's used to refer to these kind of novelty seeking things that we saw on Monday, like pseudocounts and so on.
So the common method of exploration that we saw on Monday is that you somehow incentivize your policy, π a given s, to explore diverse states, to visit novelties.
So you can do this by visiting novel states that have not been visited very frequently before.
And you could do this before seeing any reward.
On Monday, this was because the reward is very delayed or very sparse.
It could be done in settings where the reward is absent altogether.
But what will you actually get when you do this if there's no reward at all?
Well, if you reward visiting novel states, you know, essentially if a state is visited often, then it becomes not novel.
So you can add a variety of exploration bonuses that will do this.
We learned about things like counts.
The one that I'll talk about here.
Here is a very simple bonus, which is just negative log p of s.
You could use, you know, plus 1 over n of s, negative log p of s.
This is sometimes referred to as intrinsic motivation.
It's just another variant on the same idea.
But everything I'm going to say is also true for pseudocounts, ex2, hash exploration, all these things.
So let's say that we're basically penalizing density under π.
So we want to visit states with low density.
So you could imagine the following procedure.
Update your policy to maximize this reward with this bonus.
And then update your state distribution to fit your current state marginal and then repeat.
Right?
Very standard procedure.
Very similar to what we saw on Monday.
Well, what you're going to get if you do this when there's no reward at all is that when your policy goes and does something, then the density estimator will fit whatever your policy did.
And then the policy will go and do something else.
And then the policy will go and do something else.
And then the density estimator will fit what that policy did.
And then it will go and do something else.
And so on and so on.
So the density estimator, this p_π of s, it will eventually have high coverage.
It will assign reasonably high probabilities to all states.
But the policy that you'll end up with will kind of be arbitrary.
The policy will end up kind of chasing its tail all around the space.
You won't actually end up with a policy that gets uniform coverage over states.
The final policy will be arbitrary.
It will just go to whatever policy.
It will just go to whatever policy.
The previous policy has been to less often.
But the density estimator will get good coverage when you do this.
So this is not by itself a great way of solving that original problem I posed, where what you want is a policy that goes to many different places so that it can select between them.
So what if you actually do want a policy that gets good coverage?
So you don't just want a density estimator that has uniform coverage over the state space, but you actually want a policy that will randomly go to different states and will have about equal probability.
However, it doesn't give you an optimal estimated probability of landing in all possible states.
So the state marginal matching problem can be posed as learning a policy π e given s so as to minimize the KL divergence between state marginal, p_π of s, and some target state marginal, p star of s.
If p star of s is uniform, then you're just maximizing state entropy.
But if it's not uniform in general, you might be matching some target state entropy.
These are very similar problems.
So can you use this studying inequality to determine If p star of s is uniform, then you're just maximizing state entropy.
If it is not uniform, then you might match some target state.
intrinsic motivation idea from before?
Can you essentially use one of these novelty-seeking exploration objectives?
So let's construct our reward analogously to before.
So we're going to construct it as log p star which is the desired state distribution minus log p_π which is our current state distribution.
Now right off the bat something you might notice if you're up to speed on your information theory definitions is that the RL objective will be the expected value of this under the stationary distribution.
So it'll be the expectation under p_π of s of R tilde and the expectation under p_π of s of R tilde is exactly that KL divergence up there.
So that's kind of interesting.
It seems like the RL objective is exactly the quantity that we want.
So does that imply that RL will optimize this?
Well not exactly.
Because RL is not aware of the fact that the reward itself depends on the policy.
So this does not by itself perform state marginal matching for a very subtle reason.
Even though the objective is exactly the state marginal matching objective, the algorithm, the RL algorithm, is not aware of the fact that the minus log p_π of s depends on π.
So as a result you get this tail chasing problem that I had on the previous slide.
You get this issue where the policy will keep jumping to different places and at the end will not actually minimize the KL divergence.
But let's try to sketch out what that algorithm will look like anyway, and then we'll see that there's actually a very simple fix that will fix this problem.
So here is how we can sketch out this algorithm and I'm going to use somewhat tedious notation for- this notation will be important later.
So, at every iteration, we're going to learn a policy π k where k here indexes the iterations, So the first iteration is K equals 1, the second iteration is K equals 2, etc.
We'll learn a policy π k to maximize the expected value under π of r tilde k.
And again, I'm using the superscript k on r tilde to denote that this is the r tilde that used the density estimator from iteration k.
And then you will update your density estimator, maybe it's a variational autoencoder or some other distribution, to fit the current state marginal, the state marginal of this new policy π k.
And then you'll repeat this process.
So at the end, let's say that this orange circle is the density we're trying to match, your policy will keep jumping around, so at the end your final policy, let's say π 4 here, is going to some arbitrary place, it's going to the lower right, but the green circle is the density estimate for all the other states.
And the density estimate for all these policies actually is not too far off from the orange circle.
So we need to modify step 2 a little bit, we need to update π k s to fit all the states seen so far, not just states from the latest policy, but states from all the policies, right?
So then we get the union of the green circles, not just the last green circle.
That's a very easy change to make, essentially fit your density estimator to your replay buffer.
But another change that we're going to make is instead of returning this latest policy, we'll actually return a mixture policy.
So the final policy that we'll return will be a mixture model that averages together all the policies seen so far.
So the end result is not one policy, it's actually many policies, and the way that you're going to actually do this is you're going to run a randomly chosen iterate, a randomly chosen π k.
That might seem like kind of a weird decision.
Like you'd think that the last policy would be the best one.
Why are we randomly selecting from among all the policies we saw during learning?
Well, it turns out that this procedure does perform marginal matching, and proving this requires a little bit of game theory.
So the thing is that the last...
The state distribution where p_π of s is equal to p star of s is the Nash equilibrium of a two-player game.
The players in that game are the state density estimator and the policy.
So this is a game between π k and p_π k.
There's a special case here.
If...
p star of s is a constant, then you have a uniform target.
And maybe it's a little easier to think about it that way.
In that case, the KL diverge is just the entropy.
Now, it turns out that the way that you can recover the Nash equilibrium in a two-player game like this is by just having the players play against each other, meaning that each time each player gives the best response, so the best response for the density matching algorithm is to actually fit the density.
The best response for the...
The best response for the policy is to maximize r tilde.
But simply running this best...
iterated best response algorithm doesn't actually produce a Nash equilibrium.
It turns out that you get a Nash equilibrium if you do what's called self-play.
And in self-play, what you're supposed to do is you're supposed to return the historical average of all the iterates that you've seen.
So the final iterate is not the Nash equilibrium, but the mixture of all the iterates is the Nash equilibrium.
And this is a very well-known result in game theory.
So essentially, you can prove that this π^{*} that I have in step three, which is a mixture of all the π k's, is the Nash equilibrium for this two-player game.
And that means that it's going to be minimizing the scale divergence between p_π and p star.
If you want to learn more about this, if you want to go into more detail about this, check out these two papers linked at the bottom of the slide, Efficient Exploration via State Marginal Matching and Provably Efficient Matching.
So now it should be exploration.
Okay, so a few experiments.
This is a little ant robot that's supposed to run around in this maze with three different wings.
And the SAC, this is just a standard RL algorithm, doesn't explore all three wings equally.
It kind of gravitates, in this case, towards the top right one, whereas the state marginal matching does cover all of them equally.
So it gets better coverage.
And there's some quantitative results as well.
So anyway, high-level idea to take away from.
This is that the individual iterates that you get when you run intrinsic motivation do not get good state coverage.
They do not match target distributions.
But if you average together those iterates, then you do.
And the way you prove that is by using the theory of self-play, which you can show is a Nash equilibrium of this two-player game.
Okay.
One thing I want to talk about briefly next is, so far we talked about how, these unsupervised exploration methods, they aim to get good coverage, either a uniform distribution over goals or matching some state distribution, matching maybe a uniform distribution.
But is coverage of valid states actually a good exploration objective?
Like, why do we want to cover as many states as possible?
So in SKUFIT, we were covering the space of goals.
In state marginal matching, if you have the special case where P star is a constant, then you're maximizing state entropy.
They're kind of more or less the same.
So what is this a good idea?
Well, here's a little result of this.
It's kind of an obvious result.
I'm going to call it somewhat humorously Eisenbach's theorem after Ben Eisenbach, who's the student that actually wrote this out in the paper linked at the bottom.
But it's not really a theorem.
It's kind of a really trivial result that follows from classic maximum entropy modeling.
What you can show is that if at test time, an adversary chooses the worst possible goal, G, there's actually a single answer to what goals you should practice during training.
So if at test time, you're going to get the worst possible goal, essentially, when you come home in the evening, your robot has done the practicing, you're intentionally going to give the robot the hardest test.
Like, you know, you hate the company that made this robot.
You want to give them a one-star review.
You're going to intentionally give it the hardest task just to watch it fail so that you can complain afterwards.
If you're going to give the robot the worst possible goal and the robot knows that you're going to give it the worst possible goal, and the robot knows that you're going to give it the worst possible goal, which goals should it practice during training?
How should it construct that training distribution?
Take a moment to think about this.
So it turns out that we can show that the best distribution to use during training, if you believe that you're going to get an adversarially chosen goal, is actually the uniform distribution.
And this is actually a very simple result that follows from classical maximum entropy modeling results.
It's very simple, and it's described, and this paper linked at the bottom of the slide, called Unsupervised Metal Learning for Reinforcement Learning.
So what this means is that in the absence of any knowledge about what you're going to get, maximizing the entropy of your goals or the entropy of your states is kind of the best thing you can do.
Because if you don't know what kind of goal you'll get at test time, the only thing you can really assume is that it might be the worst case goal.
So if you want to make the worst case as good as possible, go for uniformly distributed goals, if you can.
And that's kind of the justification for doing this uniform coverage business.
And the provably efficient maximum entropy exploration paper also discusses this point a fair bit.