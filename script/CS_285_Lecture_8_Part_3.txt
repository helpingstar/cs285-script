Okay, in the next section, I want to briefly discuss kind of another view that we can take of these Q-learning algorithms that maybe provides a little bit more of a unified perspective, because we covered lots of different variants.
We covered fitted Q iteration, online Q-learning, deep Q-learning with replay buffers.
We can unify all of these in one kind of conceptual framework, and I just want to highlight this in the next portion.
And many of you might probably already suspect what that more general framework is, but let's just make it really explicit.
So here is the general Q-learning with replay buffers and target networks that I discussed before.
So here we have this outer-outer loop where we save the target network parameters, ϕ' goes to ϕ.
And we could use this polyhack averaging trick or just a standard flip every n steps.
In step two, we collect some number of data points using some policy and add it to the buffer.
In step three, we sample a batch.
In step four, we take a gradient step.
And then we alternate steps three and four some number of times.
Now, this is written out here as a kind of an inner loop style algorithm, but it's really not.
It's really a bunch of parallel processes.
So the fitted Q iteration algorithm that we had before looks a little bit different.
Here, I've written it in a way so that it better resembles the version above, but you could also write it with supervised regression.
So that one, we have the data collection in the outer-outer loop, and then the networks are updated in the middle loop.
But they're really kind of the same thing.
And if you view them as having the same basic processes running in parallel at different rates, then all of these methods can be unified into one kind of parallel framework.
So in the fitted Q iteration algorithm, the inner-inner loop is just SGD.
The DQN method we had before is a special case.
Where N equals 1 and K equals 1.
But all of them are really just special cases of this more general view.
So we have our data set of transitions, our replay buffer.
This is the basic object at the center of all of this.
We periodically interact with the world.
And when we interact with the world, what we typically do is we take our latest vector ϕ.
We construct some policy out of ϕ.
For instance, using ϵ-greedy or Boltzmann exploration.
We send it out into the world, and it brings back one or more transitions.
And you can think of this not as a discrete decision that we make periodically, but as a continuous process that is always running.
So let's call it process one, the data collection process.
The data collection process takes steps in the environment, and each step it takes, it sends back to our replay buffer.
Now, our replay buffer is a finite size.
We can't just keep adding stuff to it forever.
So we also have another process, an eviction process.
Which periodically throws things out of the buffer when it gets too big.
There are a lot of decisions about how and when you throw things out.
But a very simple and reasonable choice is to simply structure the replay buffer as a ring buffer.
Where the oldest thing in the buffer gets thrown out when a new thing gets added in.
So if your buffer has one million transitions, then as soon as the one million and one transition gets added in, then the oldest transition gets thrown in the garbage.
And that ensures that your buffer doesn't grow.
And that it's not unbounded.
Then you have your target parameters, ϕ'.
And your target parameters are used to compute those target values.
And you have your current parameters, ϕ.
Your current parameters are the ones that you're going to give to process one, in order to construct that ϵ-greedy policy to collect more data.
And you have process two, which updates the target parameters.
So process two will periodically copy ϕ into ϕ'.
Or perform that polyhack averaging.
And process two is typically a very slow process.
So it typically runs very infrequently.
And then you have process three, which is kind of the main learning process.
And what process three does is it loads a batch of transitions from the replay buffer.
So that's step three in the pseudo-code above.
It loads in the target parameters, ϕ'.
It uses the target parameters to calculate target values for every transition in the batch that was sampled.
It uses that to update the current parameters, ϕ.
That's step four above.
And then saves them back out into the current parameters right there.
So this is a kind of graphical depiction of a general Q-learning recipe that encompasses all of the algorithms we've discussed.
All of them can essentially be instantiated as special cases of this general three-process, or four-process if you also include eviction, parallel architecture.
And in fact, you could actually implement this as a parallel architecture.
You could actually have these as separate processes in different threads.
Or you can implement it as a serial sequential process, but this mental model that there are really three different things that can all happen at different rates is still useful for thinking about it.
So even though it seems like there are many different Q-learning algorithms, essentially they all just involve different decisions for the rates at which we run process one, process two, and process three.
So online Q-learning, the basic, basic Watkins online Q-learning that we had in the previous lecture, is a special case where you evict immediately, meaning the size of your buffer is one.
It's a ring buffer of size one.
And then process one, process two, and process three all run at the same speed, and they all take one step sequentially.
So process one takes one step, which means collect one transition.
Process two takes one step, which means that your target values are always computed using the latest parameters.
And then process three takes one step, which means that you make one gradient update.
The DQN algorithm that we mentioned before is also pretty similar.
Process one and process three run at the same speed, which is a slightly arbitrary choice when you think about it, because process one and process three are actually fairly decoupled, but they run at the same speed, so you always take one step of data collection and one step of gradient update.
And then process two is very slow, and the replay buffer is quite large, so you might store up to a million transitions.
Part of why this starts looking so weird is that when your replay buffer is large, process one and process three are pretty heavily decoupled, because once it's large enough, the probability that you'll sample the transition that you just collected becomes pretty low.
It does turn out that it's actually quite important to collect data pretty quickly, because the performance of your Q-learning algorithm can degrade rapidly if you don't collect data fast enough, but nonetheless, process one and process three have quite a bit of buffer space between them, literally.
And then the Fitted Q iteration algorithm that I used for illustrating these concepts can also be viewed as a special case of this.
In Fitted Q iteration, process three is actually in the inner loop of process two, which itself is in the inner loop of process one.
So in the Fitted Q iteration algorithm, you do your regression all the way to convergence, then you update your target network parameters, and you might alternate these a few times, and then in the outer outer loop, you pop all the way back out and collect more data.
But these are really not all that different.
They're just particular choices about the rates at which we run all these different processes.
And there's something, of course, a little bit deeper about this, because for each of these processes, they each create non-stationarity for every other process.
So if process two and process three are completely halted, then process three is just faced with a standard, convergent, supervised learning problem.
So by varying the rates of these different processes, by making them all run at different rates, we're essentially mitigating the effects of non-stationarity.
Because if the rate of process two, for example, is very different from process three, then process three, which is running a lot faster, to it, it will kind of look like everything is almost stationary.
So that's the kind of deeper reason why having these three different processes running at different rates can help Q-learning algorithms converge more effectively.