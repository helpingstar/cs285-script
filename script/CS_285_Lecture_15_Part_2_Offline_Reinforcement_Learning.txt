[p.18]

All right, in the next two portions of the lecture, we're going to talk about classic offline reinforcement learning methods that kind of predate the deep RL techniques.
And these are, you know, generally these are not the methods that you would start with if you wanted to use offline RL today.
You would start with methods that we would cover a bit later.
But I think it helps to discuss these to give everybody kind of the perspective of where a lot of these ideas come from and how people have thought about offline RL and batch RL in the past.
And by the way, in terms of terminology, the term batch reinforcement learning really kind of was popularized in around the early 2000s.
And somewhere in the last few years, the term offline RL became a little bit more prevalent because it's a bit more self-explanatory.
It better captures what's really going on.
And the term batch is kind of overloaded in current machine learning parlance.
But they mean exactly the same thing.
So if you see a paper that says batch RL, it means exactly the same thing as offline RL.
So the topic that I'll discuss in the next portion of the lecture is batch RL or offline RL by importance sampling.
Most of the methods that we'll talk about in this course for offline RL are value-based methods, dynamic programming-based methods.
But we will discuss importance sampling-based methods a bit.
This does occupy a significant portion of the classic literature on these kinds of techniques.

[p.19]

Now, a lot of this you guys will already be familiar with from our discussion of importance sample policy gradients.
And that forms the basic idea for importance sampling techniques for offline and batch RL.
So we have our RL objective.
We have our policy gradient, just like what we covered before, ∇log π times Q.
And the problem that you're all hopefully familiar with by this point is that estimating the policy gradient requires samples from π_θ.
So if you only have samples from π_β, what you would do is importance sampling.
And we learned all about importance sampling so far.
We multiply our policy gradient by an importance weight, which is the ratio of the probabilities of a trajectory under π_θ and π_β.
And as we saw before, when we write out those probabilities, all the terms that don't depend on the policy cancel out.

[p.20]

So this is just a recap of the policy gradient lecture.
So the ratio of the trajectory probabilities under the two policies consists of a product of initial states, transitions, and policies, but because the initial states and transition probabilities are exactly the same for both π_θ and π_β, it's only the ratio of the action probabilities that shows up.
And as we discussed before, this is a perfectly reasonable unbiased way to construct an estimator for the policy gradient using only samples from π_β, but it has a big problem because you are multiplying together these action probabilities and the number of probabilities that you're multiplying in general is O(T), which means that the weights are exponential in capital T, which means that the weights are likely to become degenerate as capital T becomes large.
By degenerate I mean that one weight will become very big and all the other weights will become vanishingly small, which means effectively as T increases you're estimating your policy gradient using one fairly arbitrary sample.
Mathematically what this means is that although the policy gradient with these importance weights is unbiased, meaning that if you were to generate many many different samples, or if you were to run the estimator many many times with independent samples, on average it would in fact give you the right answer, but the variance of the estimator is very large.
In fact it is exponentially large, which of course means that you need exponentially many samples to get an accurate estimate.
Can we fix this?
Well before I actually break down this equation, one comment I would make here is that we did see in our discussion of advanced policy gradients that a common way to use important sampling and practical modern policy gradient methods is to simply drop the p of a given s terms in those weights, for all the time steps prior to t.
And in the advanced policy gradients lecture we learned how this is reasonable to do if π_θ and the policy that generated the data, which in our case is π_β, are similar.
That doesn't really apply to offline RL, because in offline RL, the whole point is to get a much better policy.
So the short answer to can we fix this is no, we can't.
But we can sort of ruminate on this point a little bit more.
And to ruminate on it a little bit more, we can separate those weights into two parts.
So you can think of it as a product of all the action probabilities for the whole trajectory, the product from 0 to little t, and the product from little t to capital T.
And you can break that into two halves, and you can put those halves, one of them in front of ∇log π and one of them after.
Now, I didn't actually change anything.
This is just because multiplication commutes, so this is just exploiting the commutative property.
To just write the same exact importance weight in a different way.
But writing it in this way makes it a little bit more apparent that there are really two parts to the importance weight.
The part that multiplies all the actions prior to little t, which you can think of as essentially accounting for the fact that π_θ has a different probability of reaching the state s_t than π_β does.
And then all the stuff afterwards, which basically accounts for the fact that your value, the value Q had that you estimate by summing up the rewards in π_β, might be different from the value that you would get from π_θ.
So the second part of the weight accounts for the difference in reward to go.
So the first part accounts for the difference in probability of landing in s_t, because we have states sampled from d^{π_β} and we want states from d^{π_θ}.
And the second part accounts for having the incorrect ^{Q}, because ^{Q} here in the classic Monte Carlo policy gradient is formed by just summing up the rewards that you saw from π_β.
And instead, what you want is the rewards from π_θ.
So you could disregard the first term.
That's what classic on, that's what classic on policy techniques with multiple gradient steps do.
So that's what, for example, PPO does.
These are methods that do collect active samples, but then they take many gradient steps with an important sampled estimator and then collect some more samples.
And the justification for dropping that is basically if the policy that collected the data is close enough to your latest policy, then it's okay to disregard this because you have a bound as we discussed in the advanced policy gradient selection.
So that's why that's what that could be a reasonable approximation, but only if you are willing to not have π_θ deviate too much from π_β.

[p.21]

So we could talk about just the other term.
Um, so the other term naively, you would estimate ^{Q} by just summing up the rewards from π_β.
But what you want is you want to sum up the rewards from π_θ.
Uh, so you could think about breaking up this, uh, these importance weights even further.
Uh, you could say that the, this, this sum ^{Q} is really a sum from t' equals t to capital T of the reward that you actually saw at t', uh, multiplied by that, that whole importance weight.
So I didn't change anything here.
That's just the distributive property.
Um, but you know that actions in the future don't affect rewards in the past.
So one of the things you could do is, for any time step t', you could sum up only the actual probabilities from t to t', not from t to capital T.
Right.
So essentially I have the reward at the current time step.
That thing doesn't affect, that thing is not affected by the actual two time steps from now.
So I can exclude that from the importance weights.
Um, the importance weights are still going to be multiplying O of capital T terms.
So in terms of big O, this didn't actually get any better, but it does mean that we have lower variance for time steps closer to the current one.
So it's still exponential, but it's a little bit better.
Um, in fact, it actually turns out that to avoid exponentially exploding importance weights, we must use value function estimation.
There's a, there's actually no way to avoid the exponential problem altogether without using value function estimation.
But that hasn't stopped various techniques in the literature from trying to still make this not as bad.
None of them avoid the exponential problem altogether, but there are many ways to still reduce the variance to make it more manageable.
So one of them is the one on the slide, which is to only, um, multiply together the action probabilities from t to t' rather than from t to capital T for each for the reward of time step t'.
Um, but there are better ways to do it.
So, later on, we'll talk about how this would work if you, uh, if you knew Q^{π_θ} or if you had to learn Q^{π_θ}.
But first, let's conclude our discussion of importance sampling with a few other ideas that that have been explored in literature.

[p.21]

So one idea that is worth discussing because, you know, it has served to inspire quite a few more more recent techniques is the idea of the doubly robust estimator.
You can think of the doubly robust estimator as a little bit like a baseline, but for importance sampling.
So, this is just the important sample value estimate from the previous slide, and it's still exponential in the time horizon.
So notice that it's multiplying together the action probabilities from little t to little t' and little t' goes from, you know, goes all the way up to capital T.
So at the very last time step, you're still multiplying together O of capital T probabilities.
Um, I will for simplicity, I'll just drop the indices and I will turn my 't's into t just to keep it simple.
So before notice that I was writing V(s_t).
Now I'll just write it as V(s_0).
I'll just write it out for the initials time step.
Most of this is just to declutter my notation, but you can basically substitute and replace the zero with T and you would get all the stuff from before.
And what I'm going to try to do is I'm going to try to reduce the variance of these importance weights further.
So I'm going to introduce a little bit of notation.
I'm going to introduce ρ_{t'} and ρ_{t'} will denote p_θ(a_{t'}|s_{t'}) divided by π_β(a_{t'}|s_{t'}).
So I'll just condense that ratio into ρ_{t'}.
So now we can see that this important sampled value estimator is a sum over all the time steps of a product of all the rows up until that time step times γ^t r_t.
And it's that product of rows that we're concerned about.
So if we were to actually write this out, just like actually expand the sum, you would get the first term, which is ρ_0 ⋅ r_0.
Then you get the second term, which is ρ_0 ⋅ γ ⋅ ρ_1 ⋅ r_1.
And then you would get ρ_0 ⋅ γ ⋅ ρ_1 ⋅ γ ⋅ ρ_2 ⋅ γ, et cetera, r_2.
And I wrote it in kind of a slightly counterintuitive way intentionally where I actually interleave the 'γ's with the rows.
So I could also just collect all the 'γ's and just write γ^t.
But I intentionally wrote it this way so that you get this alternating pattern of ρ γ ρ γ ρ γ.
And this is going to be important later.
So what we can then do is we can put some parentheses around.
You'll notice that every term in the sum starts with ρ_0.
So you can just take all the ρ_0 out and collect all the other terms in parentheses.
So now you have ρ_0 times in parentheses, a big sum, which consists of all the other stuff.
r_0 plus γ plus all the future stuff.
And then you can repeat the process.
You can collect all the terms that have ρ_0 and ρ_1.
And that's the second set of parentheses.
So that's that's why you have γ ρ_1.
And then in parentheses are one plus all the other stuff.
And you can just keep going like this and you get all these nested summations.
Right.
We're not actually changing anything.
We're just basically using distributive and commutative properties of multiplication and addition to group these terms together.
So just a little bit of arithmetic.
A little bit of algebra.
And let's call this bar{V}^T.
bar{V}^T is an important sampling estimator of V^{π_θ}(s_0).
And now you can notice that there is a recursion here.
So bar{V}^{T+1-t} is equal to ρ_t(r_t + γ ⋅ bar{V}^{T-t}).
OK.
So essentially bar{V}^{T-t}.
That's the stuff in parentheses after the γ.
OK.
So if this is not completely obvious, you may want to pause the video here.
You could consider even getting out a little sheet of paper and just working this out to convince yourself that this is true.
This is a little subtle.
Right.
We're introducing a little bit of notation to induce a recursion that allows us to describe this important sampling estimator in a recursive way.
OK.
So our goal is to ultimately get bar{V}^T.
And this recursion describes a way to essentially bootstrap our way to bar{V}^T.
So now let's talk about doubly robust estimation.
Doubly robust estimation is a little bit easier to derive first in the case of a bandit problem.
So in the bandit case, there is only one time step.
And all you're trying to do is you're trying to estimate the reward.
Now, you can still do this with importance sampling.
Doing a bandit with importance sampling is a little weird, but it works.
And it gives us the intuition for the multi-step case.
So a regular importance sampled bandit would just be ρ(s,a) times r_{s,a}.
Right.
So we have rewards from some other distribution.
And we're going to multiply them by an importance weight.
And that will give us the value of our bandit.
And this is a contextual bandit.
So this is a bandit that has a state.
But now let's say that we have some guess as to the value function.
This guess doesn't have to be very accurate.
Right.
So we have some guess ^{V}(s) and some guess ^{Q}(s,a).
And how do we get this guess?
Well, maybe we just train a neural network to regress onto the values.
One thing we need here is we need ^{V}(s) to actually be the expected value of ^{Q}(s,a) with respect to the actions.
But that's pretty easy to get.
You know, you could just estimate ^{Q}(s,a).
And then just estimate ^{V}(s) with samples from your policy.
Then the doubly robust estimator basically takes this importance weighted rewards, subtracts your estimated function approximation, and adds back in its expected value.
So this is a lot like the control variants or baselines that we learned about before.
And the doubly robust estimator is going to be unbiased in expectation, just like the baseline, regardless of the choice of ^{Q}, so long as ^{V} is in fact the expected value of ^{Q}.
But of course the closer ^{Q} is to the true Q values, the lower the variance of this will be.
Because, you know, in the best case, if ^{Q} perfectly cancels off r_{s,a} here, then that second term, the high variance important sample term, goes to zero.
And the first term, which has very low variance, dominates.
So just like the baseline allowed us to reduce variance, this function approximator allows us to reduce the variance of this important sample estimator.
Now this is the bandit case.
The real trick with the doubly robust estimator is to extend it to the multi-step case.
And what we're going to do is we're going to take this thing in the blue box, and we're going to try to apply the same idea to these 'bar{V}'s.
So let's do that.
So we're going to define, in the same way that we define bar{V}^{T+1-t}, we're going to define a doubly robust version, bar{V}^{T+1-t}.
And the way we're going to do that is we will directly substitute in the bandit case into this.
So in the bandit case, we're doing an important sampled estimate of r_{s,a}.
Now we're doing an important sampled estimate of r_t + γ bar{V}^{T-t}.
So essentially, this equation that I have, the bar{V}_{DR}, is exactly the bandit case in the blue box, but with r replaced by r + γ ⋅ bar{V}^{T-t}.
So it's essentially a recursive version of the bandit case for the multi-step problem.
So in order to do this, you need to construct an estimate ^{Q}(s_t,a_t), and ^{Q}(s_t,a_t) could be some neural net, your favorite function approximator, and you need to get its expected value with respect to the actions, distributed according to your policy π_θ, and that gives you ^{V}.
And then just like the recursion, bar{V}^{T+1-t}, can be used to obtain the important sampled estimate, the doubly robust recursion can be used to obtain a doubly robust version of the important sampled estimate.
Okay.
So that's the idea behind the doubly robust off-policy evaluation.
Now, this is an off-policy evaluation method.
This is an OPE method.
It is not a reinforcement learning method.
So this will give you estimates of values, and you could use those values just to evaluate which policy is better, or you can plug them into an important sampled policy gradient estimator.
Okay.

[p.23]

There is one more topic that I want to very briefly cover.
I'm not going to go into the technical details for this, but I just want to describe this so that all of you are aware it exists, which is marginalized importance sampling.
So, so far when we talk about importance sampling, we always talked about importance sampling for the case where you're computing importance weights as ratios of action probabilities.
But it is actually possible to do importance sampling with state probabilities.
So the main idea in what is called marginalized importance sampling is that instead of using a product of action probabilities like we did before, we're going to estimate importance weights that are ratios of state probabilities or state action probabilities.
Now, the difference between states and state actions is not actually that different, because once you have a ratio of state probabilities, it's very easy to turn them into ratios of state action probabilities, because you know (a|s).
But if you can recover these state action importance weights, then it's very easy to estimate the value of some policy just by summing over all of your samples and averaging together the weighted rewards.
So doing off-policy evaluation is trivial if you can recover these 'w(s,a)'s.
And of course, if you can do off-policy evaluation, then you could also plug those value estimates into policy gradient as well, if you prefer.
But typically, marginalized importance sampling in the literature has been used just for off-policy evaluation.
I haven't seen it used very much for policy learning, although I think that, you know, should be possible.
So the biggest challenge with this is, of course, how to determine w(s,a).
And typically, since we don't know the state marginals of either our policy or the behavior policy, what we would typically do is we would write down some kind of consistency condition on w and then solve that consistency condition.
You can think of this consistency condition as kind of the equivalent of the Bellman equation, but for importance weights.
So the Bellman equation describes the consistency condition on value functions, that like, for example, Q(s,a) should be equal to r_{s,a} + γ Q(s',a').
That's a consistency condition.
And if you can make that equality hold true everywhere, then you will recover a valid Q function.
In the same way, you can write down a consistency condition for wW, and if you can make that condition hold true everywhere, then you will have recovered the true state or state action importance weights.
So here is one such consistency condition.
This is from a paper by Zhang et al. called GenDICE.
I won't go through this in detail because the derivation for this is kind of involved, and I've already spent quite a bit of time on important sampling.
But I want to just give you a taste for what the general gist of this is.
So if you look at this consistency condition, you can see that on the left-hand side, you have the probability of seeing a state and an action (s',a') under the behavior policy π_β times the weight.
Now, what's going on here?
Well, if you multiply d^{π_β} by the weight, you get d^{π_θ}, right?
Because the weight is d^{π_θ} over d^{π_θ}.
So what this is really describing is a condition that state action marginals need to obey.
When it comes time to actually optimize this in practice, of course, we're going to subtract the right-hand side from the left-hand side, and as long as we get a multiplier of d^{π_β} in front of everything, then we can approximate it with samples.
So in reality, we never actually estimate these d^{π_β} terms directly.
We always use samples from our data set as samples from d^{π_β}.
So the left-hand side of this is just the probability of seeing (s',a') under the policy π_θ.
And the probability of seeing (s',a') is basically equal to the probability that you start in (s',a'), and that's what the first term captures.
So it's the probability you start in s' times the probability that you take the action a' plus the probability that you transition into (s',a') from another state.
The probability you transition into (s',a') from another state, in this case s,a, is given by the probability that you are in that state, which is d^{π_θ}(s,a), and that's exactly what the product of those last two terms gives you, times the probability that you make that transition, which is what you get by multiplying it by p(s'|s,a), times π_θ(a'|s').
So this is the probability of starting in (s',a'), and this is the probability of transitioning into it from another state, multiplied by the probability that you are actually in that state, which is what the last two terms account for.
So solving for w(s,a) typically involves some kind of fixed point problem.
So it involves subtracting the right-hand side from the left-hand side and minimizing the square difference.
And the trick in deriving these algorithms is to basically turn that difference into an expected value under d^{π_β}, because once you can express it as an expected value under d^{π_β}, then you can use samples from d^{π_β} to estimate it.
And that means that you never have to explicitly approximate.
You never need to have a neural net that approximates d^{π_β} or d^{π_θ}.
You only need a neural net that approximates w.
So this is the basic idea of marginalized importance sampling.
Write down a relationship between the 'w's at future states and current states.
Turn that relationship into an error, and then estimate that error using only the samples in your data set.
And then typically you would represent the 'w's with some kind of neural net.

[p.24]

Okay, so that was kind of a quick whirlwind tour of importance sampling for off-policy evaluation and batch RL.
If you want to learn more about this, classic work on importance sample policy gradients and return estimation by Doina Precup, as well as by Peshkin and Shelton, doubly robust estimators.
Very interesting if you want to learn about OPE with importance weights.
So certainly for bandits and for small MDPs, doubly robust estimators are typically the method of choice if you're doing things like ad placement stuff like that but there are better techniques for actual learning policies these days.
Some analysis and theory so this paper by philip thomas high confidence of policy evaluation provides a lot of analysis of these types of estimators.
And if you want to learn about marginalized importance sampling, consider checking out these two papers, as well as the Zhang et al. paper that I referenced on the previous slide.