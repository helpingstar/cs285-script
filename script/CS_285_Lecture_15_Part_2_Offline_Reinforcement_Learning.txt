All right, in the next two portions of the lecture, we're going to talk about classic offline reinforcement learning methods that kind of predate the deep RL techniques.
And these are, you know, generally these are not the methods that you would start with if you wanted to use offline RL today.
You would start with methods that we would cover a bit later.
But I think it helps to discuss these to give everybody kind of the perspective of where a lot of these ideas come from and how people have thought about offline RL and batch RL in the past.
And by the way, in terms of terminology, the term batch reinforcement learning really kind of was popularized in around the early 2000s.
And somewhere in the last few years, the term offline RL became a little bit more prevalent because it's a bit more self-explanatory.
It better captures what's really going on.
And the term batch is kind of overloaded in current machine learning parlance.
But they mean exactly the same thing.
So if you see a paper that says batch RL, it means exactly the same thing as offline RL.
So the topic that I'll discuss in the next portion of the lecture is batch RL or offline RL by importance sampling.
Most of the methods that we'll talk about in this course for offline RL are value-based methods, dynamic programming-based methods.
But we will discuss importance sampling-based methods a bit.
This does occupy a significant portion of the classic literature on these kinds of techniques.
Now, a lot of this you guys will already be familiar with from our discussion of importance sample policy gradients.
And that forms the basic idea for importance sampling techniques for offline and batch RL.
So we have our RL objective.
We have our policy gradient, just like what we covered before, ∇log π times q.
And the problem that you're all hopefully familiar with by this point is that estimating the policy gradient requires samples from π θ.
So if you only have samples from π beta, what you would do is importance sampling.
And we learned all about importance sampling so far.
We multiply our policy gradient by an importance weight, which is the ratio of the probabilities of a trajectory under π θ and π beta.
And as we saw before, when we write out those probabilities, all the terms that don't depend on the policy gradient are the same.
cancel out.
So this is just a recap of the policy gradient lecture.
So the ratio of the trajectory probabilities under the two policies consists of a product of initial states, transitions, and policies, but because the initial states and transition probabilities are exactly the same for both π θ and π beta, it's only the ratio of the action probabilities that shows up.
And as we discussed before, this is a perfectly reasonable unbiased way to construct an estimator for the policy gradient using only samples from π beta, but it has a big problem because you are multiplying together these action probabilities and the number of probabilities that you're multiplying in general is O , which means that the weights are exponential in capital T, which means that the weights are likely to become degenerate as capital T becomes large.
By degenerate I mean that one weight will become very big and all the other weights will become vanishingly small, which means effectively as T increases you're estimating your policy gradient.
So this is a very simple way to do this.
So let's look at the policy gradient.
So let's look at the policy gradient.
So let's look at the policy gradient.
So let's look at the policy gradient.
So let's look at the policy gradient.
So let's look at the policy gradient using one fairly arbitrary sample.
Mathematically what this means is that although the policy gradient with these importance weights is unbiased, meaning that if you were to generate many many different samples, or if you were to run the estimator many many times with independent samples, on average it would in fact give you the right answer, but the variance of the estimator is very large.
In fact it is exponentially large, which of course means that you need exponentially many samples.
And that the variance of the estimator that is defined by the policy gradient is very large because it needs exponentially many more samples to get an accurate estimate.
Can we fix this?
Well before I actually break down this equation, one comment I would make here is that we did see in our discussion of advanced policy gradients that a common way to use important sampling and practical modern policy gradient methods is to simply drop the P of A given S terms in those weights, for all the time steps prior to T.
learned how this is reasonable to do if π θ and the policy that generated the data, which in our case is π beta, are similar.
That doesn't really apply to offline RL, because in offline RL, the whole point is to get a much better policy.
So the short answer to can we fix this is no, we can't.
But we can sort of ruminate on this point a little bit more.
And to ruminate on it a little bit more, we can separate those weights into two parts.
So you can think of it as a product of all the action probabilities for the whole trajectory, the product from 0 to little t, and the product from little t to capital T.
And you can break that into two halves, and you can put those halves, one of them in front of ∇log π and one of them after.
Now, I didn't actually change anything.
This is just because multiplication commutes, so this is just exploiting the commutative property.
To just write the same exact importance weight in a different way.
But writing it in this way makes it a little bit more apparent that there are really two parts to the importance weight.
The part that multiplies all the actions prior to little t, which you can think of as essentially accounting for the fact that π θ has a different probability of reaching the state s_t than π beta does.
And then all the stuff afterwards, which basically accounts for the fact that your value, the value q had that you estimate by summing up the rewards in π beta, might be different from the value that you would get from π θ.
So the second part of the weight accounts for the difference in reward to go.
So the first part accounts for the difference in probability of landing in s_t, because we have states sampled from d π beta and we want states from d π θ.
And the second part accounts for having the incorrect q hat, because q hat here in the classic Monte Carlo policy gradient is formed by just summing up the rewards that you saw from π beta.
And instead, what you want is the rewards from π θ.
So you could disregard the first term.
That's what classic on, that's what classic on policy techniques with multiple gradient steps do.
So that's what, for example, PPO does.
These are methods that do collect active samples, but then they take many gradient steps with an important sampled estimator and then collect some more samples.
And the justification for dropping that is basically if the policy that collected the data is close enough to your latest policy, then it's okay to disregard this because you have a bound as we discussed in the advanced policy.
Gradient selection.
So that's why that's what that could be a reasonable approximation, but only if you are willing to not have π θ deviate too much from π beta.
So we could talk about just the other term.
Um, so the other term naively, you would estimate q hat by just summing up the rewards from π beta.
But what you want is you want to sum up the rewards from π θ.
Uh, so you could think about breaking up this, uh, these importance weights even further.
Uh, you could say that the, this, this sum q hat is really a sum from t' equals t to capital T of the reward that you actually saw at t', uh, multiplied by that, that whole importance weight.
So I didn't change anything here.
That's just the distributive property.
Um, but you know that actions in the future don't affect rewards in the past.
So one of the things you could do is, for any time step t', you could sum up only the, um, the actual probabilities from t to t', not from t to capital T.
Right.
So essentially I have the reward at the current time step.
That thing doesn't affect, that thing is not affected by the actual two time steps from now.
So I can exclude that from the importance weights.
Um, the importance weights are still going to be multiplying O of capital T terms.
So in terms of big O, this didn't actually get, it didn't get any better, but it does mean that we have lower variance for time steps closer to the current one.
So it's still exponential, but it's a little bit better.
Um, in fact, it actually turns out that to avoid exponentially exploding importance weights, we must use value function estimation.
There's a, there's actually no way to avoid the exponential problem altogether without using value function estimation.
But that hasn't stopped various techniques in the literature from trying to still make the, uh, still make this not as bad.
None of them avoid the exponential problem altogether, but there are many ways to still reduce the variance to make it more manageable.
So one of them is the one on the slide, which is to only, um, multiply together the action probabilities from T to T prime rather than from T to capital T for each for the reward of time step T prime.
Um, but there are better ways to do it.
So, later on, we'll talk about how this would work if you, uh, if you knew Q π θ or if you had to learn Q π θ.
But first, let's conclude our discussion of importance sampling with a few other ideas that that have been explored in literature.
So one idea that is worth discussing because, you know, it has served to inspire quite a few more more recent techniques is the idea of the doubly robust estimator.
You can think of the doubly robust estimator as a little bit like a baseline, but for importance sampling.
So, this is just the important sample value estimate from the previous slide, and it's still exponential in the time horizon.
So notice that it's multiplying together the action probabilities from little t to little t' and little t' goes from, you know, goes all the way up to capital T.
So at the very last time step, you're still multiplying together O of capital T probabilities.
Um, I will for simplicity, I'll just drop the indices and I will turn my T primes into T just to keep it simple.
So before notice that I was writing V of s_t.
Now I'll just write it as V of S zero.
I'll just write it out for the initials time step.
Most of this is just to declutter my notation, but you can basically substitute and replace the zero with T and you would get all the stuff from before.
And what I'm going to try to do is I'm going to try to do is I'm going to try to do a little bit of a test.
I'm going to try to do a little bit of a test.
I'm going to try to do a little bit of a test.
I'm going to try to do a little bit of a test.
I'm going to try to reduce the variance of these importance weights further.
So I'm going to introduce a little bit of notation.
I'm going to introduce rho T prime and rho T prime will denote P θ A T prime given S T prime divided by π beta A T prime given S T prime.
So I'll just condense that ratio into rho T prime.
So now we can see that this important sampled value estimator is a sum over all the time steps of a product of all the rows up until that time step times gamma T R T.
And it's that product of rows that we're concerned about.
So if we were to actually write this out, just like actually expand the sum, you would get the first term, which is rho zero times R zero.
Then you get the second term, which is rho zero times gamma times rho one times R one.
And then you would get rho zero times gamma rho one gamma rho two gamma, et cetera, R two.
And I wrote it in kind of a slightly.
counterintuitive way intentionally where I actually interleave the gammas with the rows.
So I could also just collect all the gammas and just write gamma to the T.
But I intentionally wrote it this way so that you get this alternating pattern of rho gamma rho gamma rho γ.
And this is going to be important later.
So what we can then do is we can put some parentheses around.
You'll notice that every term in the sum starts with rho zero.
So you can just take all the rho zero out and collect all the other terms in parentheses.
So now you have rho.
Zero times in parentheses, a big sum, which consists of all the other stuff.
R zero plus gamma plus all the future stuff.
And then you can repeat the process.
You can collect all the terms that have rho zero and rho one.
And that's the second set of parentheses.
So that's that's why you have gamma rho one.
And then in parentheses are one plus all the other stuff.
And you can just keep going like this and you get all these nested summations.
Right.
We're not actually changing anything.
We're just basically using distributive and commutative properties of multiplication.
And addition to group these terms together.
So just a little bit of arithmetic.
A little bit of algebra.
And let's call this bar{V} superscript T.
bar{V} superscript T is an important sampling estimator of V π θ S zero.
And now you can notice that there is a recursion here.
So bar{V} T plus one minus T is equal to rho T times R T plus gamma bar{V} capital T minus little t.
OK.
So essentially bar{V} to the capital T minus little t.
That's the stuff in parentheses after the γ.
OK.
So if this is not completely obvious, you may want to pause the video here.
You could consider even getting out a little sheet of paper and just working this out to convince yourself that this is true.
This is a little subtle.
Right.
We're introducing a little bit of notation to induce a recursion that allows us to describe this important sampling estimator in a recursive way.
OK.
So our goal is to ultimately get bar{V} capital T.
And this recursion describes a way to essentially bootstrap our way to bar{V} capital T.
So now let's talk about doubly robust estimation.
Doubly robust estimation is a little bit easier to derive first in the case of a bandit problem.
So in the bandit case, there is only one time step.
And all you're trying to do is you're trying to estimate the reward.
Now, you can still do this with importance sampling.
Doing a bandit with importance sampling is a little bit easier.
But you can still do this with importance sampling.
So you can do this with a bandit with importance sampling.
It's a little weird, but it works.
And it gives us the intuition for the multi-step case.
So a regular importance sampled bandit would just be rho s,a times r(s,a).
Right.
So we have rewards from some other distribution.
And we're going to multiply them by an importance weight.
And that will give us the value of our bandit.
And this is a contextual bandit.
So this is a bandit that has a state.
But now let's say that we have some guess.
So we have some guess V hat S and some guess Q hat SA.
And how do we get this guess?
Well, maybe we just train a neural network to regress onto the values.
One thing we need here is we need V hat S to actually be the expected value of Q hat s,a with respect to the actions.
But that's pretty easy to get.
You know, you could just estimate Q hat SA.
And then just estimate V hat S with samples from your policy.
Then the doubly robust estimator basically takes this importance weighted rewards, subtracts your estimated function approximation, and adds back in its expected value.
So this is a lot like the control variants or baselines that we learned about before.
And the doubly robust estimator is going to be used to calculate the function approximation.
So we can get the unbiased in expectation, just like the baseline, regardless of the choice of Q hat, so long as V hat is in fact the expected value of Q hat.
But of course the closer Q hat is to the true Q values, the lower the variance of this will be.
Because, you know, in the best case, if Q hat perfectly cancels off r(s,a) here, then that second term, the high variance important sample term, goes to zero.
And the first term, which has very low variance, dominates.
So again, we can't just do this.
So we can't just do this.
So we can't just do this.
So just like the baseline allowed us to reduce variance, this function approximator allows us to reduce the variance of this important sample estimator.
Now this is the bandit case.
The real trick with the doubly robust estimator is to extend it to the multi-step case.
And what we're going to do is we're going to take this thing in the blue box, and we're going to try to apply the same idea to these bar{V}s.
So let's do that.
So we're going to define, in the same way that we define bar{V} capital T plus one, minus little t, we're going to define a doubly robust version, bar{V} capital T plus one minus little t.
And the way we're going to do that is we will directly substitute in the bandit case into this.
So in the bandit case, we're doing an important sampled estimate of r(s,a).
Now we're doing an important sampled estimate of RT plus gamma bar{V} capital T minus little t.
So essentially, this equation that I have, the bar{V} dr, is exactly the bandit case in the blue box, but with R replaced by R plus gamma, bar{V} capital T minus little t.
So it's essentially a recursive version of the bandit case for the multi-step problem.
So in order to do this, you need to construct an estimate Q hat STAT, and Q hat STAT could be some neural net, your favorite function approximator, and you need to get its expected value with respect to the actions, distributed according to your policy π θ, and that gives you V hat.
And then just like the recursion, if you can get the value of bar{V} capital T plus one minus little t, can be used to obtain the important sampled estimate, the doubly robust recursion can be used to obtain a doubly robust version of the important sampled estimate.
Okay.
So that's the idea behind the doubly robust off-policy evaluation.
Now, this is an off-policy evaluation method.
This is an OPE method.
It is not a reinforcement learning method.
So this will give you estimates of values, and you could use those values just to evaluate which policy is better, or you can plug them into an important sampled policy gradient estimator.
Okay.
There is one more topic that I want to very briefly cover.
I'm not going to go into the technical details for this, but I just want to describe this so that all of you are aware it exists, which is marginalized importance sampling.
So, so far when we talk about importance sampling, we always talked about importance sampling for the case where you're computing importance weights as ratios of action probabilities.
But it is actually possible to do importance sampling with state probabilities.
So the main idea in what is called marginalized importance sampling is that instead of using a product of action probabilities like we did before, we're going to estimate importance weights that are ratios of state probabilities or state action probabilities.
Now, the difference between states and state actions is not actually that different, because once you have a ratio of state probabilities, it's very easy to turn them into ratios of state action probabilities, because you know A given S.
But if you can recover these state action importance weights, then it's very easy to estimate the value of some policy just by summing over all of your samples and averaging together the weighted rewards.
So doing off-policy evaluation is true.
Okay.
So you can do off-policy evaluation, but it's also trivial if you can recover these WSAs.
And of course, if you can do off-policy evaluation, then you could also plug those value estimates into policy gradient as well, if you prefer.
But typically, marginalized importance sampling in the literature has been used just for off-policy evaluation.
I haven't seen it used very much for policy learning, although I think that, you know, should be possible.
So the biggest challenge with this is, of course, how to determine WSA.
And typically, since we don't know the state marginals of either our policy or the behavior policy, what we would typically do is we would write down some kind of consistency condition on W and then solve that consistency condition.
You can think of this consistency condition as kind of the equivalent of the Bellman equation, but for importance weights.
So the Bellman equation describes the consistency condition on value functions, that like, for example, QSA should be equal to r(s,a) plus gamma QS prime A prime.
That's a consistency condition.
And if you can make that equality hold true everywhere, then you will recover a valid Q function.
In the same way, you can write down a consistency condition for W, and if you can make that condition hold true everywhere, then you will have recovered the true state or state action importance weights.
So here is one such consistency condition.
This is from a paper by Zhang et al.
called GENDIZE.
I won't go through this in detail because the derivation for this is kind of involved, and I've already spent quite a bit of time on it.
It's an important sampling.
But I want to just give you a taste for what the general gist of this is.
So if you look at this consistency condition, you can see that on the left-hand side, you have the probability of seeing a state and an action s' A prime under the behavior policy π beta times the weight.
Now, what's going on here?
Well, if you multiply d π beta by the weight, you get d π θ, right?
Because the weight is d π θ over d π θ.
So what this is really describing is a condition that state action marginals need to obey.
When it comes time to actually optimize this in practice, of course, we're going to subtract the right-hand side from the left-hand side, and as long as we get a multiplier of d π beta in front of everything, then we can approximate it with samples.
So in reality, we never actually estimate these d π beta terms directly.
We always use samples from our data set as samples from d π beta.
So the left-hand side of this is just the probability of seeing s' A prime under the policy π θ.
And the probability of seeing s' A prime is basically equal to the probability that you start in s' A prime, and that's what the first term captures.
So it's the probability you start in s' times the probability that you take the action a prime plus the probability that you transition into s' A prime from another state.
The probability you transition into s' A prime from another state, in this case S A, is given by the probability that you are in that state, which is d π θ S A, and that's exactly what the product of those last two terms gives you, times the probability that you make that transition, which is what you get by multiplying it by P of s', given S A, times π θ A prime, given s'.
So this is the probability of starting in s' A prime, and this is the probability of transitioning into it from another state, multiplied by the probability that you leave it in s' A prime, that you are actually in that state, which is what the last two terms account for.
So solving for WSA typically involves some kind of fixed point problem.
So it involves subtracting the right-hand side from the left-hand side and minimizing the square difference.
And the trick in deriving these algorithms is to basically turn that difference into an expected value under d π beta, because once you can express it as an expected value under d π beta, then you can use samples from d π beta to estimate it.
And that means that you never have to explicitly approximate.
You never need to have a neural net that approximates d π beta or d π θ.
You only need a neural net that approximates W.
So this is the basic idea of marginalized importance sampling.
Write down a relationship between the Ws at future states and current states.
Turn that relationship into an error, and then estimate that error using only the samples in your data set.
And then typically you would represent the Ws with some kind of neural net.
Okay, so that was kind of a quick whirlwind tour of importance sampling for off-policy evaluation and batch RL.
If you want to learn more about this, classic work on importance sample policy gradients and return estimation by Doina Precup, as well as by Peshkin and Shelton, doubly robust estimators.
Very interesting if you want to learn about OPE with importance weights.
So certainly for bandits and for small MDPs, doubly robust estimators are typically the method of evaluation.
And if you want to learn more about OPE with importance weights, you can look at the paper by Philip Thomas, High Confidence Off Policy Evaluation.
It provides a lot of analysis of these types of estimators.
And if you want to learn about marginalized importance sampling, consider checking out these two papers, as well as the Zhang et al.
paper that I referenced on the previous slide.