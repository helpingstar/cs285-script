[p.01]

Alright, welcome to lecture 13 of CS285.
Today, we're going to talk about exploration.
Today's lecture is going to be a little bit on the longer side, but to make up for it, the next lecture, which is going to be part 2 of exploration, will be quite a bit shorter.
So, if this lecture feels like it's going on for a while, we're going to give you a little bit of a break for Wednesday's lecture, where it won't be quite as long.
Alright, let's get started.

[p.02]

So, what's the problem that we're going to talk about today?
Well, the problem can be illustrated with an example like this.
If you're working on Homework 3, if you're finishing it up now, you might have tried a few different Atari games.
Some of these Atari games are actually pretty easy.
So, if you want to play Pong or Breakout, mostly your Homework 3 Q-learning implementation will probably work pretty well on those tasks.
But some other Atari games are actually quite a bit harder.
So, this game, for example, is almost impossible.
If you try to run it, this is called Montezuma's Revenge, if you try to run your Q-learning implementation on this game, you'll probably find that it doesn't get very far.
So, why is that?
Why is the game on the right so much harder than the game on the left?
Well, it's not because the game itself is necessarily harder.
For a person playing Montezuma's Revenge, you know, I've played it myself, I don't think it's a very good game, but it's not a particularly difficult one.
In fact, getting that trick shot in Breakout, where it bounces, bounces around up top, is probably harder actually than playing Montezuma's Revenge.
But it's very hard for an RL agent to play this game.

[p.03]

So, in Montezuma's Revenge, the goal is to traverse this pyramid that's made up of multiple different rooms, and each room has a different challenge.
So, in this first room, there's a skull that bounces around that kills you if you step on it, and you have to go fetch this key and then open one of the doors at the top.
Now, we understand some of these things.
We understand that the key is a good thing, that keys open doors.
We might not know what exactly the skull is supposed to do, but we kind of know that skulls are probably not good things, and touching the skull is probably not a good idea.
Now, in the game, you get a reward for getting the key.
You also get a reward for opening the door.
Getting killed by the skull actually doesn't do anything, so you lose a life, but you don't actually get a negative reward for that.
If you lose all your lives, then you start over.
That's also not obvious whether that's good or bad, because when you start over, you might get another opportunity to pick up the key, and maybe that's good, because then you get the reward for the key again.
So, the reward structure of the game doesn't really guide you each step of the way, and while we know ourselves that some of these things are good or bad, the agent really doesn't, and the agent might figure out that a good way to keep getting reward is to keep getting killed by the skull so they can pick up the key again instead of moving on to the next room.
The trouble is that finishing the game only weakly correlates with rewarding events.
It's not that you get little pieces of reward when you're on the right track and negative reward when you're on the wrong track.
So, we know what to do because we understand what all these little sprites and pictures mean, but the RL algorithm has to figure it out through trial and error.

[p.04]

To try to understand kind of how the algorithm feels when trying to play one of these games, let's think of a different example, an example that's a lot less intuitive for humans.
So, there's a card game called Mao.
It's also similar in principle to a game called Calvin Ball.
The idea is that the only rule you may be told is this one.
So, when you start playing the game, you just don't know the rules of the game, and one of the players, who's the chairman, can call you out for not following a rule, but they don't explain the rule to you.
They just tell you that you incur a penalty for failing to follow a rule.
And you can only discover the rules through trial and error.
And then this makes the game very frustrating and quite demanding.
So, even though the rules might be fairly simple, because you don't know those rules, and you have to discover them through trial and error, the game ends up being very, very challenging.
And the rules don't always make sense to you.
So, the whole point of this game is for other players to make up rules that are kind of weird and kind of counterintuitive.
So, temporally extended tasks like Montezuma's Revenge or the game Mao can become increasingly difficult based on how extended the task is and how little you know about the rules.
Essentially, even seemingly simple tasks where you don't know the rules and you have to discover them through trial and error as a result of poorly shaped rewards can prove to be exceptionally challenging.
And imagine taking this a step further.
Imagine that your goal in life was to win 50 games of Mao.
So, you're just going about your day, you can go to class, you can do your homework, but if you happen to win 50 games of Mao, you're going to get a million dollars.
Now, you're pretty unlikely to just sort of randomly go and do this.
So, this is essentially the exploration problem.
The exploration problem relates to this setting where you have temporally delayed rewards where the structure of the task doesn't really tell you what are the things you need to do to get larger rewards in the future.
Alright.

[p.05]

Here's another example that looks very different at first but actually describes kind of a similar type of problem.
So, this is a continuous control task.
So, here this robotic hand is supposed to pick up a ball and move it to this location.
Now, this is also a difficult exploration problem because in order to figure out how to get reward by putting the object in the right place, the hand needs to essentially wiggle the joints on the fingers randomly.
And again, just like a priori, we don't understand the rules of the game Mao, here the hand doesn't understand that moving and picking up objects is actually a thing.
All it knows is that it can wiggle its fingers around and the reward is so delayed that it gets very little intermediate signal for actually grasping objects.

[p.06]

Alright.
So, let's talk a little bit more about this exploration thing.
In RL, we often refer to the exploration versus exploitation problem as one where at each trial, the agent has to essentially choose whether they want to do a better job of exploring by trying something they don't know how to do yet or whether they just want to do the thing that gets them the largest reward.
So, the agent in Montezuma's Revenge that's just going after the key each time they die is essentially performing a kind of exploitation.
They know one thing that gives them reward, which is the key.
And they know one way to get that reward or just to die and get the key again.
And they're just capitalizing on that, getting the rewards they know how to get instead of trying to find better rewards elsewhere.
So, there are two potential definitions of the exploration problem in light of this.
The first is how can an agent discover high-reward strategies that require a temporarily extended sequence of complex behaviors that individually are not rewarding?
And the second is how can an agent decide whether to attempt new behaviors to discover ones with higher reward or continue to do the best thing it knows so far?
And these are really the same problem because if you want to discover temporarily extended sequences of behaviors that lead to high reward, you need to decide whether you should be exploring more or whether you've already found the most temporarily extended sequence and you should just keep doing that or maybe refine how well you do that.
So, they're actually the same problem.
Exploitation is doing what you know will yield the highest reward.
Exploration is doing things you haven't done before.
In the hopes of getting even higher reward.
And the trouble is you don't know which one of those you should be doing.
And of course, they're not totally disjoint.
So, for example, in some cases you might want to exploit a little bit so that you can explore further.
If you've figured out how to go to the second room in Montezuma's Revenge, a good way to explore is to exploit a bit to go into that second room and then explore from there.
So, it's not like you just have to flip a coin and decide between exploitation and exploration.
It's really kind of a dynamic and persistent decision you have to keep making.

[p.07]

So, here are a few examples which I borrowed from some of David Silver's lecture notes.
Imagine that you have to select which restaurant to go to.
Perhaps not something that you're doing in 2020, but, you know, in the previous year we lived in, back when going to restaurants was a thing.
Exploitation would mean that you go to your favorite restaurant.
Exploration means you try a new restaurant.
Now, this example makes it seem very binary, and I think that binary sense is a little misleading because in reality, it might be more complex than that.
Like the example of Montezuma's Revenge I mentioned before, where the best way to explore might actually be to exploit a little bit and then explore from the last thing you landed.
Online ad placement.
This is a classic exploration-exploitation trade-off problem.
Exploitations mean you show the most successful ad, the one that makes you the most money.
Exploration means you show a different, perhaps randomly chosen advertisement.
Oil drilling.
Exploitation.
Maybe you drill at the best known location.
Exploration.
Find a new location to drill at, which might not contain oil, or it might contain even more oil.

[p.08]

Now, exploration is very hard, both practically and also theoretically.
It's a theoretically hard and intractable problem.
So a question that we might ask when we go to devise exploration algorithms is, can we derive an optimal exploration strategy?
And that's actually what we're going to talk about in today's lecture.
But in order to do that, we have to understand, what does optimal even mean?
So one of the ways that we could define the optimality of our exploration strategy is in terms of regret against a Bayes optimal strategy.
And we'll make this more formal later, but intuitively you could imagine a perfect Bayesian agent that maintains the uncertainty about how the world works and therefore makes optimal exploration decisions, maybe optimal decisions to optimally resolve the unknowns about the world.
And now such an optimal Bayesian agent would be intractable.
It would require estimating a really complex posterior over your MDPs, but you could use this as a gold standard and for your practical exploration algorithm, measure its regret against this Bayes optimal hypothetical agent.
We can kind of place different problem settings on a spectrum from theoretically tractable to theoretically intractable.
Theoretically tractable means that we can quantify or understand whether your given exploration strategy is optimal, meaning that it's close to this Bayes optimal strategy, or suboptimal, meaning it has much worse regret than the Bayes optimal strategy.
Intractable means that we cannot make this estimate exactly in that setting.
So the most theoretically tractable problems are what are called multi-armed bandit problems.
You can think of multi-armed bandit problems as one-time step stateless RL problems.
So in RL you have a state and an action, and the action leads to the next state.
In a bandit, you only take one action and then the episode terminates, and there is no state.
So you just have to decide on an action.
And these are the most theoretically tractable problems because in multi-armed bandits we can actually understand which exploration strategies are theoretically optimal and which ones are not optimal in terms of their regret versus the Bayes optimal agent.
Then the next step up are contextual bandit problems.
Contextual bandit problems are just like multi-armed bandits, only they do have a state.
So they still only have one time step, you still only take one action, your action only affects your reward, it does not affect the next state, but you have some context which is kind of like your state.
So ad placement could be one such problem.
You observe something about the user, maybe you have a feature vector about the user, and then you have to select which ad to show to that user.
Next step up are small finite MDPs.
So these are MDPs that can be solved exactly, maybe using value iteration.
These are not nearly as theoretically tractable as bandits, but there are some things we could say about exploration in small finite MDPs.
And then of course the next step up, the setting we are really concerned with in Deep RL, are large infinite MDPs, perhaps with continuous state spaces, or very large state spaces like images.
And generally for these problems, there isn't much that we can say theoretically.
But what we can do is we can take inspiration from the theoretically principled algorithms that we can devise in the bandit setting, and then kind of adapt similar techniques in the large infinite MDPs and hope that they work well.

[p.09]

So what makes an exploration problem tractable?
Well, for multi-armed bandits and contextual bandits, one of the things we can do is we can formalize the exploration problem as another kind of MDP, or rather a partially observed MDP, a POMDP.
So while the multi-armed bandit is a single step problem, you can view the problem of exploring in the multi-armed bandit as a multi-step problem, because even though your actions don't affect your state, they do affect what you know.
So if you explicitly reason about the evolution of your beliefs, that now forms a temporal process, which is technically a partially observed MDP, and then you could solve it using POMDP methods.
And because these multi-armed bandits are fairly simple, even the POMDP can actually be solved tractably, at least in theory.
And then the next step are small finite MDPs.
Here you can frame exploration as Bayesian model identification, and then reason explicitly about things like value of information, kind of extending similar ideas to the ones we had in bandits.
For large or infinite MDPs, these optimal methods don't work, in the sense that we can't prove anything about them, but we can still take inspiration from the optimal methods in the simpler settings, and adapt them to these larger settings, and find that they actually work well, at least empirically, even though we can't say anything about them theoretically.
And of course, we use lots of hacks, as we always do in deep reinforcement learning.
And that's the theme that you're going to find in this lecture, that we'll have some very principled approaches in simpler, smaller problems like multi-armed bandits, we'll sort of adapt those approaches by analogy in larger MDPs, and then use some hacks to make them work well in practice.

[p.10]

Okay, so let's start with a little discussion of bandits.
What's a bandit anyway?
So the bandits that we're talking about when we talk about exploration are not these guys.
The bandit is actually kind of the Drosophila of exploration problems.
So in the same way that biologists study fruit flies as their kind of simple model organism, in reinforcement learning, we study the bandit as our simple model organism.
And the bandit that we're referring to is this thing.
So the term multi-armed bandit is kind of one of these quaint American colloquialisms that stems from the term one-armed bandit.
So the one-armed bandit is a slot machine.
It's a machine in a casino where you pull the lever, and with some random probability, this thing will produce some reward.
Maybe you'll lose your money, or maybe you'll get money.
The multi-armed bandit...
So in a one-armed bandit, you have only one action, which is to pull the arm.
And you don't know what the reward for pulling that arm is, and the reward in general will be stochastic, so it's really a reward distribution.
You can think of a multi-armed bandit as a bank of different slot machines, and the decision you have to make is which slot machine to play.
So you have n of these machines, and different machines will give different payoffs and different reward distributions.
Now just because you pulled one of the arms doesn't mean that's a bad arm.
Maybe you pulled that arm and you got very little money, but that's just because you got unlucky.
Maybe in general that machine gives very high payoff, and if you pull the arm repeatedly, on average you might make a lot of money.
So you don't know the reward for each arm, and you don't know the reward distribution for each arm.
So you could assume that the reward of each arm is distributed according to some probability distribution, and then you could imagine even learning this probability distribution.
So there's an unknown per-action distribution for each arm.

[p.11]

So how can we define the bandit?
Well, we assume that the reward for each action is distributed according to some distribution, and the distribution for action a_i is parameterized by a parameter vector θ_i.
So for example, if your rewards are 0 and 1, you might be in a setting where the probability of getting reward is θ_i, which is just a number, and the probability of getting reward is 1-θ_i.
If your rewards are continuous, maybe you have some continuous distribution.
And you don't really know what the θ_i are, but you could assume that you have a prior on them.
You could use an uninformative prior if you like, but in general we'd say we have some prior p(θ).
So that's defining our bandit.
Now the cool thing about this is that you could also view this as defining a POMDP for exploration, where the state is the vector of θ for all of your actions.
Now you don't know this state, but if you knew this state, then you could figure out what the right action is.
So instead of knowing the state, you will belief.
So you have some belief ^{p}(θ_1, ..., θ_n), and you can update your belief each time you pull an arm.
So each time you pull an arm, you observe the reward of that arm, and you can update your belief about the θ corresponding to that arm.
And you could solve this POMDP to basically figure out what is the right sequence of actions to maximize your reward in this POMDP.
And this will yield the optimal exploration strategy, because if it is the optimal policy in the POMDP, it is the optimal thing to do under this kind of uncertainty.
And that will be the optimal exploration strategy, the best exploration strategy you could possibly have.
Now this is overkill.
The belief state is huge, even for a simple POMDP with binary rewards.
Remember, your belief state is not the vector of θs.
It is actually a probability distribution over θs.
So even in the simple binary reward bandit, the θs correspond to the probability of getting a reward of 1.
The ^{p}(θ) now needs to be some parametric class, maybe a bunch of β distributions.
You could have covariances between the different θs.
So it is potentially a really complex belief state.
And the cool thing about bandits is that you can provably do very well with much simpler strategies than solving this full POMDP.
And the way that you would quantify doing well is by quantifying the regret of your strategy relative to how well actually solving the POMDP does.
So when we say that a particular exploration strategy is optimal, what we really mean is that it is not much worse than actually solving the POMDP.
And not much worse is usually defined in a kind of a big O sense.
So how do we measure the goodness of an exploration algorithm?
Well, we do it in terms of regret.
And regret is the difference from the optimal policy at time step capital T.
So you can write the regret as capital T times the expected value of the reward of a^{*}, that is the optimal policy, minus the sum of rewards that you actually got.
So the optimal policy will always take a^{*}, and that means that if you are going for capital T steps, it will be capital T times the expected reward of a^{*}.
So that is what the optimal policy will do.
And then your regret is the difference between that and the sum of rewards that you have actually gotten from running your strategy.
So this is the expected reward of the best action, the best you can hope for in expectation, and this is the actual reward of the action that was actually taken.

[p.12]

All right.
So in the next portion, I am going to talk about how we can minimize regret in terms of closing the gap between our tractable strategies and this POMDP that we have defined.