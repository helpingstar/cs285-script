All right, next let's talk about some actual exploration algorithms that we could use in deep reinforcement learning.
So to recap, the classes of exploration methods that we have are optimistic exploration, which basically say that visiting a new state is a good thing.
This requires estimating some kind of state visitation frequency or novelty, just like we had to count the number of times that we took each action in the bandit setting.
And this is typically realized by means of some kind of exploration bonus.
We have Thomson sampling style algorithms.
So these are algorithms that learn a distribution over something, either a model, a Q function, or a policy, just like we learned this distribution over bandit parameters before.
And then they sample and act according to that sample.
And then we have information gain style algorithms, which reason about the information gain from visiting new states and then actually choose the transitions that lead to large information gain.
So let's start with the optimistic exploration.
So we have a lot of different methods.
So in the bandit world, we saw that one rule that we could use to balance exploitation and exploration is to select an action based on the argmax of its average expected value, empirical estimate, based on what we've seen before, plus the square root of 2 times log t divided by n a.
And the important thing here is really the denominator.
So you're basically assigning bonuses based on some function of the inverse of the number of times you've pulled that arm.
So this is a...
essentially a kind of exploration bonus.
And the intuition in reinforcement learning is that we're going to construct an exploration bonus that is not just for different actions, but actually also for different states.
And the thing is, lots of different functions will work for this exploration bonus, as long as they decrease with n of a.
So don't worry too much about the fact that it's a square root or that it has a 2 times log t in the numerator.
The important thing is that it's some quantity that decreases rapidly as n of a.
So it's a function that increases.
Okay, so can we use this idea with MDPs?
One thing we could do is essentially extend it to the MDP setting and create what is called count-based exploration.
So instead of counting the number of times you've pulled some arm, which is n of a, you would count the number of times you've visited some state action tuple s,a, or even just the number of times you've visited some state n of s, and use it to add an exploration bonus.
So we can use this idea to estimate the reward for the exploration bonus.
So the UCB estimate in the bandit case is estimating the reward with an additional exploration bonus.
In the MDP case, we will also estimate reward with an exploration bonus.
So what that means is that we will define a new reward function, r plus, which is the original reward, plus this bonus function applied to n of s.
And the bonus function is just some function that decreases with n of s.
So maybe it's the square root of 1 over n of s.
And then we would simply use r plus instead of r as our reward function for any RL algorithm that we'd care to use.
And of course, in this case, r plus will change as our policy changes.
So maybe every episode it would update r plus.
So we need to make sure that our RL algorithm doesn't get too confused by the fact that our rewards are constantly changing.
But other than that, this is a perfectly reasonable way to kind of extend this UCB idea into the MDP setting.
So it's a simple addition to any RL algorithm.
It's very modular.
But you do need to tune a weight on this bonus because, you know, if you do 1 million divided by n of s, that'll work very differently than if you do, you know, 0.001 divided by n of s.
So you need to decide how important the bonus is relative to the reward.
And of course, you need to figure out how to actually do the counting.
So let's talk about that second problem.
What's the trouble with counts?
The trouble with counts is that the notion of a count, while it makes sense in small, discrete MDPs, doesn't necessarily make sense in more complex MDPs.
So let's look at this frame from Montezuma's Revenge.
So clearly, if I find myself seeing the same exact image 10 times, then the count for that image should be 10.
But what if only one thing in the image varies?
So what if, for example, the guy here just stands in the same spot, but the skull moves around?
Every location for the skull.
Every location for the skull is now a totally different state.
Now if the guy is moving and the skull is moving, what are the chances that they would ever be in the same exact combination of locations twice?
So maybe they'll be in very similar states, but they might not be in exactly the same spot twice.
And in general, if you have many different factors of variation, you get combinatorially many states, which means the probability of visiting the same exact state a second time becomes very low.
So all these moving elements will cause issues.
What about continuous spaces?
There, the situation is even more dire.
So if you imagine that robotic hand example from before, now the space is continuous, so no two states are going to be the same.
So the trouble is, in these larger RL problems, we basically never see the same thing twice, which makes counting kind of useless.
So how can we extend counts to this complex setting, where you either have a very large number of states or even continuous states?
Well, the notion that we want to exploit is that some states are more similar than others.
So even though we never visit the same exact state twice, there's still a lot of similarity between those states, and we can take advantage of that by using a generative model or a density estimator.
So here's the idea.
We're going to fit some density model to Pθ or Pθ , depending on whether we want state counts or state attributes.
So we're going to fit some density model to Pθ or Pθ , depending on whether we want state counts or state attributes.
So we're going to fit some density model to Pθ or Pθ , depending on whether we want state counts or state attributes.
So a density model could be something simple, like a Gaussian, or it could be something really complicated.
So a density model could be something simple, like a Gaussian, or it could be something really complicated.
We'll talk about the particular choice of density model later, but for now we just need it to be something that produces an answer to the question, what is the density or the likelihood of this state?
Now, if you learn a density model as, for example, some highly expressive model like a neural network, then Pθ might be a very similar model to the density model, but with a different density.
Pθ might be high even for totally new states that you've never seen before, if they're very similar to states that you have seen before.
So maybe you've never seen the guy and the skull in precisely this position, but you've seen the guy in that position, and you've seen the skull in that position, just not together, so that state will probably have a higher density than if something totally weird happened, like if, for example, you suddenly picked up the key.
You know, if in all prior states the key is always present, now suddenly it's absent, that'll have a very low density.
So the question that we could ask then is, can we somehow use Pθ as a sort of pseudo-count?
Now, it's not a count, because it's not literally telling you how many times you've visited a particular state, but it kind of looks a little bit like a count, in that, you know, if you take P and you multiply it by the total number of states you've seen, that will be a kind of a density for that state.
So, if you have a small m2, you can do this.
So, if you have a small m2, you can do this.
So, if you have a small m2, you can do this.
And then if you have an mdp, where practical counting is doable, then the probability of a state is just the count on that state divided by the total number of states you've seen.
So, it's n divided by n.
So, the probability relates to the count and the total number of states you visited.
Which means that after you see the state s, your new probability is the old count plus one divided by the old n plus one.
So here's the question.
Can you get p of theta of s and p of theta prime of s to obey these equations?
So instead of keeping track of counts, we keep track of p of s's, but we'll update theta when we see s to get a new theta prime, meaning we'll update our density model, we'll change its parameters.
So can we look at how p theta of s and p theta prime of s have changed and recover something that also obeys these equations, that essentially looks like a count?
It's not a count, but it looks like a count and it acts like a count, so it could be used as a count.
So this is based on a paper called Unifying Count-Based Exploration by Belmar et al.
The idea is this.
We're going to fit a model, p theta of s, to all the states that we've seen so far in our data set D.
Then we will take a step i and observe the new state si.
Then we will fit a new state i and observe the new state si.
Then we will take a step i and observe the new state si.
So we can estimate the new model, p theta prime of s to D with the new state appended to it.
And then we'll use p theta si and p theta prime of si to estimate a pseudocount, which I'm going to call n hat of s.
And then we'll set R plus to be R plus some bonus determined by n hat of s.
So this n hat of s is a pseudocount.
And then we repeat this process.
So how do you get the pseudo-count?
Well, we'll use the equations from the previous slide.
So the equations in the previous slide describe how counts relate to probabilities.
So we'll say that we want our pseudo-counts to also relate to probabilities in the same way.
So we know pθ of s, and we know pθ' of s, because that's what we get by updating our density model.
We don't know n-hat of s, and we don't know little n-hat.
However, we have two equations and two unknowns.
So we could actually solve this system of equations and recover n-hat of s and little n-hat.
So we have two equations and two unknowns.
If we do a bit of algebra, here's what the solution looks like.
n-hat of s is equal to little n-hat times pθ of s.
That's kind of the obvious statement.
And if you manipulate the algebra, you can solve for n-hat and find that it's equal to 1 over pθ' of s divided by pθ' of s minus pθ.
And that whole thing multiplied by pθ of s.
It's a little bit of an opaque expression, but it's pretty easy to solve for.
You basically take that expression for n-hat of s, substitute that in in place of n-hat of s for the top two equations, so then you can get two equations that are both expressed in terms of little n-hat, and then you solve them for little n-hat, and you get the solution at the bottom.
So now every time step, you just use these equations to figure out big n-hat of s, and use that to calculate your bonus.
And now your bonus will be aware of similarity between states.
Now there are a few technical issues left.
We have to resolve what kind of bonus to use, and what kind of density model to use.
Now there are lots of bonus functions that are used in the literature, and they're all basically inspired by methods that are known to be optimal for bandits or for small MDPs.
So for example, the classic UCB bonus would be 2 times log little, divided by big N, and then you take the square root of that whole thing.
Another bonus in this paper by Strela and Lippmann is to just use a square root of 1 over n-hat of s.
That's a little simpler.
Another one is to use 1 over n-hat of s.
They're all pretty good.
They could all work.
This is the one used by Bellmeyer et al., but you could choose whichever one you prefer.
Does this algorithm work?
Well, here's the evaluation that's used in the pseudo-counts paper.
So in this paper, they are comparing different methods.
The important curves to pay attention to are the green curve and the black curve.
So the black curve is basically Q-learning the way that you're implementing it right now, and the green curve is their method with a 1 over square root of n bonus.
And you can see here that, you know, on some games it makes very little difference, like Hero.
On some games it makes a little bit of a difference, and on some games, like Montezuma's Revival, it makes an enormous difference, where there's almost no progress without it.
The pictures at the bottom illustrate the rooms that you visited.
And so, as I mentioned before, in Montezuma's Revenge, the rooms are arranged in a kind of pyramid, and you start at the top of the pyramid.
And you can see that without the bonus, you only visit two rooms.
With the bonus, you actually visit more than half of the pyramid.
So the method is doing something pretty sensible.
What kind of model should you use for p-theta of s?
Well, there are a few choices.
There are choices to be made about this model that are a little peculiar than the trade-offs we typically consider for density modeling and generative modeling.
Usually, when we want to train a generative model, like a GAN or a VAE, what we care about is being able to sample from that model.
But for pseudo-counts, all you really want is a model that will produce a density score.
You don't really need to sample from it, and you don't even really need those scores to be normalized.
So as long as the number goes up, as the state has higher density, you're happy with it.
So that means that the trade-offs for these density models are sometimes a little different than what we're used to from the world of generative modeling.
In fact they're often the opposite of the considerations for many popular generative models in the literature, like GANs, which can produce great samples but don't produce densities.
The particular model that Belmar et al.
uses is a little peculiar.
So it's the CTS model.
It's actually a very simple model.
It just models the probability of each pixel conditioned on its upper and left neighbors.
So you can think of it as a directed graphical model where there are edges from the upper and left neighbors of each pixel pointing to that pixel.
It's a little weird, but it's very simple and it produces these scores.
It's not a good density model, and there are much better choices.
But that's the one they use in the paper.
So other papers have used stochastic neural networks, compression length, and something called EX2, which I'll cover shortly.
But in general, you could use any density model you want so long as it produces good probability scores without caring about whether it produces good samples or not.