Alright, in the last portion of today's lecture we're going to go over another way to impose the constraint which is a little more approximate but leads to a pretty simple algorithmic framework called natural gradient.
Okay, so so far we discussed this optimization problem maximize the expected value under the old policy state distribution of the expected value under the old policies action distribution of the important sampled estimator subject to a constraint that the KL divergence between pi theta prime and pi theta is bounded by epsilon and we saw how if you can enforce this for a small enough epsilon then this is guaranteed to improve j theta prime minus j theta.
In the previous section we saw how we could impose this constraint by using something like a dual gradient descent method typical Lagrange multiplier and then essentially optimize the Lagrangian as a kind of penalized objective to find the new policy parameters.
Now we're going to talk about a more approximate way of doing this which at first will seem a little bit worse because we're going to make a lot of approximations but in fact is actually highly desirable because it allows us to recover a very simple method that doesn't require any additional penalties and simply applies a linear transformation to the standard policy gradient that we saw earlier.
In the previous section we saw how we could enforce the constraint by optimizing the gradient descent method to the same rule as we did back in lecture 3.
Okay so as before I'm going to use a bar theta prime to denote this whole objective.
When we calculate the gradient of some objective and we use gradient descent or gradient descent that can be interpreted as optimizing a first-order Taylor expansion of that objective.
This might seem like a slightly silly statement because a first-order Taylor expansion is a fairly simple thing to do.
It's just a gradient but when there's a constraint present it actually has some meaning.
So essentially if we want to optimize some complicated function like this blue curve here one way we could do it is we could pick a region compute a very simple approximation, a linear approximation to that function shown here with this green line and that's obtained by taking the gradient and then instead of optimizing the blue curve which is really complicated we optimize the green line which is very very simple.
Of course, if we don't impose any constraint the green line goes to positive and negative infinity so this only makes sense if we impose a constraint which is essentially the region within which we trust the degree to which the green line approximates the blue line, a trust region.
And that's what this red box represents.
So if you find the best value of the green line within the red box if the red box is chosen to be small enough you might hope that you can find the best value of the green line within the red box if the red box is chosen to be small enough you might hope that you can get it right away.
that point is also going to improve the blue curve which is what you really want.
So if we are minimizing then we would pick this point.
We'd pick the point on the edge of the red region, where the green function has the lowest value, which will hopefully also be a point where the blue function has a lower value.
And if we were maximizing it would go the other way.
So what this means is that if you approximate your complicated nonlinear function with a first-order Taylor expansion, essentially a linear function, you could optimize that linear function so long as you impose a trust region, so long as you limit your optimization to just the region where that linear approximation is a good approximation to the true nonlinear function.
So we could do that with our optimization problem and we could change it to take the argmax of grad A transpose theta prime minus theta subject to this constraint and the constraint is what's imposing the trust region.
And now this is very very easy to do of course in terms of the objective because the objective is simply linear and theta prime which is our optimization variable.
The constraint is still kind of complicated so that makes things a little more complex.
Okay so the basic intuition behind what I'm going to describe next is that we're going to approximate that constraint in some very tractable way which will actually give us a closed-form solution for theta prime where we won't actually have to do any complicated nonlinear optimization procedure with that constraint.
So we're going to do that.
And then we're going to do that with multiple gradient steps but in fact a single step of policy gradient will actually take care of this entire constrained optimization problem.
And the intuition for why that's possible is because once our objective is linear optimization becomes easy.
So our objective turned into a first-order Taylor approximation.
By the way just as an aside why do we want to do all this?
Well the reason we want to do all this is because we're going to get a very very simple algorithm.
So we're going to introduce these additional steps of approximation but the payoff that we'll get is a simpler algorithm that still works quite well.
Okay so there's our new simplified constraint problem.
And one really appealing thing about simplifying it in this way is that our gradient grad theta a theta is going to essentially become exactly the normal policy gradient.
So here's the equation for grad theta prime a theta prime and since only the importance weight does not matter.
It depends on theta prime.
We just get the importance sampled policy gradient.
But if we evaluate it at the point theta prime equals theta then the importance weights actually cancel out, because we get the same thing in the numerator and the denominator.
So here I just wrote it out for grad theta a bar theta and when they cancel out you're just left with the standard policy gradient equation which we saw in the previous lecture.
And this is very simple.
This is essentially exactly the policy gradient that we had back in reinforce, except that we have some choice of advantage estimator for a pi theta.
And we can use anything we want there.
Okay.
So that's nice.
And that's just the thing that we're going to plug into our objective.
That's just the regular policy gradient.
Now, could we just use gradient ascent then?
If we're just calculating the policy gradient, our objective is grad theta j theta transpose theta prime.
Does gradient ascent do something similar to this constraint optimization problem?
So gradient ascent, we'll just take theta and set it to be the old theta plus alpha times grad theta j theta.
One problem with doing this is that as we change theta, the probabilities of the new policy, the pi theta a given s, will change by different amounts.
Because some parameters, some entries in theta affect the probabilities more than others.
So in general, taking a step like this will usually not respect the KL divergence constraint, because you might have some very, very small change in some parameter, but that parameter might have a very, very large influence on the probabilities.
But what does gradient ascent do?
Well, my claim is that gradient ascent actually solves this constraint optimization problem, which is very simple.
So let's take gradient descent.
So gradient descent is basically the same as gradient descent, just the difference is that it's actually similar to our original one.
So the difference is that our original constraint optimization problem had grad j theta prime minus theta as its objective, but the constraint was the KL divergence between pi theta prime and pi theta.
In gradient descent, the constraint is the square Euclidean distance between theta prime and theta.
So, in a policy whose distribution is close to the policy pi theta, gradient descent instead produces parameters that are close to the parameters theta.
Geometrically you can think of it like this, that in gradient descent we have our parameter space which here is represented by this 2D region, so there's two axes corresponding to two policy parameters.
When we linearize our objective, we can visualize our objective as this kind of gradient field, as a colored gradient over the space, so lower right is better values, upper left is worse values, and this constraint corresponds to a circle in this space.
So all the points that are epsilon away from the dot in the center all satisfy that constraint.
So because your objective is linear and your constraint is a circle, then the solution will always lie on the surface of that circle in the direction of the objective.
So that's the idea of the solution.
So in the direction of grad j theta.
So that means that when you do gradient ascent, you will take a step in the direction grad j theta, and the length of that step will be determined by epsilon.
In fact, the learning rate in gradient ascent can actually be obtained as the Lagrange multiplier for this constraint, and you can also derive a closed form equation for it.
So if you perform gradient ascent like this, where your learning rate is given by the square root of epsilon divided by the square root of epsilon, and you can actually prove that you will exactly satisfy that constraint.
And the way that you prove this is you take that equation for theta prime, substitute that into the constraint, the thetas will cancel, and you'll be left with the square root of epsilon divided by grad j squared, and then that whole thing's squared.
So then the square root goes away, and you're left with grad j squared at the top, grad j squared at the bottom, those cancel out, and you're just left with epsilon, and epsilon is less than or equal to epsilon.
If this is unclear to you, then what I would encourage you to do is to take a piece of paper, take this equation that I wrote for theta prime, plug that into the constraint, and do the algebra, and you'll be able to show that the result is that epsilon is less than or equal to epsilon.
Okay, so where do we get to with all this?
The conclusion from all this is that gradient ascent does actually solve a constrained optimization problem, but it's the wrong constraint.
The constraint is in fact not the constraint, it is the constraint of a constraint.
And so you and me, we have to try not to get lost in this.
We're not getting lost in this constraint, because gratitude is not the constraint.
theta space instead of distribution space.
And because different entries in theta might affect the distribution to different degrees, in general a constraint in theta space will not enforce the constraint in KL divergence space because some parameters theta might change a tiny bit but produce very big changes in the corresponding distribution.
So intuitively the constraint shape is a circle for gradient descent but we want it to be a kind of ellipse.
We want the ellipse to be squished along the highly sensitive direction so we want a tighter constraint in the direction of theta that results in big changes in probability and we want the ellipse to be elongated in the directions where large changes in theta result in small changes in probability.
So that's what we're going to try to get next.
So what we're going to do, we're going to recognize that these are not the same and we're going to construct a new constraint in the distribution space.
So we're going to construct a tractable approximation to the KL divergence constraint which is the one that we actually want.
So the way that we're going to do this is by doing the same thing to the constraint that we did to the objective.
So for the objective we calculated a Taylor expansion, specifically a first order Taylor expansion.
For the constraint we're actually going to use a second order Taylor expansion around the point theta prime equals theta.
We don't want to use a first order Taylor expansion because the KL divergence has a derivative of zero at theta prime equals theta.
It's actually flat but its second derivative is not zero.
The second order Taylor expansion is a quadratic form with some matrix f and it turns out that the matrix that goes in there is something called the Fisher information matrix.
The calculation for this is a bit involved but you basically start with a formula for the KL divergence and if you take the second derivative you can show that the correct thing to use for that matrix is the Fisher information matrix.
So the Fisher information matrix is the expected value under pi theta of the outer product of the grad log π's.
And one very convenient thing about the Fisher information matrix is that you can approximate it using samples.
Because it's the expected value of some quantity you could simply use the same samples that you drew from pi theta to estimate your policy gradient to also estimate the Fisher information matrix.
So the Fisher information matrix is the expected value of the grad log π's.
Now if we're going to use this second order Taylor expansion as an approximation for our KL divergence constraint something that we can note is that gradient descent actually also has a quadratic constraint.
Theta minus theta prime squared can also be interpreted as theta prime minus theta transpose times identity times theta prime minus theta.
And now we have the same thing only instead of identity we have f.
So we can estimate our Fisher information matrix with samples and then we can plug that in as a quadratic constraint just using the matrix f instead of identity.
And visually you can think of it as turning that circle into some ellipse where the shape of the ellipse is determined by the matrix f.
The matrix f basically describes which directions in theta space around the point theta lead to large changes in probability and which ones lead to small changes in probability.
And you can actually show that if your constraint is quadratic like this and you write out the Lagrangian if you know the Lagrange multiplier then the solution will be given by this equation.
The reason for that of course is that the Lagrangian will have a linear term grad j transpose theta prime and a quadratic term given by the quadratic KL divergence expansion and a second order polynomial function like this has a closed form solution which uses the Lagrangian function to calculate the Lagrangian function.
So if we write out the Lagrangian function we can see that the Lagrangian function is given by the quadratic term and the quadratic term which uses the inverse of the quadratic term times the linear term.
So this is the closed form solution when the constraint is given by one half theta prime minus theta transpose times f times theta prime minus theta.
And furthermore if we want to enforce the constraint that this second order expansion be less than or equal to epsilon we can use the same general idea as we used in gradient descent and figure out the Lagrangian function that satisfies that constraint.
And again you can check that the step size is correct by plugging in this equation for alpha then plugging in the resulting equation for theta prime into our KL divergence approximation and then you will be able to show that you end up with the constraint being satisfied.
So this expression is called the natural gradient so you take the regular gradient and you multiply it by the inverse of the Fischer information matrix and you get the natural gradient.
Okay, so take a moment to look over the slide.
If something here is unclear, please make sure to write a question in the comments.
All right, now here what I want to do is I want to back up a little bit and just ask, well, is this even a problem in practice?
Like, it kind of seemed like regular gradient ascent, regular vanilla policy gradient, already enforces some kind of constraint.
Maybe that constraint is just good enough.
I mean, it does keep theta prime close to theta.
Maybe not quite in the right way, but maybe that's a reasonable approximation.
Well, let's think back to that example that we had in the policy gradient lecture, where we had a one-dimensional state space and a one-dimensional action space, where you get to choose to go left or right.
Your reward was given by negative s squared minus a squared, and you had two policy parameters, k and sigma, where the probability of the actions is normally distributed, where the mean is k times s, and sigma is the variance.
So here your parameters are k and sigma.
In that setting, you're basically adding some Gaussian noise and walking left and right, in that setting we saw that the regular policy gradient actually looks very unfortunate.
The regular policy gradient does have to point in the right direction, it always does, but it's extremely ill-conditioned.
Essentially, as sigma decreases, the gradient with respect to sigma becomes much, much larger than the gradient with respect to k.
So even though the optimal solution is that k equals negative 1, you might never reach that solution because you'll be too busy decreasing sigma.
Intuitively, the reason this is happening is that k and sigma affect the probabilities to different degrees.
And because sigma affects the probabilities so much more when it's small, then the corresponding gradient component is much larger.
And, simply, staying within a fixed ball in theta space, even in this very simple 2-dimensional space, is just not enough.
So it's essentially the same as a very ill-conditioned optimization problem.
The natural policy gradient largely resolves this issue.
So if you just multiply the gradient by f inverse, then that correctly accounts for the fact that sigma affects the parameters a lot more, and actually prioritizes k-large, it actually increases the gradient component with respect to k and gets you to point to work in the right direction.
So this really is a problem in practice, especially for continuous distributions like Gaussians.
So in practice natural gradient really does give you a much more well-conditioned gradient direction and it makes it a lot easier to adjust learning rates.
Alright, a few practical notes.
So natural policy gradient as classically described in the literature is an algorithm where you calculate the Fisher information matrix F approximately using samples and then manually select a step size alpha.
It's generally a good choice to stabilize policy gradient training and if you want to learn more about this, check out this paper by Peters and Schall called Reinforcement Learning of Motor Skills with Policy Gradients.
Practical implementations require computing this product between F inverse and grad J, which is a little bit non-trivial to do without computing full matrix.
But if you want to implement this in practice, check out this paper called Trust Region Policy Optimization.
There are a few tricks that are very useful.
So one trick is that you can actually solve for the step size alpha, which actually respects a particular value of epsilon.
So if instead of picking alpha you want to pick epsilon, here's the formula.
But even more importantly, one of the things you could do which makes this much more efficient to calculate is you can use the conjugate gradient method to calculate F inverse grad J and in the process of running conjugate gradient, you're actually going to get the step size alpha given by this equation automatically as a byproduct.
I'm not going to go into details about this and I would encourage you to check out the Trust Region Policy Optimization paper for details, but it's a fairly straightforward application of CG of conjugate gradient.
Now, of course, you could just use the importance sampled objective directly with something like great dual gradient descent, or even a heuristic selection of the Lagrange multiplier, and that works fairly well.
So that can be interpreted as a kind of regularization to stay close to the old policy.
And that's what I covered in the previous section.
If you want to learn more about this, check out Proximal Policy Optimization.
Okay, so to review, we talked about how policy gradient can be interpreted as policy iteration.
How you can optimize the advantage of the old policy under the new policy state distribution, and that is exactly policy iteration.
But furthermore, if you use the old policy state distribution, then you're optimizing a bound if the policies are close enough.
And this results in a constraint optimization problem.
You can enforce this constraint explicitly, or you can impose the constraint approximately by doing a first-order approximation of the objective, which gives you gradient descent, but regular gradient descent has the wrong constraint, so you use natural gradient descent.
So you can use the natural gradient, which gives you an approximation to the KL divergence constraint.
So to summarize, regular policy gradient is a constraint procedure, but it has the wrong constraint.
Natural policy gradient replaces it with an approximation to the KL divergence, or you could actually optimize the original important sampled objective for many gradient steps with an additional regularizer to stay close to the policy, the previous policy, multiplied by Lagrange multiplier.
Practical algorithms that use this, natural policy gradient, and natural gradient descent.
So we have the policy gradient, trust region policy optimization, and proximal policy optimization.